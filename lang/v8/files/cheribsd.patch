diff --git BUILD.gn BUILD.gn
index 00770169bf5..25d7f275dcd 100644
--- BUILD.gn
+++ BUILD.gn
@@ -4265,6 +4265,7 @@ v8_source_set("v8_base_without_compiler") {
     "src/codegen/unoptimized-compilation-info.cc",
     "src/common/assert-scope.cc",
     "src/common/code-memory-access.cc",
+    "src/common/ptr-compr.cc",
     "src/compiler-dispatcher/lazy-compile-dispatcher.cc",
     "src/compiler-dispatcher/optimizing-compile-dispatcher.cc",
     "src/date/date.cc",
diff --git include/v8-fast-api-calls.h include/v8-fast-api-calls.h
index 6ef950c10f2..68995c14aaf 100644
--- include/v8-fast-api-calls.h
+++ include/v8-fast-api-calls.h
@@ -439,10 +439,12 @@ struct AnyCType {
   };
 };
 
+#ifndef __CHERI_PURE_CAPABILITY__
 static_assert(
     sizeof(AnyCType) == 8,
     "The AnyCType struct should have size == 64 bits, as this is assumed "
     "by EffectControlLinearizer.");
+#endif
 
 class V8_EXPORT CFunction {
  public:
diff --git include/v8-internal.h include/v8-internal.h
index 970e592bcd5..afd0331853e 100644
--- include/v8-internal.h
+++ include/v8-internal.h
@@ -40,6 +40,7 @@ constexpr size_t TB = size_t{GB} * 1024;
  * Configuration of tagging scheme.
  */
 const int kApiSystemPointerSize = sizeof(void*);
+const int kApiSystemPointerAddrSize = sizeof(ptraddr_t);
 const int kApiDoubleSize = sizeof(double);
 const int kApiInt32Size = sizeof(int32_t);
 const int kApiInt64Size = sizeof(int64_t);
@@ -115,6 +116,27 @@ struct SmiTagging<8> {
   }
 };
 
+// Smi constants for systems where tagged pointer is a 64-bit value and
+// the system point is a 128-bit value.
+template <>
+struct SmiTagging<16> {
+  enum { kSmiShiftSize = 31, kSmiValueSize = 32 };
+
+  static constexpr intptr_t kSmiMinValue =
+      static_cast<intptr_t>(kUintptrAllBitsSet << (kSmiValueSize - 1));
+  static constexpr intptr_t kSmiMaxValue = -(kSmiMinValue + 1);
+
+  V8_INLINE static int SmiToInt(const internal::Address value) {
+    int shift_bits = kSmiTagSize + kSmiShiftSize;
+    // Shift down and throw away top 32 bits.
+    return static_cast<int>(static_cast<intptr_t>(value) >> shift_bits);
+  }
+  V8_INLINE static constexpr bool IsValidSmi(intptr_t value) {
+    // To be representable as a long smi, the value must be a 32-bit integer.
+    return (value == static_cast<int32_t>(value));
+  }
+};
+
 #ifdef V8_COMPRESS_POINTERS
 // See v8:7703 or src/common/ptr-compr-inl.h for details about pointer
 // compression.
@@ -122,7 +144,7 @@ constexpr size_t kPtrComprCageReservationSize = size_t{1} << 32;
 constexpr size_t kPtrComprCageBaseAlignment = size_t{1} << 32;
 
 static_assert(
-    kApiSystemPointerSize == kApiInt64Size,
+    kApiSystemPointerAddrSize == kApiInt64Size,
     "Pointer compression can be enabled only for 64-bit architectures");
 const int kApiTaggedSize = kApiInt32Size;
 #else
@@ -269,7 +291,11 @@ constexpr bool kAllowBackingStoresOutsideSandbox = true;
 // size allows omitting bounds checks on table accesses if the indices are
 // guaranteed (e.g. through shifting) to be below the maximum index. This
 // value must be a power of two.
+#if defined(__CHERI_PURE_CAPABILITY__)
+static const size_t kExternalPointerTableReservationSize = 256 * MB;
+#else
 static const size_t kExternalPointerTableReservationSize = 128 * MB;
+#endif
 
 // The maximum number of entries in an external pointer table.
 static const size_t kMaxSandboxedExternalPointers =
@@ -396,7 +422,14 @@ class Internals {
   static const int kIsolateLongTaskStatsCounterOffset =
       kIsolateFastApiCallTargetOffset + kApiSystemPointerSize;
   static const int kIsolateRootsOffset =
+#if defined(__CHERI_PURE_CAPABILITY__)
+      // With CHERI's stronger alignment, padding of sizeof(size_t) is added
+      // between the long_stat_task_counter_ (size_t) and roots_table_ (uintptr_t)
+      // fields.
+      kIsolateLongTaskStatsCounterOffset + kApiSizetSize + kApiSizetSize;
+#else
       kIsolateLongTaskStatsCounterOffset + kApiSizetSize;
+#endif
 
   static const int kExternalPointerTableBufferOffset = 0;
   static const int kExternalPointerTableCapacityOffset =
@@ -456,7 +489,11 @@ class Internals {
   }
 
   V8_INLINE static bool HasHeapObjectTag(const internal::Address value) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return (value & (size_t) kHeapObjectTagMask) == static_cast<Address>(kHeapObjectTag);
+#else
     return (value & kHeapObjectTagMask) == static_cast<Address>(kHeapObjectTag);
+#endif
   }
 
   V8_INLINE static int SmiValue(const internal::Address value) {
diff --git include/v8config.h include/v8config.h
index 0e5bb1558d0..f641faf291f 100644
--- include/v8config.h
+++ include/v8config.h
@@ -288,6 +288,8 @@ path. Add it with -I<path> to the command line
 //
 //  V8_HAS_ATTRIBUTE_ALWAYS_INLINE      - __attribute__((always_inline))
 //                                        supported
+//  V8_HAS_ATTRIBUTE_CONSTINIT          - __attribute__((require_constant_
+//                                                       initialization))
 //  V8_HAS_ATTRIBUTE_NONNULL            - __attribute__((nonnull)) supported
 //  V8_HAS_ATTRIBUTE_NOINLINE           - __attribute__((noinline)) supported
 //  V8_HAS_ATTRIBUTE_UNUSED             - __attribute__((unused)) supported
@@ -333,6 +335,8 @@ path. Add it with -I<path> to the command line
 #endif
 
 # define V8_HAS_ATTRIBUTE_ALWAYS_INLINE (__has_attribute(always_inline))
+# define V8_HAS_ATTRIBUTE_CONSTINIT \
+    (__has_attribute(require_constant_initialization))
 # define V8_HAS_ATTRIBUTE_NONNULL (__has_attribute(nonnull))
 # define V8_HAS_ATTRIBUTE_NOINLINE (__has_attribute(noinline))
 # define V8_HAS_ATTRIBUTE_UNUSED (__has_attribute(unused))
@@ -432,6 +436,14 @@ path. Add it with -I<path> to the command line
 # define V8_ASSUME_ALIGNED(ptr, alignment) (ptr)
 #endif
 
+// A macro to mark a declaration as requiring constant initialization.
+// Use like:
+//   int* foo V8_CONSTINIT;
+#if V8_HAS_ATTRIBUTE_CONSTINIT
+# define V8_CONSTINIT __attribute__((require_constant_initialization))
+#else
+# define V8_CONSTINIT
+#endif
 
 // A macro to mark specific arguments as non-null.
 // Use like:
diff --git src/api/api.cc src/api/api.cc
index f5eac55602e..0ad7bc9d27f 100644
--- src/api/api.cc
+++ src/api/api.cc
@@ -5407,6 +5407,13 @@ template <>
 struct OneByteMask<8> {
   static const uint64_t value = 0xFF00'FF00'FF00'FF00;
 };
+#if defined(__CHERI_PURE_CAPABILITY__)
+template <>
+struct OneByteMask<16> {
+  static const uint64_t value = 0xFF00'FF00'FF00'FF00;
+};
+#endif
+
 static const uintptr_t kOneByteMask = OneByteMask<sizeof(uintptr_t)>::value;
 static const uintptr_t kAlignmentMask = sizeof(uintptr_t) - 1;
 static inline bool Unaligned(const uint16_t* chars) {
diff --git src/ast/scopes.cc src/ast/scopes.cc
index 679472c7c62..dcb427533a8 100644
--- src/ast/scopes.cc
+++ src/ast/scopes.cc
@@ -36,9 +36,16 @@ namespace internal {
 //       use. Because a Variable holding a handle with the same location exists
 //       this is ensured.
 
+#ifdef __CHERI_PURE_CAPABILITY__
 static_assert(sizeof(VariableMap) == (sizeof(void*) + 2 * sizeof(uint32_t) +
+			              sizeof(uint64_t) + // Padding
                                       sizeof(ZoneAllocationPolicy)),
               "Empty base optimization didn't kick in for VariableMap");
+#else
+static_assert(sizeof(VariableMap) == (sizeof(void*) + 2 * sizeof(uint32_t) +
+                                      sizeof(ZoneAllocationPolicy)),
+              "Empty base optimization didn't kick in for VariableMap");
+#endif
 
 VariableMap::VariableMap(Zone* zone)
     : ZoneHashMap(8, ZoneAllocationPolicy(zone)) {}
diff --git src/base/address-region.h src/base/address-region.h
index 517f5935e0f..34844056f2f 100644
--- src/base/address-region.h
+++ src/base/address-region.h
@@ -57,7 +57,7 @@ class AddressRegion {
     Address overlap_start = std::max(begin(), region.begin());
     Address overlap_end =
         std::max(overlap_start, std::min(end(), region.end()));
-    return {overlap_start, overlap_end - overlap_start};
+    return {overlap_start, (size_t) (overlap_end - overlap_start)};
   }
 
   bool operator==(AddressRegion other) const {
diff --git src/base/atomic-utils.h src/base/atomic-utils.h
index e81176e582a..c679659e1b0 100644
--- src/base/atomic-utils.h
+++ src/base/atomic-utils.h
@@ -35,8 +35,33 @@ class AtomicValue {
   }
 
  private:
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static_assert(sizeof(T) <= sizeof(base::AtomicIntPtr));
+#else
   static_assert(sizeof(T) <= sizeof(base::AtomicWord));
+#endif
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+  template <typename S>
+  struct cast_helper {
+    static base::AtomicIntPtr to_storage_type(S value) {
+      return static_cast<base::AtomicIntPtr>(value);
+    }
+    static S to_return_type(base::AtomicIntPtr value) {
+      return static_cast<S>(value);
+    }
+  };
+
+  template <typename S>
+  struct cast_helper<S*> {
+    static base::AtomicIntPtr to_storage_type(S* value) {
+      return reinterpret_cast<base::AtomicIntPtr>(value);
+    }
+    static S* to_return_type(base::AtomicIntPtr value) {
+      return reinterpret_cast<S*>(value);
+    }
+  };
+#else
   template <typename S>
   struct cast_helper {
     static base::AtomicWord to_storage_type(S value) {
@@ -56,8 +81,13 @@ class AtomicValue {
       return reinterpret_cast<S*>(value);
     }
   };
+#endif
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+  base::AtomicIntPtr value_;
+#else
   base::AtomicWord value_;
+#endif
 };
 
 // Provides atomic operations for a values stored at some address.
@@ -201,6 +231,9 @@ using AsAtomic8 = AsAtomicImpl<base::Atomic8>;
 using AsAtomic16 = AsAtomicImpl<base::Atomic16>;
 using AsAtomic32 = AsAtomicImpl<base::Atomic32>;
 using AsAtomicWord = AsAtomicImpl<base::AtomicWord>;
+#if defined(__CHERI_PURE_CAPABILITY__)
+using AsAtomicIntPtr = AsAtomicImpl<base::AtomicIntPtr>;
+#endif
 
 template <int Width>
 struct AtomicTypeFromByteWidth {};
@@ -221,6 +254,12 @@ template <>
 struct AtomicTypeFromByteWidth<8> {
   using type = base::Atomic64;
 };
+#if defined(__CHERI_PURE_CAPABILITY__)
+template <>
+struct AtomicTypeFromByteWidth<16> {
+  using type = base::AtomicIntPtr;
+};
+#endif
 #endif
 
 // This is similar to AsAtomicWord but it explicitly deletes functionality
@@ -232,7 +271,11 @@ class AsAtomicPointerImpl : public AsAtomicImpl<TAtomicStorageType> {
   static bool SetBits(T* addr, T bits, T mask) = delete;
 };
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+using AsAtomicPointer = AsAtomicPointerImpl<base::AtomicIntPtr>;
+#else
 using AsAtomicPointer = AsAtomicPointerImpl<base::AtomicWord>;
+#endif
 
 template <typename T,
           typename = typename std::enable_if<std::is_unsigned<T>::value>::type>
diff --git src/base/atomicops.h src/base/atomicops.h
index 8ffcfac2b83..96312947ce4 100644
--- src/base/atomicops.h
+++ src/base/atomicops.h
@@ -63,9 +63,12 @@ using Atomic32 = int32_t;
 #if defined(__ILP32__)
 using Atomic64 = int64_t;
 #else
-using Atomic64 = intptr_t;
+using Atomic64 = int64_t;
 #endif  // defined(__ILP32__)
-#endif  // defined(V8_HOST_ARCH_64_BIT)
+#if defined(__CHERI_PURE_CAPABILITY__)
+using AtomicIntPtr = intptr_t;
+#endif  // defined(__CHERI_PURE_CAPABILITY__)
+#endif  // defined(V8_HOST_ARCH_64_BIT) || defined(V8_HOST_ARCH_64_BITS)
 #endif  // V8_OS_STARBOARD
 
 // Use AtomicWord for a machine-sized pointer. It will use the Atomic32 or
@@ -75,7 +78,12 @@ using AtomicWord = Atomic64;
 #else
 using AtomicWord = Atomic32;
 #endif
+
+#if defined(__CHERI_PURE_CAPABILITY__)
+static_assert(sizeof(void*) == sizeof(AtomicIntPtr));
+#else
 static_assert(sizeof(void*) == sizeof(AtomicWord));
+#endif  // defined(__CHERI_PURE_CAPABILITY__)
 
 namespace helper {
 template <typename T>
@@ -282,9 +290,15 @@ inline Atomic64 SeqCst_AtomicExchange(volatile Atomic64* ptr,
 
 inline Atomic64 Relaxed_AtomicIncrement(volatile Atomic64* ptr,
                                         Atomic64 increment) {
+#ifdef V8_HOST_CHERI_PURE_CAPABILITY
+  return std::atomic_fetch_add_explicit(helper::to_std_atomic(ptr),
+                                        increment,
+                                        std::memory_order_relaxed) + (size_t) increment;
+#else
   return increment + std::atomic_fetch_add_explicit(helper::to_std_atomic(ptr),
                                                     increment,
                                                     std::memory_order_relaxed);
+#endif
 }
 
 inline Atomic64 Acquire_CompareAndSwap(volatile Atomic64* ptr,
@@ -342,6 +356,89 @@ inline Atomic64 SeqCst_Load(volatile const Atomic64* ptr) {
                                    std::memory_order_seq_cst);
 }
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+inline AtomicIntPtr Relaxed_CompareAndSwap(volatile AtomicIntPtr* ptr,
+                                           AtomicIntPtr old_value, AtomicIntPtr new_value) {
+  std::atomic_compare_exchange_strong_explicit(
+      helper::to_std_atomic(ptr), &old_value, new_value,
+      std::memory_order_relaxed, std::memory_order_relaxed);
+  return old_value;
+}
+
+inline AtomicIntPtr Relaxed_AtomicExchange(volatile AtomicIntPtr* ptr,
+                                           AtomicIntPtr new_value) {
+  return std::atomic_exchange_explicit(helper::to_std_atomic(ptr), new_value,
+                                       std::memory_order_relaxed);
+}
+
+inline AtomicIntPtr SeqCst_AtomicExchange(volatile AtomicIntPtr* ptr,
+                                          AtomicIntPtr new_value) {
+  return std::atomic_exchange_explicit(helper::to_std_atomic(ptr), new_value,
+                                       std::memory_order_seq_cst);
+}
+
+inline AtomicIntPtr Relaxed_AtomicIncrement(volatile AtomicIntPtr* ptr,
+                                            AtomicIntPtr increment) {
+  return std::atomic_fetch_add_explicit(helper::to_std_atomic(ptr),
+                                        increment,
+                                        std::memory_order_relaxed) + (size_t) increment;
+}
+
+inline AtomicIntPtr Acquire_CompareAndSwap(volatile AtomicIntPtr* ptr,
+                                           AtomicIntPtr old_value, AtomicIntPtr new_value) {
+  std::atomic_compare_exchange_strong_explicit(
+      helper::to_std_atomic(ptr), &old_value, new_value,
+      std::memory_order_acquire, std::memory_order_acquire);
+  return old_value;
+}
+
+inline AtomicIntPtr Release_CompareAndSwap(volatile AtomicIntPtr* ptr,
+                                           AtomicIntPtr old_value, AtomicIntPtr new_value) {
+  std::atomic_compare_exchange_strong_explicit(
+      helper::to_std_atomic(ptr), &old_value, new_value,
+      std::memory_order_release, std::memory_order_relaxed);
+  return old_value;
+}
+
+inline AtomicIntPtr AcquireRelease_CompareAndSwap(volatile AtomicIntPtr* ptr,
+                                                  AtomicIntPtr old_value,
+                                                  AtomicIntPtr new_value) {
+  std::atomic_compare_exchange_strong_explicit(
+      helper::to_std_atomic(ptr), &old_value, new_value,
+      std::memory_order_acq_rel, std::memory_order_acquire);
+  return old_value;
+}
+
+inline void Relaxed_Store(volatile AtomicIntPtr* ptr, AtomicIntPtr value) {
+  std::atomic_store_explicit(helper::to_std_atomic(ptr), value,
+                             std::memory_order_relaxed);
+}
+
+inline void Release_Store(volatile AtomicIntPtr* ptr, AtomicIntPtr value) {
+  std::atomic_store_explicit(helper::to_std_atomic(ptr), value,
+                             std::memory_order_release);
+}
+
+inline void SeqCst_Store(volatile AtomicIntPtr* ptr, AtomicIntPtr value) {
+  std::atomic_store_explicit(helper::to_std_atomic(ptr), value,
+                             std::memory_order_seq_cst);
+}
+
+inline AtomicIntPtr Relaxed_Load(volatile const AtomicIntPtr* ptr) {
+  return std::atomic_load_explicit(helper::to_std_atomic_const(ptr),
+                                   std::memory_order_relaxed);
+}
+
+inline AtomicIntPtr Acquire_Load(volatile const AtomicIntPtr* ptr) {
+  return std::atomic_load_explicit(helper::to_std_atomic_const(ptr),
+                                   std::memory_order_acquire);
+}
+
+inline AtomicIntPtr SeqCst_Load(volatile const AtomicIntPtr* ptr) {
+  return std::atomic_load_explicit(helper::to_std_atomic_const(ptr),
+                                   std::memory_order_seq_cst);
+}
+#endif  // defined(__CHERI_PURE_CAPABILITY__)
 #endif  // defined(V8_HOST_ARCH_64_BIT)
 
 inline void Relaxed_Memcpy(volatile Atomic8* dst, volatile const Atomic8* src,
diff --git src/base/bits.h src/base/bits.h
index 49d45570e95..899e34ed670 100644
--- src/base/bits.h
+++ src/base/bits.h
@@ -24,8 +24,12 @@ namespace bits {
 // CountPopulation(value) returns the number of bits set in |value|.
 template <typename T>
 constexpr inline
+#ifdef __CHERI_PURE_CAPABILITY__
+    typename std::enable_if<std::is_unsigned<T>::value && sizeof(T) <= 16, unsigned>::type
+#else
     typename std::enable_if<std::is_unsigned<T>::value && sizeof(T) <= 8,
                             unsigned>::type
+#endif
     CountPopulation(T value) {
   static_assert(sizeof(T) <= 8);
 #if V8_HAS_BUILTIN_POPCOUNT
@@ -57,6 +61,13 @@ constexpr inline
 #endif
 }
 
+#ifdef __CHERI_PURE_CAPABILITY__
+template<>
+constexpr inline unsigned CountPopulation(uintptr_t value) {
+  return CountPopulation<size_t>(static_cast<size_t>(value));
+}
+#endif
+
 // ReverseBits(value) returns |value| in reverse bit order.
 template <typename T>
 T ReverseBits(T value) {
@@ -109,25 +120,44 @@ inline constexpr unsigned CountLeadingZeros64(uint64_t value) {
 // returns {sizeof(T) * 8}.
 // See CountTrailingZerosNonZero for an optimized version for the case that
 // |value| is guaranteed to be non-zero.
-template <typename T, unsigned bits = sizeof(T) * 8>
+template <typename t, unsigned bits = sizeof(t) * 8>
 inline constexpr
-    typename std::enable_if<std::is_integral<T>::value && sizeof(T) <= 8,
+    typename std::enable_if<std::is_integral<t>::value,
                             unsigned>::type
-    CountTrailingZeros(T value) {
-#if V8_HAS_BUILTIN_CTZ
+    CountTrailingZeros(t value) {
+#if v8_has_builtin_ctz
   return value == 0 ? bits
                     : bits == 64 ? __builtin_ctzll(static_cast<uint64_t>(value))
                                  : __builtin_ctz(static_cast<uint32_t>(value));
 #else
-  // Fall back to popcount (see "Hacker's Delight" by Henry S. Warren, Jr.),
-  // chapter 5-4. On x64, since is faster than counting in a loop and faster
+  // fall back to popcount (see "hacker's delight" by henry s. warren, jr.),
+  // chapter 5-4. on x64, since is faster than counting in a loop and faster
   // than doing binary search.
-  using U = typename std::make_unsigned<T>::type;
+  using U = typename std::make_unsigned<t>::type;
   U u = value;
   return CountPopulation(static_cast<U>(~u & (u - 1u)));
 #endif
 }
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+template <>
+inline constexpr
+    typename std::enable_if<(sizeof(uintptr_t) > sizeof(intmax_t)) &&
+                            sizeof(intmax_t) == 8, unsigned>::type
+    CountTrailingZeros<uintptr_t>(uintptr_t value) {
+#if v8_has_builtin_ctz
+  return value == 0 ? sizeof(uint64_t) * 8 : __builtin_ctzll(static_cast<uint64_t>(value));
+#else
+  // fall back to popcount (see "hacker's delight" by henry s. warren, jr.),
+  // chapter 5-4. on x64, since is faster than counting in a loop and faster
+  // than doing binary search.
+  
+  uint64_t u = value;
+  return CountPopulation(static_cast<uint64_t>(~u & (u - 1)));
+#endif
+}
+#endif
+
 inline constexpr unsigned CountTrailingZeros32(uint32_t value) {
   return CountTrailingZeros(value);
 }
diff --git src/base/build_config.h src/base/build_config.h
index 3befde51e7f..80c7367c1b5 100644
--- src/base/build_config.h
+++ src/base/build_config.h
@@ -24,6 +24,9 @@
 #elif defined(__AARCH64EL__) || defined(_M_ARM64)
 #define V8_HOST_ARCH_ARM64 1
 #define V8_HOST_ARCH_64_BIT 1
+#ifdef __CHERI_PURE_CAPABILITY__
+#define V8_HOST_CHERI_PURE_CAPABILITY 1
+#endif
 #elif defined(__ARMEL__)
 #define V8_HOST_ARCH_ARM 1
 #define V8_HOST_ARCH_32_BIT 1
@@ -127,6 +130,9 @@
 #define V8_TARGET_ARCH_32_BIT 1
 #elif V8_TARGET_ARCH_ARM64
 #define V8_TARGET_ARCH_64_BIT 1
+#ifdef __CHERI_PURE_CAPABILITY__
+#define V8_TARGET_ARCH_CHERI_PURE_CAPABILITY 1
+#endif
 #elif V8_TARGET_ARCH_MIPS
 #define V8_TARGET_ARCH_32_BIT 1
 #elif V8_TARGET_ARCH_MIPS64
diff --git src/base/flags.h src/base/flags.h
index 2a36ca77e82..d4e3895ad58 100644
--- src/base/flags.h
+++ src/base/flags.h
@@ -89,6 +89,136 @@ class Flags final {
   mask_type mask_;
 };
 
+/*
+#if defined(__CHERI_PURE_CAPABILITY__)
+template <typename T>
+class Flags<T, intptr_t> final {
+ public:
+  using flag_type = T;
+  using mask_type = intptr_t;
+
+  constexpr Flags() : mask_(0) {}
+  constexpr Flags(flag_type flag)  // NOLINT(runtime/explicit)
+      : mask_(static_cast<size_t>(flag)) {}
+  constexpr explicit Flags(uintptr_t mask) : mask_(static_cast<size_t>(mask)) {}
+
+  constexpr bool operator==(flag_type flag) const {
+    return mask_ == static_cast<size_t>(flag);
+  }
+  constexpr bool operator!=(flag_type flag) const {
+    return mask_ != static_cast<size_t>(flag);
+  }
+
+  Flags& operator&=(const Flags& flags) {
+    mask_ &= flags.mask_;
+    return *this;
+  }
+  Flags& operator|=(const Flags& flags) {
+    mask_ |= (size_t) flags.mask_;
+    return *this;
+  }
+  Flags& operator^=(const Flags& flags) {
+    mask_ ^= flags.mask_;
+    return *this;
+  }
+
+  constexpr Flags operator&(const Flags& flags) const {
+    return Flags(mask_ & flags.mask_);
+  }
+  constexpr Flags operator|(const Flags& flags) const {
+    return Flags(mask_ | flags.mask_);
+  }
+  constexpr Flags operator^(const Flags& flags) const {
+    return Flags(mask_ ^ flags.mask_);
+  }
+
+  Flags& operator&=(flag_type flag) { return operator&=(Flags(flag)); }
+  Flags& operator|=(flag_type flag) { return operator|=(Flags(flag)); }
+  Flags& operator^=(flag_type flag) { return operator^=(Flags(flag)); }
+
+  constexpr Flags operator&(flag_type flag) const {
+    return operator&(Flags(flag));
+  }
+  constexpr Flags operator|(flag_type flag) const {
+    return operator|(Flags(flag));
+  }
+  constexpr Flags operator^(flag_type flag) const {
+    return operator^(Flags(flag));
+  }
+
+  constexpr Flags operator~() const { return Flags(~mask_); }
+
+  constexpr operator intptr_t() const { return (intptr_t) mask_; }
+  constexpr bool operator!() const { return !mask_; }
+
+  Flags without(flag_type flag) const { return *this & (~Flags(flag)); }
+
+  friend size_t hash_value(const Flags& flags) { return flags.mask_; }
+
+ private:
+  size_t mask_;
+};
+  
+template <typename T>
+class Flags<T, uintptr_t> final {
+ public:
+  using flag_type = T;
+  using mask_type = uintptr_t;
+
+  constexpr Flags() : mask_(0) {}
+  constexpr Flags(flag_type flag)  // NOLINT(runtime/explicit)
+      : mask_(static_cast<size_t>(flag)) {}
+  constexpr explicit Flags(uintptr_t mask) : mask_(static_cast<size_t>(mask)) {}
+
+  constexpr bool operator==(flag_type flag) const {
+    return mask_ == static_cast<size_t>(flag);
+  }
+  constexpr bool operator!=(flag_type flag) const {
+    return mask_ != static_cast<size_t>(flag);
+  }
+
+  Flags& operator&=(const Flags& flags) {
+    mask_ &= flags.mask_;
+    return *this;
+  }
+  Flags& operator|=(const Flags& flags) {
+    mask_ |= flags.mask_;
+    return *this;
+  }
+  Flags& operator^=(const Flags& flags) {
+    mask_ ^= flags.mask_;
+    return *this;
+  }
+
+  Flags& operator&=(flag_type flag) { return operator&=(Flags(flag)); }
+  Flags& operator|=(flag_type flag) { return operator|=(Flags(flag)); }
+  Flags& operator^=(flag_type flag) { return operator^=(Flags(flag)); }
+
+  constexpr Flags operator&(flag_type flag) const {
+    return operator&(Flags(flag));
+  }
+  constexpr Flags operator|(flag_type flag) const {
+    return operator|(Flags(flag));
+  }
+  constexpr Flags operator^(flag_type flag) const {
+    return operator^(Flags(flag));
+  }
+
+  constexpr Flags operator~() const { return Flags(~mask_); }
+
+  constexpr operator uintptr_t() const { return (uintptr_t) mask_; }
+  constexpr bool operator!() const { return !mask_; }
+
+  Flags without(flag_type flag) const { return *this & (~Flags(flag)); }
+
+  friend size_t hash_value(const Flags& flags) { return flags.mask_; }
+
+ private:
+  size_t mask_;
+};
+#endif
+*/
+
 #define DEFINE_OPERATORS_FOR_FLAGS(Type)                                 \
   V8_ALLOW_UNUSED V8_WARN_UNUSED_RESULT inline constexpr Type operator&( \
       Type::flag_type lhs, Type::flag_type rhs) {                        \
diff --git src/base/functional.h src/base/functional.h
index 04d646ccbab..4d11db25cce 100644
--- src/base/functional.h
+++ src/base/functional.h
@@ -118,6 +118,16 @@ V8_INLINE size_t hash_value(double v) {
   return v != 0.0 ? hash_value(base::bit_cast<uint64_t>(v)) : 0;
 }
 
+#ifdef __CHERI_PURE_CAPABILITY__
+V8_INLINE size_t hash_value(intptr_t v) {
+  return hash_value(static_cast<size_t>(v));
+}
+
+V8_INLINE size_t hash_value(uintptr_t v) {
+  return hash_value(static_cast<size_t>(v));
+}
+#endif
+
 template <typename T, size_t N>
 V8_INLINE size_t hash_value(const T (&v)[N]) {
   return hash_range(v, v + N);
diff --git src/base/logging.h src/base/logging.h
index 08db24a947e..4c2e69f339f 100644
--- src/base/logging.h
+++ src/base/logging.h
@@ -14,6 +14,7 @@
 #include "src/base/compiler-specific.h"
 #include "src/base/immediate-crash.h"
 #include "src/base/template-utils.h"
+#include "src/common/cheri.h"
 
 V8_BASE_EXPORT V8_NOINLINE void V8_Dcheck(const char* file, int line,
                                           const char* message);
@@ -135,6 +136,7 @@ V8_BASE_EXPORT void SetDcheckFunction(void (*dcheck_Function)(const char*, int,
 #endif
 
 namespace detail {
+using cheri::operator<<;
 template <typename... Ts>
 std::string PrintToString(Ts&&... ts) {
   CheckMessageStream oss;
diff --git src/base/macros.h src/base/macros.h
index 3a94093a14b..a55970c55e9 100644
--- src/base/macros.h
+++ src/base/macros.h
@@ -11,6 +11,7 @@
 #include "src/base/compiler-specific.h"
 #include "src/base/logging.h"
 #include "src/base/platform/wrappers.h"
+#include "src/common/cheri.h"
 
 // No-op macro which is used to work around MSVC's funky VA_ARGS support.
 #define EXPAND(x) x
@@ -322,15 +323,30 @@ inline uint64_t make_uint64(uint32_t high, uint32_t low) {
 template <typename T>
 inline T RoundDown(T x, intptr_t m) {
   static_assert(std::is_integral<T>::value);
-  // m must be a power of two.
+  // m must be aç power of two.
+#ifdef __CHERI_PURE_CAPABILITY__
+  DCHECK(m != 0 && ((m & static_cast<size_t>(m - 1)) == 0));
+  if constexpr(cheri::is_intcap<T>::value) {
+    return x & static_cast<size_t>(-m);
+  }
+#else
   DCHECK(m != 0 && ((m & (m - 1)) == 0));
+#endif
   return x & static_cast<T>(-m);
 }
+
 template <intptr_t m, typename T>
 constexpr inline T RoundDown(T x) {
   static_assert(std::is_integral<T>::value);
-  // m must be a power of two.
-  static_assert(m != 0 && ((m & (m - 1)) == 0));
+  // m must be a power0 of two.
+#ifdef __CHERI_PURE_CAPABILITY__
+  DCHECK(m != 0 && ((m & static_cast<size_t>(m - 1)) == 0));
+  if constexpr(cheri::is_intcap<T>::value) {
+    return x & static_cast<size_t>(-m);
+  }
+#else
+  DCHECK(m != 0 && ((m & (m - 1)) == 0));
+#endif
   return x & static_cast<T>(-m);
 }
 
@@ -340,7 +356,12 @@ inline T RoundUp(T x, intptr_t m) {
   static_assert(std::is_integral<T>::value);
   DCHECK_GE(x, 0);
   DCHECK_GE(std::numeric_limits<T>::max() - x, m - 1);  // Overflow check.
-  return RoundDown<T>(static_cast<T>(x + (m - 1)), m);
+#ifdef __CHERI_PURE_CAPABILITY__
+  if constexpr(cheri::is_intcap<T>::value) {
+    return RoundDown<T>(x + static_cast<size_t>(m - 1), m);
+  }
+#endif
+  return RoundDown<T>(x + (m - 1), m);
 }
 
 template <intptr_t m, typename T>
@@ -348,14 +369,55 @@ constexpr inline T RoundUp(T x) {
   static_assert(std::is_integral<T>::value);
   DCHECK_GE(x, 0);
   DCHECK_GE(std::numeric_limits<T>::max() - x, m - 1);  // Overflow check.
-  return RoundDown<m, T>(static_cast<T>(x + (m - 1)));
+#ifdef __CHERI_PURE_CAPABILITY__
+  if constexpr(cheri::is_intcap<T>::value) {
+    return RoundDown<m, T>(x + static_cast<size_t>(m - 1));
+  }
+#endif
+  return RoundDown<m, T>(x + (m - 1));
+}
+
+#ifdef __CHERI_PURE_CAPABILITY__FIX
+template <typename T>
+inline T RoundDown(T x, intptr_t m) {
+  // m must be a power of two.
+  DCHECK(m != 0 && ((m & static_cast<size_t>(m - 1)) == 0));
+  return x & static_cast<size_t>(-m);
 }
 
+template <intptr_t m, typename T>
+constexpr inline RoundDown(T x) {
+  DCHECK(m != 0 && ((m & static_cast<size_t>(m - 1)) == 0));
+  return x & static_cast<size_t>(-m);
+}
+
+template <typename T>
+inline typename RoundUp(T x, intptr_t m) {
+  DCHECK_GE(x, 0);
+  DCHECK_GE(std::numeric_limits<T>::max() - static_cast<size_t>(x), m - 1);  // Overflow check.
+  return RoundDown<T>(x + static_cast<size_t>(m - 1), m);
+}
+
+template <intptr_t m, typename T>
+constexpr inline RoundUp(intptr_t x) {
+  DCHECK_GE(x, 0);
+  DCHECK_GE(std::numeric_limits<T>::max() - static_cast<size_t>(x), m - 1);  // Overflow check.
+  return RoundDown<m, T>(x + static_cast<size_t>(m - 1));
+}
+#endif
+
 template <typename T, typename U>
 constexpr inline bool IsAligned(T value, U alignment) {
   return (value & (alignment - 1)) == 0;
 }
 
+#ifdef __CHERI_PURE_CAPABILITY__
+template <>
+constexpr inline bool IsAligned(uintptr_t value, uintptr_t alignment) {
+  return IsAligned<uintptr_t, size_t>(value, static_cast<size_t>(alignment));
+}
+#endif
+
 inline void* AlignedAddress(void* address, size_t alignment) {
   // The alignment must be a power of two.
   DCHECK_EQ(alignment & (alignment - 1), 0u);
@@ -435,4 +497,10 @@ bool is_inbounds(float_t v) {
 #define FRIEND_TEST(test_case_name, test_name)
 #endif
 
+#if defined(V8_TARGET_ARCH_CHERI_PURE_CAPABILITY)
+#define V8_CHERI_NO_PROVENANCE __attribute__((cheri_no_provenance))
+#else
+#define V8_CHERI_NO_PROVENANCE
+#endif
+
 #endif  // V8_BASE_MACROS_H_
diff --git src/base/page-allocator.cc src/base/page-allocator.cc
index d6da1abc343..d6e4a93b8e7 100644
--- src/base/page-allocator.cc
+++ src/base/page-allocator.cc
@@ -46,12 +46,14 @@ void* PageAllocator::GetRandomMmapAddr() {
 void* PageAllocator::AllocatePages(void* hint, size_t size, size_t alignment,
                                    PageAllocator::Permission access) {
 #if !V8_HAS_PTHREAD_JIT_WRITE_PROTECT
+#if !defined(__CHERI_PURE_CAPABILITY__)
   // kNoAccessWillJitLater is only used on Apple Silicon. Map it to regular
   // kNoAccess on other platforms, so code doesn't have to handle both enum
   // values.
   if (access == PageAllocator::kNoAccessWillJitLater) {
     access = PageAllocator::kNoAccess;
   }
+#endif
 #endif
   return base::OS::Allocate(hint, size, alignment,
                             static_cast<base::OS::MemoryPermission>(access));
diff --git src/base/platform/platform-posix.cc src/base/platform/platform-posix.cc
index 131ff9614e7..ac4330b7933 100644
--- src/base/platform/platform-posix.cc
+++ src/base/platform/platform-posix.cc
@@ -182,12 +182,22 @@ void* Allocate(void* hint, size_t size, OS::MemoryPermission access,
 int GetProtectionFromMemoryPermission(OS::MemoryPermission access) {
   switch (access) {
     case OS::MemoryPermission::kNoAccess:
+#if !defined(__CHERI_PURE_CAPABILITY__)
     case OS::MemoryPermission::kNoAccessWillJitLater:
+#endif
       return PROT_NONE;
+#if defined(__CHERI_PURE_CAPABILITY__)
+    case OS::MemoryPermission::kNoAccessWillJitLater:
+      return PROT_EXEC | PROT_READ | PROT_WRITE;
+#endif
     case OS::MemoryPermission::kRead:
       return PROT_READ;
     case OS::MemoryPermission::kReadWrite:
+#if defined(__CHERI_PURE_CAPABILITY__)
+      return PROT_EXEC | PROT_READ | PROT_WRITE;
+#else
       return PROT_READ | PROT_WRITE;
+#endif
     case OS::MemoryPermission::kReadWriteExecute:
       return PROT_READ | PROT_WRITE | PROT_EXEC;
     case OS::MemoryPermission::kReadExecute:
@@ -299,7 +309,13 @@ void* OS::GetRandomMmapAddr() {
   uintptr_t raw_addr;
   {
     MutexGuard guard(rng_mutex.Pointer());
+#if defined(__CHERI_PURE_CAPABILITY__)
+    ptraddr_t raw_ptr_addr;
+    GetPlatformRandomNumberGenerator()->NextBytes(&raw_ptr_addr, sizeof(raw_ptr_addr));
+    raw_addr = raw_ptr_addr;
+#else
     GetPlatformRandomNumberGenerator()->NextBytes(&raw_addr, sizeof(raw_addr));
+#endif
   }
 #if V8_HOST_ARCH_ARM64
 #if defined(V8_TARGET_OS_MACOS)
@@ -574,8 +590,15 @@ bool OS::CanReserveAddressSpace() { return true; }
 Optional<AddressSpaceReservation> OS::CreateAddressSpaceReservation(
     void* hint, size_t size, size_t alignment,
     MemoryPermission max_permission) {
-  // On POSIX, address space reservations are backed by private memory mappings.
   MemoryPermission permission = MemoryPermission::kNoAccess;
+#if defined(__CHERI_PURE_CAPABILITY__)
+  if (max_permission == MemoryPermission::kReadWrite) {
+    permission = MemoryPermission::kReadWrite;
+  }
+#else
+  // On POSIX, address space reservations are backed by private memory mappings.
+  permission = MemoryPermission::kNoAccess;
+#endif
   if (max_permission == MemoryPermission::kReadWriteExecute) {
     permission = MemoryPermission::kNoAccessWillJitLater;
   }
diff --git src/base/pointer-with-payload.h src/base/pointer-with-payload.h
index 94801a9af75..22de04b3de6 100644
--- src/base/pointer-with-payload.h
+++ src/base/pointer-with-payload.h
@@ -58,7 +58,7 @@ class PointerWithPayload {
   }
 
   V8_INLINE PointerType* GetPointer() const {
-    return reinterpret_cast<PointerType*>(pointer_with_payload_ & kPointerMask);
+    return reinterpret_cast<PointerType*>(pointer_with_payload_ & (size_t) kPointerMask);
   }
 
   // An optimized version of GetPointer for when we know the payload value.
@@ -89,7 +89,11 @@ class PointerWithPayload {
   }
 
   V8_INLINE void SetPayload(PayloadType new_payload) {
+#ifdef __CHERI_PURE_CAPABILITY__
+   __attribute__((cheri_no_provenance)) uintptr_t new_payload_ptr = static_cast<uintptr_t>(new_payload);
+#else
     uintptr_t new_payload_ptr = static_cast<uintptr_t>(new_payload);
+#endif
     DCHECK_EQ(new_payload_ptr & kPayloadMask, new_payload_ptr);
     pointer_with_payload_ =
         (pointer_with_payload_ & kPointerMask) | new_payload_ptr;
@@ -105,9 +109,13 @@ class PointerWithPayload {
       "storage bits. Override PointerWithPayloadTraits to guarantee available "
       "bits manually.");
 
-  static constexpr uintptr_t kPayloadMask =
+  V8_CHERI_NO_PROVENANCE static constexpr uintptr_t kPayloadMask =
       (uintptr_t{1} << NumPayloadBits) - 1;
+#ifdef __CHERI_PURE_CAPABILITY__
+  __attribute__((cheri_no_provenance)) static constexpr uintptr_t kPointerMask = ~kPayloadMask;
+#else
   static constexpr uintptr_t kPointerMask = ~kPayloadMask;
+#endif
 
   uintptr_t pointer_with_payload_ = 0;
 };
diff --git src/base/region-allocator.cc src/base/region-allocator.cc
index d4d443cacf9..e0b8f6d4d2a 100644
--- src/base/region-allocator.cc
+++ src/base/region-allocator.cc
@@ -350,14 +350,14 @@ const char* RegionStateToString(RegionAllocator::RegionState state) {
 
 void RegionAllocator::Region::Print(std::ostream& os) const {
   std::ios::fmtflags flags = os.flags(std::ios::hex | std::ios::showbase);
-  os << "[" << begin() << ", " << end() << "), size: " << size();
+  os << "[" << (size_t) begin() << ", " << (size_t) end() << "), size: " << size();
   os << ", " << RegionStateToString(state_);
   os.flags(flags);
 }
 
 void RegionAllocator::Print(std::ostream& os) const {
   std::ios::fmtflags flags = os.flags(std::ios::hex | std::ios::showbase);
-  os << "RegionAllocator: [" << begin() << ", " << end() << ")";
+  os << "RegionAllocator: [" << (size_t) begin() << ", " << (size_t) end() << ")";
   os << "\nsize: " << size();
   os << "\nfree_size: " << free_size();
   os << "\npage_size: " << page_size_;
diff --git src/baseline/arm64/baseline-assembler-arm64-inl.h src/baseline/arm64/baseline-assembler-arm64-inl.h
index e5a7477cb07..2198a267cdb 100644
--- src/baseline/arm64/baseline-assembler-arm64-inl.h
+++ src/baseline/arm64/baseline-assembler-arm64-inl.h
@@ -242,7 +242,11 @@ void BaselineAssembler::Move(interpreter::Register output, Register source) {
   Move(RegisterFrameOperand(output), source);
 }
 void BaselineAssembler::Move(Register output, TaggedIndex value) {
+#ifdef __CHERI_PURE_CAPABILITY__
+  __ Mov(output, Immediate(static_cast<size_t>(value.ptr())));
+#else
   __ Mov(output, Immediate(value.ptr()));
+#endif 
 }
 void BaselineAssembler::Move(MemOperand output, Register source) {
   __ Str(source, output);
diff --git src/bigint/bigint.h src/bigint/bigint.h
index f3669527620..db74f34350b 100644
--- src/bigint/bigint.h
+++ src/bigint/bigint.h
@@ -31,15 +31,19 @@ extern bool kAdvancedAlgorithmsEnabledInLibrary;
 #endif
 
 // The type of a digit: a register-width unsigned integer.
-using digit_t = uintptr_t;
-using signed_digit_t = intptr_t;
+//using digit_t = uintptr_t;
+//using signed_digit_t = intptr_t;
 #if UINTPTR_MAX == 0xFFFFFFFF
 // 32-bit platform.
+using digit_t = uint32_t;
+using signed_digit_t = int32_t;
 using twodigit_t = uint64_t;
 #define HAVE_TWODIGIT_T 1
 static constexpr int kLog2DigitBits = 5;
 #elif UINTPTR_MAX == 0xFFFFFFFFFFFFFFFF
 // 64-bit platform.
+using digit_t = uint64_t;
+using signed_digit_t = int64_t;
 static constexpr int kLog2DigitBits = 6;
 #if defined(__SIZEOF_INT128__)
 using twodigit_t = __uint128_t;
diff --git src/bigint/digit-arithmetic.h src/bigint/digit-arithmetic.h
index d9113efc91e..81278cd5f82 100644
--- src/bigint/digit-arithmetic.h
+++ src/bigint/digit-arithmetic.h
@@ -157,8 +157,9 @@ static inline digit_t digit_div(digit_t high, digit_t low, digit_t divisor,
   // {s} can be 0. {low >> kDigitBits} would be undefined behavior, so
   // we mask the shift amount with {kShiftMask}, and the result with
   // {s_zero_mask} which is 0 if s == 0 and all 1-bits otherwise.
-  static_assert(sizeof(intptr_t) == sizeof(digit_t),
-                "intptr_t and digit_t must have the same size");
+  // TODO: how to handle use of intptr_t in bigint
+  //static_assert(sizeof(intptr_t) == sizeof(digit_t),
+  //              "intptr_t and digit_t must have the same size");
   const int kShiftMask = kDigitBits - 1;
   digit_t s_zero_mask =
       static_cast<digit_t>(static_cast<intptr_t>(-s) >> (kDigitBits - 1));
diff --git src/builtins/arm64/builtins-arm64.cc src/builtins/arm64/builtins-arm64.cc
index b8a305061a8..4b45a7e307d 100644
--- src/builtins/arm64/builtins-arm64.cc
+++ src/builtins/arm64/builtins-arm64.cc
@@ -144,10 +144,10 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
       __ SlotAddress(dst, 0);
       // Poke the hole (receiver).
       __ Str(x4, MemOperand(dst));
-      __ Add(dst, dst, kSystemPointerSize);  // Skip receiver.
+      __ Add(dst, dst, kSystemPointerAddrSize);  // Skip receiver.
       __ Add(src, fp,
              StandardFrameConstants::kCallerSPOffset +
-                 kSystemPointerSize);  // Skip receiver.
+                 kSystemPointerAddrSize);  // Skip receiver.
       __ Sub(count, argc, kJSArgcReceiverSlots);
       __ CopyDoubleWords(dst, src, count);
     }
@@ -157,24 +157,24 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     //  --                           x1: constructor function
     //  --                           x3: new target
     // If argc is odd:
-    //  --     sp[0*kSystemPointerSize]: the hole (receiver)
-    //  --     sp[1*kSystemPointerSize]: argument 1
+    //  --     sp[0*kSystemPointerAddrSize]: the hole (receiver)
+    //  --     sp[1*kSystemPointerAddrSize]: argument 1
     //  --             ...
-    //  -- sp[(n-1)*kSystemPointerSize]: argument (n - 1)
-    //  -- sp[(n+0)*kSystemPointerSize]: argument n
-    //  -- sp[(n+1)*kSystemPointerSize]: padding
-    //  -- sp[(n+2)*kSystemPointerSize]: padding
-    //  -- sp[(n+3)*kSystemPointerSize]: number of arguments (tagged)
-    //  -- sp[(n+4)*kSystemPointerSize]: context (pushed by FrameScope)
+    //  -- sp[(n-1)*kSystemPointerAddrSize]: argument (n - 1)
+    //  -- sp[(n+0)*kSystemPointerAddrSize]: argument n
+    //  -- sp[(n+1)*kSystemPointerAddrSize]: padding
+    //  -- sp[(n+2)*kSystemPointerAddrSize]: padding
+    //  -- sp[(n+3)*kSystemPointerAddrSize]: number of arguments (tagged)
+    //  -- sp[(n+4)*kSystemPointerAddrSize]: context (pushed by FrameScope)
     // If argc is even:
-    //  --     sp[0*kSystemPointerSize]: the hole (receiver)
-    //  --     sp[1*kSystemPointerSize]: argument 1
+    //  --     sp[0*kSystemPointerAddrSize]: the hole (receiver)
+    //  --     sp[1*kSystemPointerAddrSize]: argument 1
     //  --             ...
-    //  -- sp[(n-1)*kSystemPointerSize]: argument (n - 1)
-    //  -- sp[(n+0)*kSystemPointerSize]: argument n
-    //  -- sp[(n+1)*kSystemPointerSize]: padding
-    //  -- sp[(n+2)*kSystemPointerSize]: number of arguments (tagged)
-    //  -- sp[(n+3)*kSystemPointerSize]: context (pushed by FrameScope)
+    //  -- sp[(n-1)*kSystemPointerAddrSize]: argument (n - 1)
+    //  -- sp[(n+0)*kSystemPointerAddrSize]: argument n
+    //  -- sp[(n+1)*kSystemPointerAddrSize]: padding
+    //  -- sp[(n+2)*kSystemPointerAddrSize]: number of arguments (tagged)
+    //  -- sp[(n+3)*kSystemPointerAddrSize]: context (pushed by FrameScope)
     // -----------------------------------
 
     // Call the function.
@@ -233,11 +233,11 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   __ Push(x0, x1, padreg, x3);
 
   // ----------- S t a t e -------------
-  //  --        sp[0*kSystemPointerSize]: new target
-  //  --        sp[1*kSystemPointerSize]: padding
-  //  -- x1 and sp[2*kSystemPointerSize]: constructor function
-  //  --        sp[3*kSystemPointerSize]: number of arguments (tagged)
-  //  --        sp[4*kSystemPointerSize]: context (pushed by FrameScope)
+  //  --        sp[0*kSystemPointerAddrSize]: new target
+  //  --        sp[1*kSystemPointerAddrSize]: padding
+  //  -- x1 and sp[2*kSystemPointerAddrSize]: constructor function
+  //  --        sp[3*kSystemPointerAddrSize]: number of arguments (tagged)
+  //  --        sp[4*kSystemPointerAddrSize]: context (pushed by FrameScope)
   // -----------------------------------
 
   __ LoadTaggedPointerField(
@@ -260,11 +260,11 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
   // ----------- S t a t e -------------
   //  --                                x0: receiver
-  //  -- Slot 4 / sp[0*kSystemPointerSize]: new target
-  //  -- Slot 3 / sp[1*kSystemPointerSize]: padding
-  //  -- Slot 2 / sp[2*kSystemPointerSize]: constructor function
-  //  -- Slot 1 / sp[3*kSystemPointerSize]: number of arguments (tagged)
-  //  -- Slot 0 / sp[4*kSystemPointerSize]: context
+  //  -- Slot 4 / sp[0*kSystemPointerAddrSize]: new target
+  //  -- Slot 3 / sp[1*kSystemPointerAddrSize]: padding
+  //  -- Slot 2 / sp[2*kSystemPointerAddrSize]: constructor function
+  //  -- Slot 1 / sp[3*kSystemPointerAddrSize]: number of arguments (tagged)
+  //  -- Slot 0 / sp[4*kSystemPointerAddrSize]: context
   // -----------------------------------
   // Deoptimizer enters here.
   masm->isolate()->heap()->SetConstructStubCreateDeoptPCOffset(
@@ -273,7 +273,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   __ Bind(&post_instantiation_deopt_entry);
 
   // Restore new target from the top of the stack.
-  __ Peek(x3, 0 * kSystemPointerSize);
+  __ Peek(x3, 0 * kSystemPointerAddrSize);
 
   // Restore constructor function and argument count.
   __ Ldr(x1, MemOperand(fp, ConstructFrameConstants::kConstructorOffset));
@@ -297,14 +297,14 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  --                              x3: new target
   //  --                             x12: number of arguments (untagged)
-  //  --        sp[0*kSystemPointerSize]: implicit receiver (overwrite if argc
+  //  --        sp[0*kSystemPointerAddrSize]: implicit receiver (overwrite if argc
   //  odd)
-  //  --        sp[1*kSystemPointerSize]: implicit receiver
-  //  --        sp[2*kSystemPointerSize]: implicit receiver
-  //  --        sp[3*kSystemPointerSize]: padding
-  //  -- x1 and sp[4*kSystemPointerSize]: constructor function
-  //  --        sp[5*kSystemPointerSize]: number of arguments (tagged)
-  //  --        sp[6*kSystemPointerSize]: context
+  //  --        sp[1*kSystemPointerAddrSize]: implicit receiver
+  //  --        sp[2*kSystemPointerAddrSize]: implicit receiver
+  //  --        sp[3*kSystemPointerAddrSize]: padding
+  //  -- x1 and sp[4*kSystemPointerAddrSize]: constructor function
+  //  --        sp[5*kSystemPointerAddrSize]: number of arguments (tagged)
+  //  --        sp[6*kSystemPointerAddrSize]: context
   // -----------------------------------
 
   // Round the number of arguments down to the next even number, and claim
@@ -333,7 +333,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     __ Poke(x0, 0);          // Add the receiver.
     __ SlotAddress(dst, 1);  // Skip receiver.
     __ Add(src, fp,
-           StandardFrameConstants::kCallerSPOffset + kSystemPointerSize);
+           StandardFrameConstants::kCallerSPOffset + kSystemPointerAddrSize);
     __ CopyDoubleWords(dst, src, count);
   }
 
@@ -342,11 +342,11 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   __ InvokeFunctionWithNewTarget(x1, x3, x0, InvokeType::kCall);
 
   // ----------- S t a t e -------------
-  //  -- sp[0*kSystemPointerSize]: implicit receiver
-  //  -- sp[1*kSystemPointerSize]: padding
-  //  -- sp[2*kSystemPointerSize]: constructor function
-  //  -- sp[3*kSystemPointerSize]: number of arguments
-  //  -- sp[4*kSystemPointerSize]: context
+  //  -- sp[0*kSystemPointerAddrSize]: implicit receiver
+  //  -- sp[1*kSystemPointerAddrSize]: padding
+  //  -- sp[2*kSystemPointerAddrSize]: constructor function
+  //  -- sp[3*kSystemPointerAddrSize]: number of arguments
+  //  -- sp[4*kSystemPointerAddrSize]: context
   // -----------------------------------
 
   // Store offset of return address for deoptimizer.
@@ -365,7 +365,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // Throw away the result of the constructor invocation and use the
   // on-stack receiver as the result.
   __ Bind(&use_receiver);
-  __ Peek(x0, 0 * kSystemPointerSize);
+  __ Peek(x0, 0 * kSystemPointerAddrSize);
   __ CompareRoot(x0, RootIndex::kTheHoleValue);
   __ B(eq, &do_throw);
 
@@ -517,7 +517,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
 
   // Store padding (which might be replaced by the last argument).
   __ Sub(x11, x11, 1);
-  __ Poke(padreg, Operand(x11, LSL, kSystemPointerSizeLog2));
+  __ Poke(padreg, Operand(x11, LSL, kSystemPointerAddrSizeLog2));
 
   // Poke receiver into highest claimed slot.
   __ LoadTaggedPointerField(
@@ -546,7 +546,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     __ Bind(&loop);
     __ Sub(x10, x10, 1);
     __ LoadAnyTaggedField(x11, MemOperand(x5, -kTaggedSize, PreIndex));
-    __ Str(x11, MemOperand(x12, -kSystemPointerSize, PostIndex));
+    __ Str(x11, MemOperand(x12, -kSystemPointerAddrSize, PostIndex));
     __ Cbnz(x10, &loop);
     __ Bind(&done);
   }
@@ -790,7 +790,7 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
   __ Call(trampoline_code, RelocInfo::CODE_TARGET);
 
   // Pop the stack handler and unlink this frame from the handler chain.
-  static_assert(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize,
+  static_assert(StackHandlerConstants::kNextOffset == 0 * kSystemPointerAddrSize,
                 "Unexpected offset for StackHandlerConstants::kNextOffset");
   __ Pop(x10, padreg);
   __ Mov(x11, ExternalReference::Create(IsolateAddressId::kHandlerAddress,
@@ -829,9 +829,9 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
 
   // Reset the stack to the callee saved registers.
   static_assert(
-      EntryFrameConstants::kFixedFrameSize % (2 * kSystemPointerSize) == 0,
+      EntryFrameConstants::kFixedFrameSize % (2 * kSystemPointerAddrSize) == 0,
       "Size of entry frame is not a multiple of 16 bytes");
-  __ Drop(EntryFrameConstants::kFixedFrameSize / kSystemPointerSize);
+  __ Drop(EntryFrameConstants::kFixedFrameSize / kSystemPointerAddrSize);
   // Restore the callee-saved registers and return.
   __ PopCalleeSavedRegisters();
   __ Ret();
@@ -900,7 +900,7 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
 
     // Store padding (which might be overwritten).
     __ SlotAddress(scratch, slots_to_claim);
-    __ Str(padreg, MemOperand(scratch, -kSystemPointerSize));
+    __ Str(padreg, MemOperand(scratch, -kSystemPointerAddrSize));
 
     // Store receiver on the stack.
     __ Poke(receiver, 0);
@@ -922,11 +922,11 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     __ SlotAddress(x0, 1);  // Skips receiver.
     __ Bind(&loop);
     // Load the handle.
-    __ Ldr(x11, MemOperand(argv, kSystemPointerSize, PostIndex));
+    __ Ldr(x11, MemOperand(argv, kSystemPointerAddrSize, PostIndex));
     // Dereference the handle.
     __ Ldr(x11, MemOperand(x11));
     // Poke the result into the stack.
-    __ Str(x11, MemOperand(x0, kSystemPointerSize, PostIndex));
+    __ Str(x11, MemOperand(x0, kSystemPointerAddrSize, PostIndex));
     // Loop if we've not reached the end of copy marker.
     __ Cmp(x0, scratch);
     __ B(lt, &loop);
@@ -1018,7 +1018,7 @@ static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,
   // Compute the size of the actual parameters + receiver (in bytes).
   __ Ldr(actual_params_size,
          MemOperand(fp, StandardFrameConstants::kArgCOffset));
-  __ lsl(actual_params_size, actual_params_size, kSystemPointerSizeLog2);
+  __ lsl(actual_params_size, actual_params_size, kSystemPointerAddrSizeLog2);
 
   // If actual is bigger than formal, then we should use it to free up the stack
   // arguments.
@@ -1033,10 +1033,10 @@ static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,
 
   // Drop receiver + arguments.
   if (FLAG_debug_code) {
-    __ Tst(params_size, kSystemPointerSize - 1);
+    __ Tst(params_size, kSystemPointerAddrSize - 1);
     __ Check(eq, AbortReason::kUnexpectedValue);
   }
-  __ Lsr(params_size, params_size, kSystemPointerSizeLog2);
+  __ Lsr(params_size, params_size, kSystemPointerAddrSizeLog2);
   __ DropArguments(params_size);
 }
 
@@ -1508,7 +1508,7 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
     // Note: there should always be at least one stack slot for the return
     // register in the register file.
     Label loop_header;
-    __ Lsr(x11, x11, kSystemPointerSizeLog2);
+    __ Lsr(x11, x11, kSystemPointerAddrSizeLog2);
     // Round down (since we already have an undefined in the stack) the number
     // of registers to a multiple of 2, to align the stack to 16 bytes.
     __ Bic(x11, x11, 1);
@@ -1524,7 +1524,7 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
                kInterpreterBytecodeArrayRegister,
                BytecodeArray::kIncomingNewTargetOrGeneratorRegisterOffset));
   __ Cbz(x10, &no_incoming_new_target_or_generator_register);
-  __ Str(x3, MemOperand(fp, x10, LSL, kSystemPointerSizeLog2));
+  __ Str(x3, MemOperand(fp, x10, LSL, kSystemPointerAddrSizeLog2));
   __ Bind(&no_incoming_new_target_or_generator_register);
 
   // Perform interrupt stack check.
@@ -1546,7 +1546,7 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
       ExternalReference::interpreter_dispatch_table_address(masm->isolate()));
   __ Ldrb(x23, MemOperand(kInterpreterBytecodeArrayRegister,
                           kInterpreterBytecodeOffsetRegister));
-  __ Mov(x1, Operand(x23, LSL, kSystemPointerSizeLog2));
+  __ Mov(x1, Operand(x23, LSL, kSystemPointerAddrSizeLog2));
   __ Ldr(kJavaScriptCallCodeStartRegister,
          MemOperand(kInterpreterDispatchTableRegister, x1));
   __ Call(kJavaScriptCallCodeStartRegister);
@@ -1688,7 +1688,7 @@ static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
     UseScratchRegisterScope temps(masm);
     Register scratch = temps.AcquireX();
     __ Sub(scratch, slots_to_claim, 1);
-    __ Poke(padreg, Operand(scratch, LSL, kSystemPointerSizeLog2));
+    __ Poke(padreg, Operand(scratch, LSL, kSystemPointerAddrSizeLog2));
   }
 
   const bool skip_receiver =
@@ -1701,12 +1701,12 @@ static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
   __ SlotAddress(stack_addr, skip_receiver ? 1 : 0);
 
   __ Sub(last_arg_addr, first_arg_index,
-         Operand(slots_to_copy, LSL, kSystemPointerSizeLog2));
-  __ Add(last_arg_addr, last_arg_addr, kSystemPointerSize);
+         Operand(slots_to_copy, LSL, kSystemPointerAddrSizeLog2));
+  __ Add(last_arg_addr, last_arg_addr, kSystemPointerAddrSize);
 
   // Load the final spread argument into spread_arg_out, if necessary.
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
-    __ Ldr(spread_arg_out, MemOperand(last_arg_addr, -kSystemPointerSize));
+    __ Ldr(spread_arg_out, MemOperand(last_arg_addr, -kSystemPointerAddrSize));
   }
 
   __ CopyDoubleWords(stack_addr, last_arg_addr, slots_to_copy,
@@ -1835,7 +1835,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
   // Dispatch to the target bytecode.
   __ Ldrb(x23, MemOperand(kInterpreterBytecodeArrayRegister,
                           kInterpreterBytecodeOffsetRegister));
-  __ Mov(x1, Operand(x23, LSL, kSystemPointerSizeLog2));
+  __ Mov(x1, Operand(x23, LSL, kSystemPointerAddrSizeLog2));
   __ Ldr(kJavaScriptCallCodeStartRegister,
          MemOperand(kInterpreterDispatchTableRegister, x1));
 
@@ -1948,7 +1948,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
                    (allocatable_register_count +
                     BuiltinContinuationFrameConstants::PaddingSlotCount(
                         allocatable_register_count)) *
-                       kSystemPointerSize;
+                       kSystemPointerAddrSize;
 
   UseScratchRegisterScope temps(masm);
   Register scratch = temps.AcquireX();  // Temp register is not allocatable.
@@ -1969,14 +1969,14 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
 
   // Restore registers in pairs.
   int offset = -BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp -
-               allocatable_register_count * kSystemPointerSize;
+               allocatable_register_count * kSystemPointerAddrSize;
   for (int i = allocatable_register_count - 1; i > 0; i -= 2) {
     int code1 = config->GetAllocatableGeneralCode(i);
     int code2 = config->GetAllocatableGeneralCode(i - 1);
     Register reg1 = Register::from_code(code1);
     Register reg2 = Register::from_code(code2);
     __ Ldp(reg1, reg2, MemOperand(fp, offset));
-    offset += 2 * kSystemPointerSize;
+    offset += 2 * kSystemPointerAddrSize;
   }
 
   // Restore first register separately, if number of registers is odd.
@@ -1993,10 +1993,10 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     // from LAZY is always the last argument.
     constexpr int return_offset =
         BuiltinContinuationFrameConstants::kCallerSPOffset /
-            kSystemPointerSize -
+            kSystemPointerAddrSize -
         kJSArgcReceiverSlots;
     __ add(x0, x0, return_offset);
-    __ Str(scratch, MemOperand(fp, x0, LSL, kSystemPointerSizeLog2));
+    __ Str(scratch, MemOperand(fp, x0, LSL, kSystemPointerAddrSizeLog2));
     // Recover argument count.
     __ sub(x0, x0, return_offset);
   }
@@ -2193,9 +2193,9 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     __ Peek(receiver, 0);
     __ Cmp(argc, Immediate(JSParameterCount(1)));
     __ B(lt, &done);
-    __ Peek(this_arg, kSystemPointerSize);
+    __ Peek(this_arg, kSystemPointerAddrSize);
     __ B(eq, &done);
-    __ Peek(arg_array, 2 * kSystemPointerSize);
+    __ Peek(arg_array, 2 * kSystemPointerAddrSize);
     __ bind(&done);
   }
   __ DropArguments(argc, TurboAssembler::kCountIncludesReceiver);
@@ -2275,7 +2275,7 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
     // Shift arguments one slot down on the stack (overwriting the original
     // receiver).
     __ SlotAddress(copy_from, 1);
-    __ Sub(copy_to, copy_from, kSystemPointerSize);
+    __ Sub(copy_to, copy_from, kSystemPointerAddrSize);
     __ CopyDoubleWords(copy_to, copy_from, count);
     // Overwrite the duplicated remaining last argument.
     __ Poke(padreg, Operand(argc_without_receiver, LSL, kXRegSizeLog2));
@@ -2285,7 +2285,7 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
     // receiver and padding.
     __ Bind(&even);
     __ SlotAddress(copy_from, count);
-    __ Add(copy_to, copy_from, kSystemPointerSize);
+    __ Add(copy_to, copy_from, kSystemPointerAddrSize);
     __ CopyDoubleWords(copy_to, copy_from, count,
                        TurboAssembler::kSrcLessThanDst);
     __ Drop(2);
@@ -2327,12 +2327,12 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     __ Mov(arguments_list, undefined_value);
     __ Cmp(argc, Immediate(JSParameterCount(1)));
     __ B(lt, &done);
-    __ Peek(target, kSystemPointerSize);
+    __ Peek(target, kSystemPointerAddrSize);
     __ B(eq, &done);
-    __ Peek(this_argument, 2 * kSystemPointerSize);
+    __ Peek(this_argument, 2 * kSystemPointerAddrSize);
     __ Cmp(argc, Immediate(JSParameterCount(3)));
     __ B(lt, &done);
-    __ Peek(arguments_list, 3 * kSystemPointerSize);
+    __ Peek(arguments_list, 3 * kSystemPointerAddrSize);
     __ bind(&done);
   }
   __ DropArguments(argc, TurboAssembler::kCountIncludesReceiver);
@@ -2383,13 +2383,13 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     __ Mov(new_target, undefined_value);
     __ Cmp(argc, Immediate(JSParameterCount(1)));
     __ B(lt, &done);
-    __ Peek(target, kSystemPointerSize);
+    __ Peek(target, kSystemPointerAddrSize);
     __ B(eq, &done);
-    __ Peek(arguments_list, 2 * kSystemPointerSize);
+    __ Peek(arguments_list, 2 * kSystemPointerAddrSize);
     __ Mov(new_target, target);  // new.target defaults to target
     __ Cmp(argc, Immediate(JSParameterCount(3)));
     __ B(lt, &done);
-    __ Peek(new_target, 3 * kSystemPointerSize);
+    __ Peek(new_target, 3 * kSystemPointerAddrSize);
     __ bind(&done);
   }
 
@@ -2525,7 +2525,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ LoadAnyTaggedField(scratch, MemOperand(src, kTaggedSize, PostIndex));
     __ CmpTagged(scratch, the_hole_value);
     __ Csel(scratch, scratch, undefined_value, ne);
-    __ Str(scratch, MemOperand(dst, kSystemPointerSize, PostIndex));
+    __ Str(scratch, MemOperand(dst, kSystemPointerAddrSize, PostIndex));
     __ Cbnz(len, &loop);
   }
   __ Bind(&done);
@@ -2586,8 +2586,8 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
     Register dst = x13;
     // Point to the fist argument to copy from (skipping receiver).
     __ Add(args_fp, fp,
-           CommonFrameConstants::kFixedFrameSizeAboveFp + kSystemPointerSize);
-    __ lsl(start_index, start_index, kSystemPointerSizeLog2);
+           CommonFrameConstants::kFixedFrameSizeAboveFp + kSystemPointerAddrSize);
+    __ lsl(start_index, start_index, kSystemPointerAddrSizeLog2);
     __ Add(args_fp, args_fp, start_index);
     // Point to the position to copy to.
     __ SlotAddress(dst, argc);
@@ -2734,7 +2734,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
       // here which will cause x10 to become negative.
       __ Sub(x10, sp, x10);
       // Check if the arguments will overflow the stack.
-      __ Cmp(x10, Operand(bound_argc, LSL, kSystemPointerSizeLog2));
+      __ Cmp(x10, Operand(bound_argc, LSL, kSystemPointerAddrSizeLog2));
       __ B(gt, &done);
       __ TailCallRuntime(Runtime::kThrowStackOverflow);
       __ Bind(&done);
@@ -2753,7 +2753,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
     // Round up slots_to_claim to an even number if it is odd.
     __ Add(slots_to_claim, bound_argc, 1);
     __ Bic(slots_to_claim, slots_to_claim, 1);
-    __ Claim(slots_to_claim, kSystemPointerSize);
+    __ Claim(slots_to_claim, kSystemPointerAddrSize);
 
     __ Tbz(bound_argc, 0, &copy_bound_args);
     {
@@ -2768,12 +2768,12 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
         Register copy_from = x11;
         Register copy_to = x12;
         __ SlotAddress(copy_to, slots_to_claim);
-        __ Add(copy_from, copy_to, kSystemPointerSize);
+        __ Add(copy_from, copy_to, kSystemPointerAddrSize);
         __ CopyDoubleWords(copy_to, copy_from, argc);
       }
       // 2. Write a padding in the last slot.
       __ Add(scratch, total_argc, 1);
-      __ Str(padreg, MemOperand(sp, scratch, LSL, kSystemPointerSizeLog2));
+      __ Str(padreg, MemOperand(sp, scratch, LSL, kSystemPointerAddrSizeLog2));
       __ B(&copy_bound_args);
 
       __ Bind(&argc_even);
@@ -2788,7 +2788,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
         Register copy_from = x11;
         Register copy_to = x12;
         __ SlotAddress(copy_to, total_argc);
-        __ Sub(copy_from, copy_to, kSystemPointerSize);
+        __ Sub(copy_from, copy_to, kSystemPointerAddrSize);
         __ CopyDoubleWords(copy_to, copy_from, argc,
                            TurboAssembler::kSrcLessThanDst);
       }
@@ -2811,7 +2811,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
       __ Sub(counter, counter, 1);
       __ LoadAnyTaggedField(scratch,
                             MemOperand(bound_argv, kTaggedSize, PostIndex));
-      __ Str(scratch, MemOperand(copy_to, kSystemPointerSize, PostIndex));
+      __ Str(scratch, MemOperand(copy_to, kSystemPointerAddrSize, PostIndex));
       __ Cbnz(counter, &loop);
     }
     // Update argc.
@@ -3227,7 +3227,7 @@ void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,
   if (argv_mode == ArgvMode::kStack) {
     __ SlotAddress(temp_argv, x0);
     //  - Adjust for the receiver.
-    __ Sub(temp_argv, temp_argv, 1 * kSystemPointerSize);
+    __ Sub(temp_argv, temp_argv, 1 * kSystemPointerAddrSize);
   }
 
   // Reserve three slots to preserve x21-x23 callee-saved registers.
@@ -3239,9 +3239,9 @@ void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,
       builtin_exit_frame ? StackFrame::BUILTIN_EXIT : StackFrame::EXIT);
 
   // Poke callee-saved registers into reserved space.
-  __ Poke(argv, 1 * kSystemPointerSize);
-  __ Poke(argc, 2 * kSystemPointerSize);
-  __ Poke(target, 3 * kSystemPointerSize);
+  __ Poke(argv, 1 * kSystemPointerAddrSize);
+  __ Poke(argc, 2 * kSystemPointerAddrSize);
+  __ Poke(target, 3 * kSystemPointerAddrSize);
 
   // We normally only keep tagged values in callee-saved registers, as they
   // could be pushed onto the stack by called stubs and functions, and on the
@@ -3312,9 +3312,9 @@ void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,
   // Restore callee-saved registers x21-x23.
   __ Mov(x11, argc);
 
-  __ Peek(argv, 1 * kSystemPointerSize);
-  __ Peek(argc, 2 * kSystemPointerSize);
-  __ Peek(target, 3 * kSystemPointerSize);
+  __ Peek(argv, 1 * kSystemPointerAddrSize);
+  __ Peek(argc, 2 * kSystemPointerAddrSize);
+  __ Peek(target, 3 * kSystemPointerAddrSize);
 
   __ LeaveExitFrame(save_doubles == SaveFPRegsMode::kSave, x10, x9);
   if (argv_mode == ArgvMode::kStack) {
@@ -3402,7 +3402,7 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
   DoubleRegister double_scratch = temps.AcquireD();
 
   // Account for saved regs.
-  const int kArgumentOffset = 2 * kSystemPointerSize;
+  const int kArgumentOffset = 2 * kSystemPointerAddrSize;
 
   __ Push(result, scratch1);  // scratch1 is also pushed to preserve alignment.
   __ Peek(double_scratch, kArgumentOffset);
@@ -3640,33 +3640,33 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
   // Target state:
-  //   sp[0 * kSystemPointerSize]: kHolder
-  //   sp[1 * kSystemPointerSize]: kIsolate
-  //   sp[2 * kSystemPointerSize]: undefined (kReturnValueDefaultValue)
-  //   sp[3 * kSystemPointerSize]: undefined (kReturnValue)
-  //   sp[4 * kSystemPointerSize]: kData
-  //   sp[5 * kSystemPointerSize]: undefined (kNewTarget)
+  //   sp[0 * kSystemPointerAddrSize]: kHolder
+  //   sp[1 * kSystemPointerAddrSize]: kIsolate
+  //   sp[2 * kSystemPointerAddrSize]: undefined (kReturnValueDefaultValue)
+  //   sp[3 * kSystemPointerAddrSize]: undefined (kReturnValue)
+  //   sp[4 * kSystemPointerAddrSize]: kData
+  //   sp[5 * kSystemPointerAddrSize]: undefined (kNewTarget)
 
   // Reserve space on the stack.
-  __ Claim(FCA::kArgsLength, kSystemPointerSize);
+  __ Claim(FCA::kArgsLength, kSystemPointerAddrSize);
 
   // kHolder.
-  __ Str(holder, MemOperand(sp, 0 * kSystemPointerSize));
+  __ Str(holder, MemOperand(sp, 0 * kSystemPointerAddrSize));
 
   // kIsolate.
   __ Mov(scratch, ExternalReference::isolate_address(masm->isolate()));
-  __ Str(scratch, MemOperand(sp, 1 * kSystemPointerSize));
+  __ Str(scratch, MemOperand(sp, 1 * kSystemPointerAddrSize));
 
   // kReturnValueDefaultValue and kReturnValue.
   __ LoadRoot(scratch, RootIndex::kUndefinedValue);
-  __ Str(scratch, MemOperand(sp, 2 * kSystemPointerSize));
-  __ Str(scratch, MemOperand(sp, 3 * kSystemPointerSize));
+  __ Str(scratch, MemOperand(sp, 2 * kSystemPointerAddrSize));
+  __ Str(scratch, MemOperand(sp, 3 * kSystemPointerAddrSize));
 
   // kData.
-  __ Str(call_data, MemOperand(sp, 4 * kSystemPointerSize));
+  __ Str(call_data, MemOperand(sp, 4 * kSystemPointerAddrSize));
 
   // kNewTarget.
-  __ Str(scratch, MemOperand(sp, 5 * kSystemPointerSize));
+  __ Str(scratch, MemOperand(sp, 5 * kSystemPointerAddrSize));
 
   // Keep a pointer to kHolder (= implicit_args) in a scratch register.
   // We use it below to set up the FunctionCallbackInfo object.
@@ -3683,16 +3683,16 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::implicit_args_ (points at kHolder as set up above).
   // Arguments are after the return address (pushed by EnterExitFrame()).
-  __ Str(scratch, MemOperand(sp, 1 * kSystemPointerSize));
+  __ Str(scratch, MemOperand(sp, 1 * kSystemPointerAddrSize));
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
   __ Add(scratch, scratch,
-         Operand((FCA::kArgsLength + 1) * kSystemPointerSize));
-  __ Str(scratch, MemOperand(sp, 2 * kSystemPointerSize));
+         Operand((FCA::kArgsLength + 1) * kSystemPointerAddrSize));
+  __ Str(scratch, MemOperand(sp, 2 * kSystemPointerAddrSize));
 
   // FunctionCallbackInfo::length_.
-  __ Str(argc, MemOperand(sp, 3 * kSystemPointerSize));
+  __ Str(argc, MemOperand(sp, 3 * kSystemPointerAddrSize));
 
   // We also store the number of slots to drop from the stack after returning
   // from the API function here.
@@ -3701,11 +3701,11 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   // a multiple of two, and related helper functions (DropArguments) expect a
   // register containing the slot count.
   __ Add(scratch, argc, Operand(FCA::kArgsLength + 1 /*receiver*/));
-  __ Str(scratch, MemOperand(sp, 4 * kSystemPointerSize));
+  __ Str(scratch, MemOperand(sp, 4 * kSystemPointerAddrSize));
 
   // v8::InvocationCallback's argument.
   DCHECK(!AreAliased(x0, api_function_address));
-  __ add(x0, sp, Operand(1 * kSystemPointerSize));
+  __ add(x0, sp, Operand(1 * kSystemPointerAddrSize));
 
   ExternalReference thunk_ref = ExternalReference::invoke_function_callback();
 
@@ -3716,11 +3716,11 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   // TODO(jgruber): Document what these arguments are.
   static constexpr int kStackSlotsAboveFCA = 2;
   MemOperand return_value_operand(
-      fp, (kStackSlotsAboveFCA + FCA::kReturnValueOffset) * kSystemPointerSize);
+      fp, (kStackSlotsAboveFCA + FCA::kReturnValueOffset) * kSystemPointerAddrSize);
 
   static constexpr int kSpillOffset = 1 + kApiStackSpace;
   static constexpr int kUseStackSpaceOperand = 0;
-  MemOperand stack_space_operand(sp, 4 * kSystemPointerSize);
+  MemOperand stack_space_operand(sp, 4 * kSystemPointerAddrSize);
 
   AllowExternalCallThatCantCauseGC scope(masm);
   CallApiFunctionAndReturn(masm, api_function_address, thunk_ref,
@@ -3770,7 +3770,7 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
 
   // Load address of v8::PropertyAccessorInfo::args_ array and name handle.
   __ Mov(x0, sp);                          // x0 = Handle<Name>
-  __ Add(x1, x0, 1 * kSystemPointerSize);  // x1 = v8::PCI::args_
+  __ Add(x1, x0, 1 * kSystemPointerAddrSize);  // x1 = v8::PCI::args_
 
   const int kApiStackSpace = 1;
 
@@ -3779,7 +3779,7 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
 
   // Create v8::PropertyCallbackInfo object on the stack and initialize
   // it's args_ field.
-  __ Poke(x1, 1 * kSystemPointerSize);
+  __ Poke(x1, 1 * kSystemPointerAddrSize);
   __ SlotAddress(x1, 1);
   // x1 = v8::PropertyCallbackInfo&
 
@@ -3800,7 +3800,7 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   // +3 is to skip prolog, return address and name handle.
   MemOperand return_value_operand(
       fp,
-      (PropertyCallbackArguments::kReturnValueOffset + 3) * kSystemPointerSize);
+      (PropertyCallbackArguments::kReturnValueOffset + 3) * kSystemPointerAddrSize);
   MemOperand* const kUseStackSpaceConstant = nullptr;
   CallApiFunctionAndReturn(masm, api_function_address, thunk_ref,
                            kStackUnwindSpace, kUseStackSpaceConstant,
@@ -4010,7 +4010,7 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
   // frame description.
   __ Add(x3, x1, FrameDescription::frame_content_offset());
   __ SlotAddress(x1, 0);
-  __ Lsr(unwind_limit, unwind_limit, kSystemPointerSizeLog2);
+  __ Lsr(unwind_limit, unwind_limit, kSystemPointerAddrSizeLog2);
   __ Mov(x5, unwind_limit);
   __ CopyDoubleWords(x3, x1, x5);
   // Since {unwind_limit} is the frame size up to the parameter count, we might
@@ -4039,15 +4039,15 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
   Label outer_push_loop, outer_loop_header;
   __ Ldrsw(x1, MemOperand(x4, Deoptimizer::output_count_offset()));
   __ Ldr(x0, MemOperand(x4, Deoptimizer::output_offset()));
-  __ Add(x1, x0, Operand(x1, LSL, kSystemPointerSizeLog2));
+  __ Add(x1, x0, Operand(x1, LSL, kSystemPointerAddrSizeLog2));
   __ B(&outer_loop_header);
 
   __ Bind(&outer_push_loop);
   Register current_frame = x2;
   Register frame_size = x3;
-  __ Ldr(current_frame, MemOperand(x0, kSystemPointerSize, PostIndex));
+  __ Ldr(current_frame, MemOperand(x0, kSystemPointerAddrSize, PostIndex));
   __ Ldr(x3, MemOperand(current_frame, FrameDescription::frame_size_offset()));
-  __ Lsr(frame_size, x3, kSystemPointerSizeLog2);
+  __ Lsr(frame_size, x3, kSystemPointerAddrSizeLog2);
   __ Claim(frame_size);
 
   __ Add(x7, current_frame, FrameDescription::frame_content_offset());
diff --git src/builtins/builtins.h src/builtins/builtins.h
index cca669ef19e..6c91f8453a6 100644
--- src/builtins/builtins.h
+++ src/builtins/builtins.h
@@ -269,6 +269,7 @@ class Builtins {
   // Returns given builtin's slot in the tier0 builtin table.
   FullObjectSlot builtin_tier0_slot(Builtin builtin);
 
+  Isolate * isolate() { return isolate_; };
  private:
   static void Generate_CallFunction(MacroAssembler* masm,
                                     ConvertReceiverMode mode);
diff --git src/codegen/aligned-slot-allocator.h src/codegen/aligned-slot-allocator.h
index 1abb7117138..cd61c33e27b 100644
--- src/codegen/aligned-slot-allocator.h
+++ src/codegen/aligned-slot-allocator.h
@@ -20,7 +20,11 @@ namespace internal {
 class V8_EXPORT_PRIVATE AlignedSlotAllocator {
  public:
   // Slots are always multiples of pointer-sized units.
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static constexpr int kSlotSize = kSystemPointerAddrSize;
+#else
   static constexpr int kSlotSize = kSystemPointerSize;
+#endif
 
   static int NumSlotsForWidth(int bytes) {
     DCHECK_GT(bytes, 0);
diff --git src/codegen/arm64/assembler-arm64-inl.h src/codegen/arm64/assembler-arm64-inl.h
index 0f42f0efa94..b0554601d69 100644
--- src/codegen/arm64/assembler-arm64-inl.h
+++ src/codegen/arm64/assembler-arm64-inl.h
@@ -200,6 +200,28 @@ struct ImmediateInitializer {
   }
 };
 
+#ifdef __CHERI_PURE_CAPABILITY__
+template <>
+struct ImmediateInitializer<intptr_t> {
+  static inline RelocInfo::Mode rmode_for(intptr_t) { return RelocInfo::NO_INFO; }
+  static inline int64_t immediate_for(intptr_t t) {
+    static_assert(sizeof(intptr_t) <= 16);
+    static_assert(std::is_integral<intptr_t>::value);
+    return static_cast<int64_t>(t);
+  }
+};
+
+template <>
+struct ImmediateInitializer<Address> {
+  static inline RelocInfo::Mode rmode_for(Address) { return RelocInfo::NO_INFO; }
+  static inline int64_t immediate_for(Address t) {
+    static_assert(sizeof(Address) <= 16);
+    static_assert(std::is_integral<Address>::value);
+    return static_cast<uint64_t>(t);
+  }
+};
+#endif
+
 template <>
 struct ImmediateInitializer<Smi> {
   static inline RelocInfo::Mode rmode_for(Smi t) { return RelocInfo::NO_INFO; }
@@ -331,7 +353,11 @@ Immediate Operand::immediate() const {
   return immediate_;
 }
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+intptr_t Operand::ImmediateValue() const {
+#else
 int64_t Operand::ImmediateValue() const {
+#endif
   DCHECK(IsImmediate());
   return immediate_.value();
 }
@@ -504,8 +530,12 @@ AssemblerBase::EmbeddedObjectIndex
 Assembler::embedded_object_index_referenced_from(Address pc) {
   Instruction* instr = reinterpret_cast<Instruction*>(pc);
   if (instr->IsLdrLiteralX()) {
+#ifdef __CHERI_PURE_CAPABILITY__
+    return Memory<EmbeddedObjectIndex>(static_cast<size_t>(target_pointer_address_at(pc)));
+#else
     static_assert(sizeof(EmbeddedObjectIndex) == sizeof(intptr_t));
     return Memory<EmbeddedObjectIndex>(target_pointer_address_at(pc));
+#endif
   } else {
     DCHECK(instr->IsLdrLiteralW());
     return Memory<uint32_t>(target_pointer_address_at(pc));
@@ -546,7 +576,11 @@ int Assembler::deserialization_special_target_size(Address location) {
     return kSpecialTargetSize;
   } else {
     DCHECK_EQ(instr->InstructionBits(), 0);
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return kSystemPointerAddrSize;
+#else
     return kSystemPointerSize;
+#endif
   }
 }
 
@@ -617,7 +651,11 @@ int RelocInfo::target_address_size() {
   } else {
     Instruction* instr = reinterpret_cast<Instruction*>(pc_);
     DCHECK(instr->IsLdrLiteralX() || instr->IsLdrLiteralW());
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return instr->IsLdrLiteralW() ? kTaggedSize : kSystemPointerAddrSize;
+#else
     return instr->IsLdrLiteralW() ? kTaggedSize : kSystemPointerSize;
+#endif
   }
 }
 
diff --git src/codegen/arm64/assembler-arm64.cc src/codegen/arm64/assembler-arm64.cc
index 52c5d322270..53b7af2f759 100644
--- src/codegen/arm64/assembler-arm64.cc
+++ src/codegen/arm64/assembler-arm64.cc
@@ -3979,8 +3979,10 @@ void Assembler::LoadStore(const CPURegister& rt, const MemOperand& addr,
 
     // Shifts are encoded in one bit, indicating a left shift by the memory
     // access size.
+#if !defined(__CHERI_PURE_CAPABILITY__)
     DCHECK((shift_amount == 0) ||
            (shift_amount == static_cast<unsigned>(CalcLSDataSize(op))));
+#endif
     Emit(LoadStoreRegisterOffsetFixed | memop | Rm(addr.regoffset()) |
          ExtendMode(ext) | ImmShiftLS((shift_amount > 0) ? 1 : 0));
   } else {
@@ -4296,19 +4298,18 @@ void Assembler::GrowBuffer() {
   byte* new_start = new_buffer->start();
 
   // Copy the data.
-  intptr_t pc_delta = new_start - buffer_start_;
-  intptr_t rc_delta = (new_start + new_size) - (buffer_start_ + old_size);
+  ptrdiff_t pc_delta = pc_offset();
+  size_t old_pc_delta = reloc_info_writer.last_pc() - buffer_start_;
   size_t reloc_size = (buffer_start_ + old_size) - reloc_info_writer.pos();
   memmove(new_start, buffer_start_, pc_offset());
-  memmove(reloc_info_writer.pos() + rc_delta, reloc_info_writer.pos(),
-          reloc_size);
+  memmove((new_start + new_size) - reloc_size, reloc_info_writer.pos(), reloc_size);
 
   // Switch buffers.
   buffer_ = std::move(new_buffer);
   buffer_start_ = new_start;
-  pc_ += pc_delta;
-  reloc_info_writer.Reposition(reloc_info_writer.pos() + rc_delta,
-                               reloc_info_writer.last_pc() + pc_delta);
+  pc_ = buffer_start_ + pc_delta;
+  reloc_info_writer.Reposition((buffer_start_ + new_size) - reloc_size,
+                               buffer_start_ + old_pc_delta);
 
   // None of our relocation types are pc relative pointing outside the code
   // buffer nor pc absolute pointing inside the code buffer, so there is no need
diff --git src/codegen/arm64/assembler-arm64.h src/codegen/arm64/assembler-arm64.h
index f12e1ef1304..30a830fb32f 100644
--- src/codegen/arm64/assembler-arm64.h
+++ src/codegen/arm64/assembler-arm64.h
@@ -50,11 +50,19 @@ class Immediate {
   template <typename T>
   inline Immediate(T value, RelocInfo::Mode rmode);
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+  intptr_t value() const { return value_; }
+#else
   int64_t value() const { return value_; }
+#endif
   RelocInfo::Mode rmode() const { return rmode_; }
 
  private:
+#if defined(__CHERI_PURE_CAPABILITY__)
+  intptr_t value_;
+#else
   int64_t value_;
+#endif
   RelocInfo::Mode rmode_;
 };
 
@@ -109,7 +117,11 @@ class Operand {
   inline Operand ToW() const;
 
   inline Immediate immediate() const;
+#if defined(__CHERI_PURE_CAPABILITY__)
+  inline intptr_t ImmediateValue() const;
+#else
   inline int64_t ImmediateValue() const;
+#endif
   inline RelocInfo::Mode ImmediateRMode() const;
   inline Register reg() const;
   inline Shift shift() const;
diff --git src/codegen/arm64/constants-arm64.h src/codegen/arm64/constants-arm64.h
index 9b877630e4b..9e8ef53bf35 100644
--- src/codegen/arm64/constants-arm64.h
+++ src/codegen/arm64/constants-arm64.h
@@ -16,7 +16,16 @@ static_assert(sizeof(1L) == sizeof(int32_t));
 static_assert(sizeof(long) == sizeof(int64_t));  // NOLINT(runtime/int)
 static_assert(sizeof(1L) == sizeof(int64_t));
 #endif
+#if defined(__CHERI_PURE_CAPABILITY__)
+// CHERI C/C++ Programming Guide S4.2.1
+// "According to the C standard, these integer types should be ‘capable of representing
+// any value of any (unsigned) integer type’. In CHERI C/C++, they are not
+// provenance-carrying and can represent the integer range of uintptr_t/intptr_t, but 
+// not the capability metadata or tag bit."
+static_assert(sizeof(intmax_t) == sizeof(int64_t));
+#else
 static_assert(sizeof(void*) == sizeof(int64_t));
+#endif
 static_assert(sizeof(1) == sizeof(int32_t));
 
 // Get the standard printf format macros for C99 stdint types.
@@ -144,7 +153,11 @@ const unsigned kFloat16ExponentBias = 15;
 // to take advantage of negative displacement values.
 // TODO(sigurds): Choose best value.
 // TODO(ishell): Choose best value for ptr-compr.
+#if defined(__CHERI_PURE_CAPABILITY__)
+constexpr int kRootRegisterBias = kSystemPointerAddrSize == kTaggedSize ? 256 : 0;
+#else
 constexpr int kRootRegisterBias = kSystemPointerSize == kTaggedSize ? 256 : 0;
+#endif
 
 using float16 = uint16_t;
 
diff --git src/codegen/arm64/cpu-arm64.cc src/codegen/arm64/cpu-arm64.cc
index 96bb73f0d45..15fc844dbe3 100644
--- src/codegen/arm64/cpu-arm64.cc
+++ src/codegen/arm64/cpu-arm64.cc
@@ -47,7 +47,9 @@ class CacheLineSizes {
 
 void CpuFeatures::FlushICache(void* address, size_t length) {
 #if defined(V8_HOST_ARCH_ARM64)
-#if defined(V8_OS_WIN)
+#if __has_builtin(__clear_cache)
+  __clear_cache(address, (void *)((uintptr_t) address + length));
+#elif defined(V8_OS_WIN)
   ::FlushInstructionCache(GetCurrentProcess(), address, length);
 #elif defined(V8_OS_DARWIN)
   sys_icache_invalidate(address, length);
diff --git src/codegen/arm64/macro-assembler-arm64-inl.h src/codegen/arm64/macro-assembler-arm64-inl.h
index a233ca17ee7..bc81cbfcb57 100644
--- src/codegen/arm64/macro-assembler-arm64-inl.h
+++ src/codegen/arm64/macro-assembler-arm64-inl.h
@@ -1248,7 +1248,9 @@ void TurboAssembler::Claim(int64_t count, uint64_t unit_size) {
   if (size == 0) {
     return;
   }
+#if !defined(__CHERI_PURE_CAPABILITY__)
   DCHECK_EQ(size % 16, 0);
+#endif
 #ifdef V8_TARGET_OS_WIN
   while (size > kStackPageSize) {
     Sub(sp, sp, kStackPageSize);
@@ -1309,7 +1311,9 @@ void TurboAssembler::Drop(int64_t count, uint64_t unit_size) {
   }
 
   Add(sp, sp, size);
+#if !defined(__CHERI_PURE_CAPABILITY__)
   DCHECK_EQ(size % 16, 0);
+#endif
 }
 
 void TurboAssembler::Drop(const Register& count, uint64_t unit_size) {
diff --git src/codegen/arm64/macro-assembler-arm64.cc src/codegen/arm64/macro-assembler-arm64.cc
index ca52e6ecca7..f6a1d4d7c71 100644
--- src/codegen/arm64/macro-assembler-arm64.cc
+++ src/codegen/arm64/macro-assembler-arm64.cc
@@ -1283,7 +1283,11 @@ void MacroAssembler::PushCalleeSavedRegisters() {
 
   static_assert(
       EntryFrameConstants::kCalleeSavedRegisterBytesPushedBeforeFpLrPair ==
+#if defined(__CHERI_PURE_CAPABILITY__)
+      18 * kSystemPointerAddrSize);
+#else
       18 * kSystemPointerSize);
+#endif
 
 #ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY
     // Use the stack pointer's value immediately before pushing the LR as the
@@ -1376,12 +1380,21 @@ void TurboAssembler::CopyDoubleWords(Register dst, Register src, Register count,
     Bind(&pointer1_below_pointer2);
     Add(pointer1, pointer1, pointer2);
   }
+
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static_assert(kSystemPointerAddrSize == kDRegSize,
+#else
   static_assert(kSystemPointerSize == kDRegSize,
+#endif
                 "pointers must be the same size as doubles");
 
   if (mode == kDstLessThanSrcAndReverse) {
     Add(src, src, Operand(count, LSL, kSystemPointerSizeLog2));
+#if defined(__CHERI_PURE_CAPABILITY__)
+    Sub(src, src, kSystemPointerAddrSize);
+#else
     Sub(src, src, kSystemPointerSize);
+#endif
   }
 
   int src_direction = (mode == kDstLessThanSrc) ? 1 : -1;
@@ -1394,29 +1407,58 @@ void TurboAssembler::CopyDoubleWords(Register dst, Register src, Register count,
   Label pairs, loop, done;
 
   Tbz(count, 0, &pairs);
+#if defined(__CHERI_PURE_CAPABILITY__)
+  Ldr(temp0, MemOperand(src, src_direction * kSystemPointerAddrSize, PostIndex));
+#else
   Ldr(temp0, MemOperand(src, src_direction * kSystemPointerSize, PostIndex));
+#endif
   Sub(count, count, 1);
+#if defined(__CHERI_PURE_CAPABILITY__)
+  Str(temp0, MemOperand(dst, dst_direction * kSystemPointerAddrSize, PostIndex));
+#else
   Str(temp0, MemOperand(dst, dst_direction * kSystemPointerSize, PostIndex));
+#endif
 
   Bind(&pairs);
   if (mode == kSrcLessThanDst) {
     // Adjust pointers for post-index ldp/stp with negative offset:
+#if defined(__CHERI_PURE_CAPABILITY__)
+    Sub(dst, dst, kSystemPointerAddrSize);
+    Sub(src, src, kSystemPointerAddrSize);
+#else
     Sub(dst, dst, kSystemPointerSize);
     Sub(src, src, kSystemPointerSize);
+#endif
   } else if (mode == kDstLessThanSrcAndReverse) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    Sub(src, src, kSystemPointerAddrSize);
+#else
     Sub(src, src, kSystemPointerSize);
+#endif
   }
   Bind(&loop);
   Cbz(count, &done);
   Ldp(temp0, temp1,
+#if defined(__CHERI_PURE_CAPABILITY__)
+      MemOperand(src, 2 * src_direction * kSystemPointerAddrSize, PostIndex));
+#else
       MemOperand(src, 2 * src_direction * kSystemPointerSize, PostIndex));
+#endif
   Sub(count, count, 2);
   if (mode == kDstLessThanSrcAndReverse) {
     Stp(temp1, temp0,
+#if defined(__CHERI_PURE_CAPABILITY__)
+        MemOperand(dst, 2 * dst_direction * kSystemPointerAddrSize, PostIndex));
+#else
         MemOperand(dst, 2 * dst_direction * kSystemPointerSize, PostIndex));
+#endif
   } else {
     Stp(temp0, temp1,
+#if defined(__CHERI_PURE_CAPABILITY__)
+        MemOperand(dst, 2 * dst_direction * kSystemPointerAddrSize, PostIndex));
+#else
         MemOperand(dst, 2 * dst_direction * kSystemPointerSize, PostIndex));
+#endif
   }
   B(&loop);
 
@@ -2152,9 +2194,6 @@ void TurboAssembler::LoadCodeDataContainerCodeNonBuiltin(
   ASM_CODE_COMMENT(this);
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   // Given the fields layout we can read the Code reference as a full word.
-  static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL ||
-                (CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
-                 CodeDataContainer::kCodeOffset + kTaggedSize));
   Ldr(destination, FieldMemOperand(code_data_container_object,
                                    CodeDataContainer::kCodeOffset));
 }
@@ -2221,7 +2260,11 @@ void TurboAssembler::StoreReturnAddressAndCall(Register target) {
   Label return_location;
   Adr(x17, &return_location);
 #ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY
+#if defined(__CHERI_PURE_CAPABILITY__)
+  Add(x16, sp, kSystemPointerAddrSize);
+#else
   Add(x16, sp, kSystemPointerSize);
+#endif
   Pacib1716();
 #endif
   Poke(x17, 0);
@@ -2377,7 +2420,11 @@ void MacroAssembler::InvokePrologue(Register formal_parameter_count,
     Mov(count, extra_argument_count);
     Bind(&loop);
     Str(undefined_value,
+#if defined(__CHERI_PURE_CAPABILITY__)
+        MemOperand(pointer_next_value, kSystemPointerAddrSize, PostIndex));
+#else
         MemOperand(pointer_next_value, kSystemPointerSize, PostIndex));
+#endif
     Subs(count, count, 1);
     Cbnz(count, &loop);
   }
@@ -2659,7 +2706,11 @@ void TurboAssembler::EnterFrame(StackFrame::Type type) {
     Mov(type_reg, StackFrame::TypeToMarker(type));
     Push<TurboAssembler::kSignLR>(lr, fp, type_reg, padreg);
     const int kFrameSize =
+#if defined(__CHERI_PURE_CAPABILITY__)
+        TypedFrameConstants::kFixedFrameSizeFromFp + kSystemPointerAddrSize;
+#else
         TypedFrameConstants::kFixedFrameSizeFromFp + kSystemPointerSize;
+#endif
     Add(fp, sp, kFrameSize);
     // sp[3] : lr
     // sp[2] : fp
@@ -2690,7 +2741,11 @@ void TurboAssembler::EnterFrame(StackFrame::Type type) {
     // The context pointer isn't part of the fixed frame, so add an extra slot
     // to account for it.
     Add(fp, sp,
+#if defined(__CHERI_PURE_CAPABILITY__)
+        TypedFrameConstants::kFixedFrameSizeFromFp + kSystemPointerAddrSize);
+#else
         TypedFrameConstants::kFixedFrameSizeFromFp + kSystemPointerSize);
+#endif
     // sp[3] : lr
     // sp[2] : fp
     // sp[1] : type
@@ -2752,13 +2807,23 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, const Register& scratch,
   //    fp -> fp[0]: CallerFP (old fp)
   //          fp[-8]: STUB marker
   //    sp -> fp[-16]: Space reserved for SPOffset.
-  static_assert((2 * kSystemPointerSize) ==
+#if defined(__CHERI_PURE_CAPABILITY__)
+    static_assert((2 * kSystemPointerAddrSize) ==
+                ExitFrameConstants::kCallerSPOffset);
+  static_assert((1 * kSystemPointerAddrSize) ==
+                ExitFrameConstants::kCallerPCOffset);
+  static_assert((0 * kSystemPointerAddrSize) ==
+                ExitFrameConstants::kCallerFPOffset);
+static_assert((-2 * kSystemPointerAddrSize) == ExitFrameConstants::kSPOffset);
+#else
+   static_assert((2 * kSystemPointerSize) ==
                 ExitFrameConstants::kCallerSPOffset);
   static_assert((1 * kSystemPointerSize) ==
                 ExitFrameConstants::kCallerPCOffset);
   static_assert((0 * kSystemPointerSize) ==
                 ExitFrameConstants::kCallerFPOffset);
-  static_assert((-2 * kSystemPointerSize) == ExitFrameConstants::kSPOffset);
+ static_assert((-2 * kSystemPointerSize) == ExitFrameConstants::kSPOffset);
+#endif
 
   // Save the frame pointer and context pointer in the top frame.
   Mov(scratch,
@@ -2767,8 +2832,11 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, const Register& scratch,
   Mov(scratch,
       ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
   Str(cp, MemOperand(scratch));
-
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static_assert((-2 * kSystemPointerAddrSize) ==
+#else
   static_assert((-2 * kSystemPointerSize) ==
+#endif
                 ExitFrameConstants::kLastExitFrameField);
   if (save_doubles) {
     ExitFramePreserveFPRegs();
@@ -3717,7 +3785,11 @@ void TurboAssembler::ComputeCodeStartAddress(const Register& rd) {
 }
 
 void TurboAssembler::RestoreFPAndLR() {
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static_assert(StandardFrameConstants::kCallerFPOffset + kSystemPointerAddrSize ==
+#else
   static_assert(StandardFrameConstants::kCallerFPOffset + kSystemPointerSize ==
+#endif
                     StandardFrameConstants::kCallerPCOffset,
                 "Offsets must be consecutive for ldp!");
 #ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY
@@ -3740,7 +3812,11 @@ void TurboAssembler::StoreReturnAddressInWasmExitFrame(Label* return_location) {
   temps.Exclude(x16, x17);
   Adr(x17, return_location);
 #ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY
+#if defined(__CHERI_PURE_CAPABILITY__)
+  Add(x16, fp, WasmExitFrameConstants::kCallingPCOffset + kSystemPointerAddrSize);
+#else
   Add(x16, fp, WasmExitFrameConstants::kCallingPCOffset + kSystemPointerSize);
+#endif
   Pacib1716();
 #endif
   Str(x17, MemOperand(fp, WasmExitFrameConstants::kCallingPCOffset));
diff --git src/codegen/arm64/register-arm64.h src/codegen/arm64/register-arm64.h
index 728ac559d50..73c85ef836c 100644
--- src/codegen/arm64/register-arm64.h
+++ src/codegen/arm64/register-arm64.h
@@ -256,7 +256,11 @@ static_assert(sizeof(Register) <= sizeof(int),
 constexpr int ArgumentPaddingSlots(int argument_count) {
   // Stack frames are aligned to 16 bytes.
   constexpr int kStackFrameAlignment = 16;
+#if defined(__CHERI_PURE_CAPABILITY__)
+  constexpr int alignment_mask = kStackFrameAlignment / kSystemPointerAddrSize - 1;
+#else
   constexpr int alignment_mask = kStackFrameAlignment / kSystemPointerSize - 1;
+#endif
   return argument_count & alignment_mask;
 }
 
diff --git src/codegen/code-stub-assembler.cc src/codegen/code-stub-assembler.cc
index d35929d7f21..6230ff37b6a 100644
--- src/codegen/code-stub-assembler.cc
+++ src/codegen/code-stub-assembler.cc
@@ -3386,7 +3386,11 @@ TNode<BigInt> CodeStubAssembler::AllocateRawBigInt(TNode<IntPtrT> length) {
       Allocate(size, AllocationFlag::kAllowLargeObjectAllocation);
   StoreMapNoWriteBarrier(raw_result, RootIndex::kBigIntMap);
   if (FIELD_SIZE(BigInt::kOptionalPaddingOffset) != 0) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    DCHECK_EQ(8, FIELD_SIZE(BigInt::kOptionalPaddingOffset));
+#else
     DCHECK_EQ(4, FIELD_SIZE(BigInt::kOptionalPaddingOffset));
+#endif
     StoreObjectFieldNoWriteBarrier(raw_result, BigInt::kOptionalPaddingOffset,
                                    Int32Constant(0));
   }
@@ -4754,7 +4758,11 @@ void CodeStubAssembler::FillFixedArrayWithSmiZero(TNode<FixedArray> array,
   // Call out to memset to perform initialization.
   TNode<ExternalReference> memset =
       ExternalConstant(ExternalReference::libc_memset_function());
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static_assert(kSizetSize == kPtrAddrSize);
+#else
   static_assert(kSizetSize == kIntptrSize);
+#endif
   CallCFunction(memset, MachineType::Pointer(),
                 std::make_pair(MachineType::Pointer(), backing_store),
                 std::make_pair(MachineType::IntPtr(), IntPtrConstant(0)),
@@ -4776,7 +4784,11 @@ void CodeStubAssembler::FillFixedDoubleArrayWithZero(
   // Call out to memset to perform initialization.
   TNode<ExternalReference> memset =
       ExternalConstant(ExternalReference::libc_memset_function());
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static_assert(kSizetSize == kPtrAddrSize);
+#else
   static_assert(kSizetSize == kIntptrSize);
+#endif
   CallCFunction(memset, MachineType::Pointer(),
                 std::make_pair(MachineType::Pointer(), backing_store),
                 std::make_pair(MachineType::IntPtr(), IntPtrConstant(0)),
diff --git src/codegen/code-stub-assembler.h src/codegen/code-stub-assembler.h
index 2b469c5e351..cd97a843156 100644
--- src/codegen/code-stub-assembler.h
+++ src/codegen/code-stub-assembler.h
@@ -504,8 +504,13 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
 
   uintptr_t ConstexprUintPtrShl(uintptr_t a, int32_t b) { return a << b; }
   uintptr_t ConstexprUintPtrShr(uintptr_t a, int32_t b) { return a >> b; }
+#ifdef __CHERI_PURE_CAPABILITY__
+  intptr_t ConstexprIntPtrAdd(intptr_t a, intptr_t b) { return a + (size_t) b; }
+  uintptr_t ConstexprUintPtrAdd(uintptr_t a, uintptr_t b) { return a + (size_t) b; }
+#else
   intptr_t ConstexprIntPtrAdd(intptr_t a, intptr_t b) { return a + b; }
   uintptr_t ConstexprUintPtrAdd(uintptr_t a, uintptr_t b) { return a + b; }
+#endif
   intptr_t ConstexprWordNot(intptr_t a) { return ~a; }
   uintptr_t ConstexprWordNot(uintptr_t a) { return ~a; }
 
@@ -828,8 +833,6 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
 #error "This code requires updating for big-endian architectures"
 #endif
     // Given the fields layout we can read the Code reference as a full word.
-    static_assert(CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
-                  CodeDataContainer::kCodeOffset + kTaggedSize);
     TNode<Object> o = BitcastWordToTagged(Load<RawPtrT>(
         code, IntPtrConstant(CodeDataContainer::kCodeOffset - kHeapObjectTag)));
     return CAST(o);
diff --git src/codegen/external-reference-table.h src/codegen/external-reference-table.h
index b50b1d534a6..30a2275cb0a 100644
--- src/codegen/external-reference-table.h
+++ src/codegen/external-reference-table.h
@@ -52,7 +52,11 @@ class ExternalReferenceTable {
       kStatsCountersReferenceCount;
   static constexpr uint32_t kEntrySize =
       static_cast<uint32_t>(kSystemPointerSize);
-  static constexpr uint32_t kSizeInBytes = kSize * kEntrySize + 2 * kUInt32Size;
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static constexpr uint32_t kSizeInBytes = RoundUp<16>(kSize * kEntrySize + 2 * kUInt32Size);
+#else
+  static constexpr uint32_t kSizeInBytes = RoundUp<8>(kSize * kEntrySize + 2 * kUInt32Size);
+#endif
 
   Address address(uint32_t i) const { return ref_addr_[i]; }
   const char* name(uint32_t i) const { return ref_name_[i]; }
@@ -115,7 +119,7 @@ class ExternalReferenceTable {
 };
 
 static_assert(ExternalReferenceTable::kSizeInBytes ==
-              sizeof(ExternalReferenceTable));
+		sizeof(ExternalReferenceTable));
 
 }  // namespace internal
 }  // namespace v8
diff --git src/codegen/machine-type.h src/codegen/machine-type.h
index ccbfbfd476a..92a95320622 100644
--- src/codegen/machine-type.h
+++ src/codegen/machine-type.h
@@ -296,10 +296,14 @@ class MachineType {
       case CTypeInfo::Type::kInt64:
         return MachineType::Int64();
       case CTypeInfo::Type::kAny:
+#if defined(__CHERI_PURE_CAPABILITY__)
+        return MachineType::AnyTagged();
+#else
         static_assert(
             sizeof(AnyCType) == kInt64Size,
             "CTypeInfo::Type::kAny is assumed to be of size 64 bits.");
         return MachineType::Int64();
+#endif
       case CTypeInfo::Type::kUint64:
         return MachineType::Uint64();
       case CTypeInfo::Type::kFloat32:
diff --git src/codegen/reloc-info.cc src/codegen/reloc-info.cc
index 461978b2c0b..bfd51977f1d 100644
--- src/codegen/reloc-info.cc
+++ src/codegen/reloc-info.cc
@@ -8,6 +8,7 @@
 #include "src/codegen/assembler-inl.h"
 #include "src/codegen/code-reference.h"
 #include "src/codegen/external-reference-encoder.h"
+#include "src/common/cheri.h"
 #include "src/deoptimizer/deoptimize-reason.h"
 #include "src/deoptimizer/deoptimizer.h"
 #include "src/heap/heap-write-barrier-inl.h"
@@ -16,6 +17,7 @@
 
 namespace v8 {
 namespace internal {
+using cheri::operator<<;
 
 const char* const RelocInfo::kFillerCommentString = "DEOPTIMIZATION PADDING";
 
diff --git src/codegen/x64/macro-assembler-x64.cc src/codegen/x64/macro-assembler-x64.cc
index d623218dc7e..9c9353462a9 100644
--- src/codegen/x64/macro-assembler-x64.cc
+++ src/codegen/x64/macro-assembler-x64.cc
@@ -2013,9 +2013,6 @@ void TurboAssembler::LoadCodeDataContainerCodeNonBuiltin(
   ASM_CODE_COMMENT(this);
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   // Given the fields layout we can read the Code reference as a full word.
-  static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL ||
-                (CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
-                 CodeDataContainer::kCodeOffset + kTaggedSize));
   movq(destination, FieldOperand(code_data_container_object,
                                  CodeDataContainer::kCodeOffset));
 }
diff --git src/common/checks.h src/common/checks.h
index 59c845b4f54..b81197f2c15 100644
--- src/common/checks.h
+++ src/common/checks.h
@@ -22,10 +22,20 @@
 #define SLOW_DCHECK_IMPLIES(v1, v2) ((void)0)
 #endif
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+#define DCHECK_TAG_ALIGNED(address) \
+  DCHECK((address & (size_t) ::v8::internal::kHeapObjectTagMask) == 0)
+#else
 #define DCHECK_TAG_ALIGNED(address) \
   DCHECK((address & ::v8::internal::kHeapObjectTagMask) == 0)
+#endif
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+#define DCHECK_SIZE_TAG_ALIGNED(size) \
+  DCHECK((size & (size_t) ::v8::internal::kHeapObjectTagMask) == 0)
+#else
 #define DCHECK_SIZE_TAG_ALIGNED(size) \
   DCHECK((size & ::v8::internal::kHeapObjectTagMask) == 0)
+#endif
 
 #endif  // V8_COMMON_CHECKS_H_
diff --git src/common/cheri.h src/common/cheri.h
new file mode 100644
index 00000000000..75c78593b09
--- /dev/null
+++ src/common/cheri.h
@@ -0,0 +1,25 @@
+#ifndef __CHERI_WORKAROUND__
+#define __CHERI_WORKAROUND__
+
+#include <stdint.h>
+#include <ostream>
+
+namespace cheri 
+{
+  inline std::ostream &operator<<(std::ostream &out, uintptr_t val)
+  {
+    return out << static_cast<size_t>(val);
+  }
+
+  inline std::ostream &operator<<(std::ostream &out, intptr_t val)
+  {
+    return out << static_cast<ssize_t>(val);
+  }
+
+  template<typename T>
+  struct is_intcap {
+    constexpr static bool value = std::is_same<intptr_t, T>::value || std::is_same<uintptr_t, T>::value;
+  };
+}
+
+#endif
diff --git src/common/globals.h src/common/globals.h
index 7d113ff1ba6..28e6f4499a1 100644
--- src/common/globals.h
+++ src/common/globals.h
@@ -260,6 +260,7 @@ constexpr int kUInt16Size = sizeof(uint16_t);
 constexpr int kIntSize = sizeof(int);
 constexpr int kInt32Size = sizeof(int32_t);
 constexpr int kInt64Size = sizeof(int64_t);
+constexpr int kUInt64Size = sizeof(uint64_t);
 constexpr int kUInt32Size = sizeof(uint32_t);
 constexpr int kSizetSize = sizeof(size_t);
 constexpr int kFloatSize = sizeof(float);
@@ -268,8 +269,17 @@ constexpr int kIntptrSize = sizeof(intptr_t);
 constexpr int kUIntptrSize = sizeof(uintptr_t);
 constexpr int kSystemPointerSize = sizeof(void*);
 constexpr int kSystemPointerHexDigits = kSystemPointerSize == 4 ? 8 : 12;
+#ifdef __CHERI_PURE_CAPABILITY__
+constexpr int kPtrAddrSize = sizeof(ptraddr_t);
+constexpr int kSystemPointerAddrSize = sizeof(ptraddr_t);
+constexpr int kPCOnStackSize = kSystemPointerAddrSize;
+constexpr int kFPOnStackSize = kSystemPointerAddrSize;
+#else
+constexpr int kPtrAddrSize = kSystemPointerSize
+constexpr int kSystemPOinterAddrSize = kSystemPointerSize
 constexpr int kPCOnStackSize = kSystemPointerSize;
 constexpr int kFPOnStackSize = kSystemPointerSize;
+#endif
 
 #if V8_TARGET_ARCH_X64 || V8_TARGET_ARCH_IA32
 constexpr int kElidedFrameSlots = kPCOnStackSize / kSystemPointerSize;
@@ -294,8 +304,14 @@ constexpr int kMaxDoubleStringLength = 24;
 constexpr size_t kMaxWasmCodeMB = 4095;
 constexpr size_t kMaxWasmCodeMemory = kMaxWasmCodeMB * MB;
 
-#if V8_HOST_ARCH_64_BIT
-constexpr int kSystemPointerSizeLog2 = 3;
+#if defined(V8_HOST_ARCH_64_BIT)
+#if defined(V8_HOST_CHERI_PURE_CAPABILITY)
+constexpr int kSystemPointerSizeLog2 = 4;
+#else
+constexpr int kPtrAddrSizeLog2 = 3;
+#endif
+constexpr int kSystemPointerAddrSizeLog2 = 3;
+constexpr int kPtrAddrSizeLog2 = 3;
 constexpr intptr_t kIntptrSignBit =
     static_cast<intptr_t>(uintptr_t{0x8000000000000000});
 constexpr bool kPlatformRequiresCodeRange = true;
@@ -357,8 +373,8 @@ static_assert(kSystemPointerSize == (1 << kSystemPointerSizeLog2));
 static constexpr bool kCompressGraphZone = COMPRESS_ZONES_BOOL;
 
 #ifdef V8_COMPRESS_POINTERS
-static_assert(
-    kSystemPointerSize == kInt64Size,
+ static_assert(
+    kPtrAddrSize == kInt64Size,
     "Pointer compression can be enabled only for 64-bit architectures");
 
 constexpr int kTaggedSize = kInt32Size;
@@ -377,7 +393,11 @@ constexpr int kTaggedSizeLog2 = kSystemPointerSizeLog2;
 // These types define raw and atomic storage types for tagged values stored
 // on V8 heap.
 using Tagged_t = Address;
+#if defined(__CHERI_PURE_CAPABILITY__)
 using AtomicTagged_t = base::AtomicWord;
+#else
+using AtomicTagged_t = base::AtomicIntPtr;
+#endif
 
 #endif  // V8_COMPRESS_POINTERS
 
@@ -407,6 +427,11 @@ static_assert(kExternalPointerSize == kSystemPointerSize);
 #endif
 
 constexpr int kEmbedderDataSlotSize = kSystemPointerSize;
+#ifdef __CHERI_PURE_CAPABILITY__
+constexpr int kEmbedderDataSlotObservableSize = kPtrAddrSize;
+#else
+constexpr int kEmbedderDataSlotObservableSize = kEmbedderDataSlotSize;
+#endif
 
 constexpr int kEmbedderDataSlotSizeInTaggedSlots =
     kEmbedderDataSlotSize / kTaggedSize;
@@ -429,6 +454,11 @@ constexpr int kBitsPerByteLog2 = 3;
 constexpr int kBitsPerSystemPointer = kSystemPointerSize * kBitsPerByte;
 constexpr int kBitsPerSystemPointerLog2 =
     kSystemPointerSizeLog2 + kBitsPerByteLog2;
+#if defined(__CHERI_PURE_CAPABILITY__)
+constexpr int kBitsPerPtrAddr = kPtrAddrSize * kBitsPerByte;
+constexpr int kBitsPerPtrAddrLog2 =
+    kPtrAddrSizeLog2 + kBitsPerByteLog2;
+#endif
 constexpr int kBitsPerInt = kIntSize * kBitsPerByte;
 
 // IEEE 754 single precision floating point number bit layout.
@@ -679,7 +709,7 @@ const uint32_t kClearedWeakHeapObjectLower32 = 3;
 
 // Zap-value: The value used for zapping dead objects.
 // Should be a recognizable hex value tagged as a failure.
-#ifdef V8_HOST_ARCH_64_BIT
+#if defined(V8_HOST_ARCH_64_BIT)
 constexpr uint64_t kClearedFreeMemoryValue = 0;
 constexpr uint64_t kZapValue = uint64_t{0xdeadbeedbeadbeef};
 constexpr uint64_t kHandleZapValue = uint64_t{0x1baddead0baddeaf};
@@ -703,7 +733,11 @@ constexpr int kCodeZapValue = 0xbadc0de;
 constexpr uint32_t kPhantomReferenceZap = 0xca11bac;
 
 // Page constants.
+#ifdef __CHERI_PURE_CAPABILITY__
+__attribute__((cheri_no_provenance)) static const intptr_t kPageAlignmentMask = (intptr_t{1} << kPageSizeBits) - 1;
+#else
 static const intptr_t kPageAlignmentMask = (intptr_t{1} << kPageSizeBits) - 1;
+#endif
 
 // On Intel architecture, cache line size is 64 bytes.
 // On ARM it may be less (32 bytes), but as far this constant is
@@ -790,6 +824,9 @@ class CompressedObjectSlot;
 class CompressedMaybeObjectSlot;
 class CompressedMapWordSlot;
 class CompressedHeapObjectSlot;
+class V8HeapCompressionScheme;
+class ExternalCodeCompressionScheme;
+template <typename CompressionScheme>
 class OffHeapCompressedObjectSlot;
 class FullObjectSlot;
 class FullMaybeObjectSlot;
@@ -818,8 +855,14 @@ struct SlotTraits {
   using TObjectSlot = CompressedObjectSlot;
   using TMaybeObjectSlot = CompressedMaybeObjectSlot;
   using THeapObjectSlot = CompressedHeapObjectSlot;
-  using TOffHeapObjectSlot = OffHeapCompressedObjectSlot;
-  using TCodeObjectSlot = OffHeapCompressedObjectSlot;
+  using TOffHeapObjectSlot =
+      OffHeapCompressedObjectSlot<V8HeapCompressionScheme>;
+#ifdef V8_EXTERNAL_CODE_SPACE
+  using TCodeObjectSlot =
+      OffHeapCompressedObjectSlot<ExternalCodeCompressionScheme>;
+#else
+  using TCodeObjectSlot = TObjectSlot;
+#endif // V8_EXTERNAL_CODE_SPACE
 #else
   using TObjectSlot = FullObjectSlot;
   using TMaybeObjectSlot = FullMaybeObjectSlot;
@@ -1122,8 +1165,13 @@ constexpr int kIeeeDoubleExponentWordOffset = 0;
     ::i::kWeakHeapObjectTag))
 
 // OBJECT_POINTER_ALIGN returns the value aligned as a HeapObject pointer
+#if defined(__CHERI_PURE_CAPABILITY__)
+#define OBJECT_POINTER_ALIGN(value) \
+  (((value) + (size_t) ::i::kObjectAlignmentMask) & (size_t) ~::i::kObjectAlignmentMask)
+#else
 #define OBJECT_POINTER_ALIGN(value) \
   (((value) + ::i::kObjectAlignmentMask) & ~::i::kObjectAlignmentMask)
+#endif
 
 // OBJECT_POINTER_PADDING returns the padding size required to align value
 // as a HeapObject pointer
diff --git src/common/ptr-compr-inl.h src/common/ptr-compr-inl.h
index f5991ddcda2..ddc9659b45f 100644
--- src/common/ptr-compr-inl.h
+++ src/common/ptr-compr-inl.h
@@ -14,6 +14,175 @@ namespace internal {
 
 #ifdef V8_COMPRESS_POINTERS
 
+//
+// V8HeapCompressionScheme
+//
+
+// static
+Address V8HeapCompressionScheme::GetPtrComprCageBaseAddress(
+    Address on_heap_addr) {
+  return RoundDown<kPtrComprCageBaseAlignment>(on_heap_addr);
+}
+
+// static
+Address V8HeapCompressionScheme::GetPtrComprCageBaseAddress(
+    PtrComprCageBase cage_base) {
+  Address base = cage_base.address();
+  base = reinterpret_cast<Address>(V8_ASSUME_ALIGNED(
+      reinterpret_cast<void*>(base), kPtrComprCageBaseAlignment));
+  return base;
+}
+
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+
+// static
+void V8HeapCompressionScheme::InitBase(Address base) {
+  CHECK_EQ(base, GetPtrComprCageBaseAddress(base));
+  base_ = base;
+}
+
+// static
+Address V8HeapCompressionScheme::base() {
+  return reinterpret_cast<Address>(V8_ASSUME_ALIGNED(
+      reinterpret_cast<void*>(base_), kPtrComprCageBaseAlignment));
+}
+#endif  // V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+
+// static
+Tagged_t V8HeapCompressionScheme::CompressTagged(Address tagged) {
+  return static_cast<Tagged_t>(static_cast<uint32_t>(tagged));
+}
+
+// static
+Address V8HeapCompressionScheme::DecompressTaggedSigned(Tagged_t raw_value) {
+  // For runtime code the upper 32-bits of the Smi value do not matter.
+  return static_cast<Address>(raw_value);
+}
+
+// static
+template <typename TOnHeapAddress>
+Address V8HeapCompressionScheme::DecompressTaggedPointer(
+    TOnHeapAddress on_heap_addr, Tagged_t raw_value) {
+#if defined(V8_COMPRESS_POINTERS_IN_SHARED_CAGE) && \
+    !defined(V8_COMPRESS_POINTERS_DONT_USE_GLOBAL_BASE)
+  Address cage_base = base();
+#else
+  Address cage_base = GetPtrComprCageBaseAddress(on_heap_addr);
+#endif
+  return cage_base + static_cast<Address>(raw_value);
+}
+
+// static
+template <typename TOnHeapAddress>
+Address V8HeapCompressionScheme::DecompressTaggedAny(
+    TOnHeapAddress on_heap_addr, Tagged_t raw_value) {
+  return DecompressTaggedPointer(on_heap_addr, raw_value);
+}
+
+// static
+template <typename ProcessPointerCallback>
+void V8HeapCompressionScheme::ProcessIntermediatePointers(
+    PtrComprCageBase cage_base, Address raw_value,
+    ProcessPointerCallback callback) {
+  // If pointer compression is enabled, we may have random compressed pointers
+  // on the stack that may be used for subsequent operations.
+  // Extract, decompress and trace both halfwords.
+  Address decompressed_low = V8HeapCompressionScheme::DecompressTaggedPointer(
+      cage_base, static_cast<Tagged_t>(raw_value));
+  callback(decompressed_low);
+  Address decompressed_high = V8HeapCompressionScheme::DecompressTaggedPointer(
+      cage_base,
+      static_cast<Tagged_t>(raw_value >> (sizeof(Tagged_t) * CHAR_BIT)));
+  callback(decompressed_high);
+}
+
+#ifdef V8_EXTERNAL_CODE_SPACE
+
+//
+// ExternalCodeCompressionScheme
+//
+
+// static
+Address ExternalCodeCompressionScheme::PrepareCageBaseAddress(
+    Address on_heap_addr) {
+  return RoundDown<kMinExpectedOSPageSize>(on_heap_addr);
+}
+
+// static
+Address ExternalCodeCompressionScheme::GetPtrComprCageBaseAddress(
+    PtrComprCageBase cage_base) {
+  Address base = cage_base.address();
+  base = reinterpret_cast<Address>(
+      V8_ASSUME_ALIGNED(reinterpret_cast<void*>(base), kMinExpectedOSPageSize));
+  return base;
+}
+
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+
+// static
+void ExternalCodeCompressionScheme::InitBase(Address base) {
+  CHECK_EQ(base, PrepareCageBaseAddress(base));
+  base_ = base;
+}
+
+// static
+Address ExternalCodeCompressionScheme::base() {
+  return reinterpret_cast<Address>(V8_ASSUME_ALIGNED(
+      reinterpret_cast<void*>(base_), kMinExpectedOSPageSize));
+}
+#endif  // V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+
+// static
+Tagged_t ExternalCodeCompressionScheme::CompressTagged(Address tagged) {
+  return static_cast<Tagged_t>(static_cast<uint32_t>(tagged));
+}
+
+// static
+Address ExternalCodeCompressionScheme::DecompressTaggedSigned(
+    Tagged_t raw_value) {
+  // For runtime code the upper 32-bits of the Smi value do not matter.
+  return static_cast<Address>(raw_value);
+}
+
+// static
+template <typename TOnHeapAddress>
+Address ExternalCodeCompressionScheme::DecompressTaggedPointer(
+    TOnHeapAddress on_heap_addr, Tagged_t raw_value) {
+#if defined(V8_COMPRESS_POINTERS_IN_SHARED_CAGE) && \
+    !defined(V8_COMPRESS_POINTERS_DONT_USE_GLOBAL_BASE)
+  Address cage_base = base();
+#else
+  Address cage_base = GetPtrComprCageBaseAddress(on_heap_addr);
+#endif
+  Address diff = static_cast<Address>(static_cast<uint32_t>(raw_value)) -
+                 static_cast<Address>(static_cast<uint32_t>(cage_base));
+  // The cage base value was chosen such that it's less or equal than any
+  // pointer in the cage, thus if we got a negative diff then it means that
+  // the decompressed value is off by 4GB.
+  if (static_cast<intptr_t>(diff) < 0) {
+    diff += size_t{4} * GB;
+  }
+  DCHECK(is_uint32(diff));
+  Address result = cage_base + diff;
+  DCHECK_EQ(static_cast<uint32_t>(result), raw_value);
+  return result;
+}
+
+// static
+template <typename TOnHeapAddress>
+Address ExternalCodeCompressionScheme::DecompressTaggedAny(
+    TOnHeapAddress on_heap_addr, Tagged_t raw_value) {
+  if (HAS_SMI_TAG(raw_value)) return DecompressTaggedSigned(raw_value);
+  if (raw_value == kClearedWeakHeapObjectLower32) return raw_value;
+  return DecompressTaggedPointer(on_heap_addr, raw_value);
+}
+
+#endif  // V8_EXTERNAL_CODE_SPACE
+
+//
+// Misc functions.
+//
+
 PtrComprCageBase::PtrComprCageBase(const Isolate* isolate)
     : address_(isolate->cage_base()) {}
 PtrComprCageBase::PtrComprCageBase(const LocalIsolate* isolate)
@@ -42,7 +211,8 @@ V8_INLINE Address GetPtrComprCageBaseAddress(PtrComprCageBase cage_base) {
 
 V8_INLINE constexpr PtrComprCageBase GetPtrComprCageBaseFromOnHeapAddress(
     Address address) {
-  return PtrComprCageBase(GetPtrComprCageBaseAddress(address));
+  return PtrComprCageBase(
+      V8HeapCompressionScheme::GetPtrComprCageBaseAddress(address));
 }
 
 // Decompresses smi value.
@@ -68,7 +238,6 @@ V8_INLINE Address DecompressTaggedAny(TOnHeapAddress on_heap_addr,
 }
 
 #else
-
 V8_INLINE Tagged_t CompressTagged(Address tagged) { UNREACHABLE(); }
 
 V8_INLINE constexpr PtrComprCageBase GetPtrComprCageBaseFromOnHeapAddress(
diff --git src/common/ptr-compr.cc src/common/ptr-compr.cc
new file mode 100644
index 00000000000..899bb4798a4
--- /dev/null
+++ src/common/ptr-compr.cc
@@ -0,0 +1,19 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/common/ptr-compr.h"
+
+namespace v8::internal {
+
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+
+uintptr_t V8HeapCompressionScheme::base_ = kNullAddress;
+
+#ifdef V8_EXTERNAL_CODE_SPACE
+uintptr_t ExternalCodeCompressionScheme::base_ = kNullAddress;
+#endif  // V8_EXTERNAL_CODE_SPACE
+
+#endif  // V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+
+}  // namespace v8::internal
diff --git src/common/ptr-compr.h src/common/ptr-compr.h
index 58d14602908..c6ff9c50e84 100644
--- src/common/ptr-compr.h
+++ src/common/ptr-compr.h
@@ -11,6 +11,131 @@
 namespace v8 {
 namespace internal {
 
+// This is just a collection of compression scheme related functions. Having
+// such a class allows plugging different decompression scheme in certain
+// places by introducing another CompressionScheme class with a customized
+// implementation. This is useful, for example, for CodeDataContainer::code
+// field (see CodeObjectSlot).
+class V8HeapCompressionScheme {
+ public:
+  V8_INLINE static Address GetPtrComprCageBaseAddress(Address on_heap_addr);
+
+  V8_INLINE static Address GetPtrComprCageBaseAddress(
+      PtrComprCageBase cage_base);
+
+  // Compresses full-pointer representation of a tagged value to on-heap
+  // representation.
+  V8_INLINE static Tagged_t CompressTagged(Address tagged);
+
+  // Decompresses smi value.
+  V8_INLINE static Address DecompressTaggedSigned(Tagged_t raw_value);
+
+  // Decompresses weak or strong heap object pointer or forwarding pointer,
+  // preserving both weak- and smi- tags.
+  template <typename TOnHeapAddress>
+  V8_INLINE static Address DecompressTaggedPointer(TOnHeapAddress on_heap_addr,
+                                                   Tagged_t raw_value);
+  // Decompresses any tagged value, preserving both weak- and smi- tags.
+  template <typename TOnHeapAddress>
+  V8_INLINE static Address DecompressTaggedAny(TOnHeapAddress on_heap_addr,
+                                               Tagged_t raw_value);
+
+  // Given a 64bit raw value, found on the stack, calls the callback function
+  // with all possible pointers that may be "contained" in compressed form in
+  // this value, either as complete compressed pointers or as intermediate
+  // (half-computed) results.
+  template <typename ProcessPointerCallback>
+  V8_INLINE static void ProcessIntermediatePointers(
+      PtrComprCageBase cage_base, Address raw_value,
+      ProcessPointerCallback callback);
+
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+  // Process-wide cage base value used for decompression.
+  V8_INLINE static void InitBase(Address base);
+  V8_INLINE static Address base();
+
+ private:
+  static V8_EXPORT_PRIVATE uintptr_t base_ V8_CONSTINIT;
+#endif  // V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+};
+
+#ifdef V8_EXTERNAL_CODE_SPACE
+
+// Compression scheme used for fields containing Code objects (namely for the
+// CodeDataContainer::code field).
+// Unlike the V8HeapCompressionScheme this one allows the cage to cross 4GB
+// boundary at a price of making decompression slightly more complex.
+// The former outweighs the latter because it gives us more flexibility in
+// allocating the code range closer to .text section in the process address
+// space. At the same time decompression of the external code field happens
+// relatively rarely during GC.
+// The base can be any value such that [base, base + 4GB) contains the whole
+// code range.
+//
+// This scheme works as follows:
+//    --|----------{---------|------}--------------|--
+//     4GB         |        4GB     |             4GB
+//                 +-- code range --+
+//                 |
+//             cage base
+//
+// * Cage base value is OS page aligned for simplicity (although it's not
+//   strictly necessary).
+// * Code range size is smaller than or equal to 4GB.
+// * Compression is just a truncation to 32-bits value.
+// * Decompression of a pointer:
+//   - if "compressed" cage base is <= than compressed value then one just
+//     needs to OR the upper 32-bits of the case base to get the decompressed
+//     value.
+//   - if compressed value is smaller than "compressed" cage base then ORing
+//     the upper 32-bits of the cage base is not enough because the resulting
+//     value will be off by 4GB, which has to be added to the result.
+//   - note that decompression doesn't modify the lower 32-bits of the value.
+// * Decompression of Smi values is made a no-op for simplicity given that
+//   on the hot paths of decompressing the Code pointers it's already known
+//   that the value is not a Smi.
+//
+class ExternalCodeCompressionScheme {
+ public:
+  V8_INLINE static Address PrepareCageBaseAddress(Address on_heap_addr);
+
+  // Note that this compression scheme doesn't allow reconstruction of the cage
+  // base value from any arbitrary value, thus the cage base has to be passed
+  // explicitly to the decompression functions.
+  static Address GetPtrComprCageBaseAddress(Address on_heap_addr) = delete;
+
+  V8_INLINE static Address GetPtrComprCageBaseAddress(
+      PtrComprCageBase cage_base);
+
+  // Compresses full-pointer representation of a tagged value to on-heap
+  // representation.
+  V8_INLINE static Tagged_t CompressTagged(Address tagged);
+
+  // Decompresses smi value.
+  V8_INLINE static Address DecompressTaggedSigned(Tagged_t raw_value);
+
+  // Decompresses weak or strong heap object pointer or forwarding pointer,
+  // preserving both weak- and smi- tags.
+  template <typename TOnHeapAddress>
+  V8_INLINE static Address DecompressTaggedPointer(TOnHeapAddress on_heap_addr,
+                                                   Tagged_t raw_value);
+  // Decompresses any tagged value, preserving both weak- and smi- tags.
+  template <typename TOnHeapAddress>
+  V8_INLINE static Address DecompressTaggedAny(TOnHeapAddress on_heap_addr,
+                                               Tagged_t raw_value);
+
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+  // Process-wide cage base value used for decompression.
+  V8_INLINE static void InitBase(Address base);
+  V8_INLINE static Address base();
+
+ private:
+  static V8_EXPORT_PRIVATE uintptr_t base_;
+#endif  // V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+};
+
+#endif  // V8_EXTERNAL_CODE_SPACE
+
 // Accessors for fields that may be unaligned due to pointer compression.
 
 template <typename V>
diff --git src/compiler/backend/arm64/code-generator-arm64.cc src/compiler/backend/arm64/code-generator-arm64.cc
index b382e7f415e..96bf06f1b72 100644
--- src/compiler/backend/arm64/code-generator-arm64.cc
+++ src/compiler/backend/arm64/code-generator-arm64.cc
@@ -636,7 +636,11 @@ void CodeGenerator::AssembleTailCallAfterGap(Instruction* instr,
   InstructionOperandConverter g(this, instr);
   int optional_padding_offset = g.InputInt32(instr->InputCount() - 2);
   if (optional_padding_offset % 2) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    __ Poke(padreg, optional_padding_offset * kSystemPointerAddrSize);
+#else
     __ Poke(padreg, optional_padding_offset * kSystemPointerSize);
+#endif
   }
 }
 
@@ -796,9 +800,17 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
              fp_mode_ == SaveFPRegsMode::kSave);
       // kReturnRegister0 should have been saved before entering the stub.
       int bytes = __ PushCallerSaved(fp_mode_, kReturnRegister0);
+#if defined(__CHERI_PURE_CAPABILITY__)
+      DCHECK(IsAligned(bytes, kSystemPointerAddrSize));
+#else
       DCHECK(IsAligned(bytes, kSystemPointerSize));
+#endif
       DCHECK_EQ(0, frame_access_state()->sp_delta());
+#if defined(__CHERI_PURE_CAPABILITY__)
+      frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerAddrSize);
+#else
       frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerSize);
+#endif
       DCHECK(!caller_registers_saved_);
       caller_registers_saved_ = true;
       break;
@@ -810,7 +822,11 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
              fp_mode_ == SaveFPRegsMode::kSave);
       // Don't overwrite the returned value.
       int bytes = __ PopCallerSaved(fp_mode_, kReturnRegister0);
+#if defined(__CHERI_PURE_CAPABILITY__)
+      frame_access_state()->IncreaseSPDelta(-(bytes / kSystemPointerAddrSize));
+#else
       frame_access_state()->IncreaseSPDelta(-(bytes / kSystemPointerSize));
+#endif
       DCHECK_EQ(0, frame_access_state()->sp_delta());
       DCHECK(caller_registers_saved_);
       caller_registers_saved_ = false;
@@ -858,7 +874,11 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         //   kArchRestoreCallerRegisters;
         int bytes =
             __ RequiredStackSizeForCallerSaved(fp_mode_, kReturnRegister0);
+#if defined(__CHERI_PURE_CAPABILITY__)
+        frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerAddrSize);
+#else
         frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerSize);
+#endif
       }
       break;
     }
@@ -1494,7 +1514,11 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       break;
     }
     case kArm64Poke: {
+#if defined(__CHERI_PURE_CAPABILITY__)
+      Operand operand(i.InputInt32(1) * kSystemPointerAddrSize);
+#else
       Operand operand(i.InputInt32(1) * kSystemPointerSize);
+#endif
       if (instr->InputAt(0)->IsSimd128Register()) {
         __ Poke(i.InputSimd128Register(0), operand);
       } else if (instr->InputAt(0)->IsFPRegister()) {
@@ -1508,10 +1532,18 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       int slot = i.InputInt32(2) - 1;
       if (instr->InputAt(0)->IsFPRegister()) {
         __ PokePair(i.InputFloat64Register(1), i.InputFloat64Register(0),
+#if defined(__CHERI_PURE_CAPABILITY__)
+                    slot * kSystemPointerAddrSize);
+#else
                     slot * kSystemPointerSize);
+#endif
       } else {
         __ PokePair(i.InputRegister(1), i.InputRegister(0),
+#if defined(__CHERI_PURE_CAPABILITY__)
+                    slot * kSystemPointerAddrSize);
+#else
                     slot * kSystemPointerSize);
+#endif
       }
       break;
     }
@@ -3047,7 +3079,11 @@ void CodeGenerator::FinishFrame(Frame* frame) {
   if (saved_count != 0) {
     DCHECK(saves_fp.bits() == CPURegList::GetCalleeSavedV().bits());
     frame->AllocateSavedCalleeRegisterSlots(saved_count *
+#if defined(__CHERI_PURE_CAPABILITY__)
+                                            (kDoubleSize / kSystemPointerAddrSize));
+#else
                                             (kDoubleSize / kSystemPointerSize));
+#endif
   }
 
   CPURegList saves =
@@ -3081,8 +3117,13 @@ void CodeGenerator::AssembleConstructFrame() {
   if (frame_access_state()->has_frame()) {
     // Link the frame
     if (call_descriptor->IsJSFunctionCall()) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+      static_assert(InterpreterFrameConstants::kFixedFrameSize % 16 == 8);
+      DCHECK_EQ(required_slots % 2, 1);
+#else
       static_assert(InterpreterFrameConstants::kFixedFrameSize % 16 == 8);
       DCHECK_EQ(required_slots % 2, 1);
+#endif
       __ Prologue();
       // Update required_slots count since we have just claimed one extra slot.
       static_assert(TurboAssembler::kExtraSlotClaimedByPrologue == 1);
@@ -3115,7 +3156,11 @@ void CodeGenerator::AssembleConstructFrame() {
     }
 
 #if V8_ENABLE_WEBASSEMBLY
+#if defined(__CHERI_PURE_CAPABILITY__)
+    if (info()->IsWasm() && required_slots * kSystemPointerAddrSize > 4 * KB) {
+#else
     if (info()->IsWasm() && required_slots * kSystemPointerSize > 4 * KB) {
+#endif
       // For WebAssembly functions with big frames we have to do the stack
       // overflow check before we construct the frame. Otherwise we may not
       // have enough space on the stack to call the runtime for the stack
@@ -3124,14 +3169,22 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
+#if defined(__CHERI_PURE_CAPABILITY__)
+      if (required_slots * kSystemPointerAddrSize < FLAG_stack_size * KB) {
+#else
       if (required_slots * kSystemPointerSize < FLAG_stack_size * KB) {
+#endif
         UseScratchRegisterScope scope(tasm());
         Register scratch = scope.AcquireX();
         __ Ldr(scratch, FieldMemOperand(
                             kWasmInstanceRegister,
                             WasmInstanceObject::kRealStackLimitAddressOffset));
         __ Ldr(scratch, MemOperand(scratch));
+#if defined(__CHERI_PURE_CAPABILITY__)
+        __ Add(scratch, scratch, required_slots * kSystemPointerAddrSize);
+#else
         __ Add(scratch, scratch, required_slots * kSystemPointerSize);
+#endif
         __ Cmp(sp, scratch);
         __ B(hs, &done);
       }
@@ -3432,7 +3485,11 @@ void CodeGenerator::MoveToTempLocation(InstructionOperand* source) {
           frame_access_state_->frame()->GetTotalFrameSlotCount() - 1;
       int sp_delta = frame_access_state_->sp_delta();
       int temp_slot = last_frame_slot_id + sp_delta + new_slots;
+#if defined(__CHERI_PURE_CAPABILITY__)
+      __ Sub(sp, sp, Operand(new_slots * kSystemPointerAddrSize));
+#else
       __ Sub(sp, sp, Operand(new_slots * kSystemPointerSize));
+#endif
       AllocatedOperand temp(LocationOperand::STACK_SLOT, rep, temp_slot);
       AssembleMove(source, &temp);
     }
@@ -3480,7 +3537,11 @@ void CodeGenerator::MoveTempLocationTo(InstructionOperand* dest,
       int temp_slot = last_frame_slot_id + sp_delta + new_slots;
       AllocatedOperand temp(LocationOperand::STACK_SLOT, rep, temp_slot);
       AssembleMove(&temp, dest);
+#if defined(__CHERI_PURE_CAPABILITY__)
+      __ Add(sp, sp, Operand(new_slots * kSystemPointerAddrSize));
+#else
       __ Add(sp, sp, Operand(new_slots * kSystemPointerSize));
+#endif
     }
   }
   // Restore the default state to release the {UseScratchRegisterScope} and to
diff --git src/compiler/backend/arm64/unwinding-info-writer-arm64.cc src/compiler/backend/arm64/unwinding-info-writer-arm64.cc
index bc3a91a769a..c0fa0ecb04b 100644
--- src/compiler/backend/arm64/unwinding-info-writer-arm64.cc
+++ src/compiler/backend/arm64/unwinding-info-writer-arm64.cc
@@ -9,6 +9,8 @@ namespace v8 {
 namespace internal {
 namespace compiler {
 
+constexpr int kPointerAddrSize = kSystemPointerAddrSize;
+
 // TODO(v8:10026): When using CFI, we need to generate unwinding info to tell
 // the unwinder that return addresses are signed.
 
@@ -26,7 +28,11 @@ void UnwindingInfoWriter::BeginInstructionBlock(int pc_offset,
   if (initial_state->saved_lr_ != saved_lr_) {
     eh_frame_writer_.AdvanceLocation(pc_offset);
     if (initial_state->saved_lr_) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+      eh_frame_writer_.RecordRegisterSavedToStack(lr, kSystemPointerAddrSize);
+#else
       eh_frame_writer_.RecordRegisterSavedToStack(lr, kSystemPointerSize);
+#endif
       eh_frame_writer_.RecordRegisterSavedToStack(fp, 0);
     } else {
       eh_frame_writer_.RecordRegisterFollowsInitialRule(lr);
@@ -72,7 +78,11 @@ void UnwindingInfoWriter::MarkFrameConstructed(int at_pc) {
   // The LR is pushed on the stack, and we can record this fact at the end of
   // the construction, since the LR itself is not modified in the process.
   eh_frame_writer_.AdvanceLocation(at_pc);
+#if defined(__CHERI_PURE_CAPABILITY__)
+  eh_frame_writer_.RecordRegisterSavedToStack(lr, kSystemPointerAddrSize);
+#else
   eh_frame_writer_.RecordRegisterSavedToStack(lr, kSystemPointerSize);
+#endif
   eh_frame_writer_.RecordRegisterSavedToStack(fp, 0);
   saved_lr_ = true;
 }
diff --git src/compiler/backend/instruction-selector-impl.h src/compiler/backend/instruction-selector-impl.h
index d8abfb21b2c..5779bc15b0f 100644
--- src/compiler/backend/instruction-selector-impl.h
+++ src/compiler/backend/instruction-selector-impl.h
@@ -354,10 +354,14 @@ class OperandGenerator {
       case IrOpcode::kExternalConstant:
         return Constant(OpParameter<ExternalReference>(node->op()));
       case IrOpcode::kComment: {
-        // We cannot use {intptr_t} here, since the Constant constructor would
-        // be ambiguous on some architectures.
+#if defined(__CHERI_PURE_CAPABILITY__)
+        using ptrsize_int_t = intptr_t;
+#else
         using ptrsize_int_t =
             std::conditional<kSystemPointerSize == 8, int64_t, int32_t>::type;
+#endif	
+        // We cannot use {intptr_t} here, since the Constant constructor would
+        // be ambiguous on some architectures.
         return Constant(reinterpret_cast<ptrsize_int_t>(
             OpParameter<const char*>(node->op())));
       }
diff --git src/compiler/backend/instruction.cc src/compiler/backend/instruction.cc
index d98cdea35d3..044a83c616f 100644
--- src/compiler/backend/instruction.cc
+++ src/compiler/backend/instruction.cc
@@ -12,6 +12,7 @@
 #include "src/codegen/machine-type.h"
 #include "src/codegen/register-configuration.h"
 #include "src/codegen/source-position.h"
+#include "src/common/cheri.h"
 #include "src/compiler/common-operator.h"
 #include "src/compiler/graph.h"
 #include "src/compiler/node.h"
@@ -29,6 +30,7 @@
 namespace v8 {
 namespace internal {
 namespace compiler {
+using cheri::operator<<;
 
 const RegisterConfiguration* (*GetRegConfig)() = RegisterConfiguration::Default;
 
diff --git src/compiler/backend/instruction.h src/compiler/backend/instruction.h
index bed43dc6363..fb9d5814e47 100644
--- src/compiler/backend/instruction.h
+++ src/compiler/backend/instruction.h
@@ -1113,11 +1113,15 @@ class V8_EXPORT_PRIVATE Constant final {
     kCompressedHeapObject,
     kHeapObject,
     kRpoNumber,
-    kDelayedStringConstant
+    kDelayedStringConstant,
+    kIntPtr
   };
 
   explicit Constant(int32_t v);
   explicit Constant(int64_t v) : type_(kInt64), value_(v) {}
+#ifdef __CHERI_PURE_CAPABILITY__
+  explicit Constant(intptr_t v) : type_(kIntPtr), value_(v) {}
+#endif
   explicit Constant(float v)
       : type_(kFloat32), value_(base::bit_cast<int32_t>(v)) {}
   explicit Constant(double v)
@@ -1172,7 +1176,7 @@ class V8_EXPORT_PRIVATE Constant final {
 
   base::Double ToFloat64() const {
     DCHECK_EQ(kFloat64, type());
-    return base::Double(base::bit_cast<uint64_t>(value_));
+    return base::Double(base::bit_cast<uint64_t>(static_cast<uint64_t>(value_)));
   }
 
   ExternalReference ToExternalReference() const {
@@ -1192,7 +1196,11 @@ class V8_EXPORT_PRIVATE Constant final {
  private:
   Type type_;
   RelocInfo::Mode rmode_ = RelocInfo::NO_INFO;
+#ifdef __CHERI_PURE_CAPABILITY__
+  intptr_t value_;
+#else
   int64_t value_;
+#endif
 };
 
 std::ostream& operator<<(std::ostream&, const Constant&);
diff --git src/compiler/backend/mid-tier-register-allocator.cc src/compiler/backend/mid-tier-register-allocator.cc
index 2ba8500ceb3..8a7f99caba0 100644
--- src/compiler/backend/mid-tier-register-allocator.cc
+++ src/compiler/backend/mid-tier-register-allocator.cc
@@ -13,6 +13,7 @@
 #include "src/codegen/machine-type.h"
 #include "src/codegen/register-configuration.h"
 #include "src/codegen/tick-counter.h"
+#include "src/common/cheri.h"
 #include "src/compiler/backend/instruction.h"
 #include "src/compiler/linkage.h"
 #include "src/logging/counters.h"
@@ -20,6 +21,7 @@
 namespace v8 {
 namespace internal {
 namespace compiler {
+using cheri::operator<<;
 
 class RegisterState;
 class DeferredBlocksRegion;
@@ -1320,7 +1322,11 @@ class RegisterBitVector {
 
   static_assert(RegisterConfiguration::kMaxRegisters <= sizeof(uintptr_t) * 8,
                 "Maximum registers must fit in uintptr_t bitmap");
+#ifdef __CHERI_PURE_CAPABILITY__
+  __attribute__((cheri_no_provenance) )uintptr_t bits_;
+#else
   uintptr_t bits_;
+#endif
 };
 
 std::ostream& operator<<(std::ostream& os, RegisterBitVector register_bits) {
diff --git src/compiler/common-operator.cc src/compiler/common-operator.cc
index 457f04b9ba5..427640f9f1e 100644
--- src/compiler/common-operator.cc
+++ src/compiler/common-operator.cc
@@ -5,6 +5,7 @@
 #include "src/compiler/common-operator.h"
 
 #include "src/base/lazy-instance.h"
+#include "src/common/cheri.h"
 #include "src/compiler/linkage.h"
 #include "src/compiler/node.h"
 #include "src/compiler/opcodes.h"
@@ -14,6 +15,7 @@
 
 namespace v8 {
 namespace internal {
+using cheri::operator<<;
 
 std::ostream& operator<<(std::ostream& os, BranchHint hint) {
   switch (hint) {
diff --git src/compiler/effect-control-linearizer.cc src/compiler/effect-control-linearizer.cc
index 5825045052b..3b809f526a9 100644
--- src/compiler/effect-control-linearizer.cc
+++ src/compiler/effect-control-linearizer.cc
@@ -4925,8 +4925,16 @@ Node* EffectControlLinearizer::AdaptFastCallTypedArrayArgument(
   static_assert(kSize == sizeof(FastApiTypedArray<double>),
                 "Size mismatch between different specializations of "
                 "FastApiTypedArray");
+#if __CHERI_PURE_CAPABILITY__
+  // Under the stronger alignment assumptions of CHERI the size of the
+  // FastApiTypeArray includes an additional padding of length size_t.
+  // Padding is applied to ensure correct alignment of the data field.
+  static_assert(
+      kSize == sizeof(uintptr_t) + sizeof(size_t) + sizeof(size_t),
+#else
   static_assert(
       kSize == sizeof(uintptr_t) + sizeof(size_t),
+#endif
       "The size of "
       "FastApiTypedArray isn't equal to the sum of its expected members.");
   Node* stack_slot = __ StackSlot(kSize, kAlign);
@@ -4937,7 +4945,15 @@ Node* EffectControlLinearizer::AdaptFastCallTypedArrayArgument(
   __ Store(StoreRepresentation(MachineType::PointerRepresentation(),
                                kNoWriteBarrier),
            stack_slot, sizeof(size_t), data_ptr);
+#ifdef __CHERI_PURE_CAPABILITY__
+  // MachineType::PointerRepresentation only supports 4 and 8 byte pointers.
+  // This will require adaption to 16 bytes pointers. As the current priortity
+  // is to compile a working interpreter, ignore this for the moment. 
+  // See src/codegen/machine-type.h
+  static_assert(sizeof(ptraddr_t) == sizeof(size_t),
+#else
   static_assert(sizeof(uintptr_t) == sizeof(size_t),
+#endif
                 "The buffer length can't "
                 "fit the PointerRepresentation used to store it.");
 
diff --git src/compiler/heap-refs.cc src/compiler/heap-refs.cc
index a9137667f6a..2a3c8097cda 100644
--- src/compiler/heap-refs.cc
+++ src/compiler/heap-refs.cc
@@ -13,6 +13,7 @@
 #include "src/base/optional.h"
 #include "src/base/platform/platform.h"
 #include "src/codegen/code-factory.h"
+#include "src/common/cheri.h"
 #include "src/compiler/compilation-dependencies.h"
 #include "src/compiler/js-heap-broker.h"
 #include "src/execution/protectors-inl.h"
@@ -26,6 +27,7 @@
 
 namespace v8 {
 namespace internal {
+using cheri::operator<<;
 namespace compiler {
 
 #define TRACE(broker, x) TRACE_BROKER(broker, x)
@@ -2174,7 +2176,7 @@ CodeRef JSFunctionRef::code() const {
   CodeT code = object()->code(kAcquireLoad);
   // Safe to do a relaxed conversion to Code here since CodeT::code field is
   // modified only by GC and the CodeT was acquire-loaded.
-  return MakeRefAssumeMemoryFence(broker(), FromCodeT(code, kRelaxedLoad));
+  return MakeRefAssumeMemoryFence(broker(), FromCodeT(code, GetPtrComprCageBase(code), kRelaxedLoad));
 }
 
 NativeContextRef JSFunctionRef::native_context() const {
diff --git src/compiler/machine-graph.cc src/compiler/machine-graph.cc
index 34464cfb052..65c94bd63a7 100644
--- src/compiler/machine-graph.cc
+++ src/compiler/machine-graph.cc
@@ -68,7 +68,7 @@ Node* MachineGraph::RelocatableInt64Constant(int64_t value,
 
 Node* MachineGraph::RelocatableIntPtrConstant(intptr_t value,
                                               RelocInfo::Mode rmode) {
-  return kSystemPointerSize == 8
+  return kSystemPointerAddrSize == 8
              ? RelocatableInt64Constant(value, rmode)
              : RelocatableInt32Constant(static_cast<int>(value), rmode);
 }
diff --git src/compiler/operator.h src/compiler/operator.h
index f641394eb1f..e73f8f860d5 100644
--- src/compiler/operator.h
+++ src/compiler/operator.h
@@ -10,6 +10,7 @@
 #include "src/base/compiler-specific.h"
 #include "src/base/flags.h"
 #include "src/base/functional.h"
+#include "src/common/cheri.h"
 #include "src/common/globals.h"
 #include "src/handles/handles.h"
 #include "src/objects/feedback-cell.h"
@@ -18,6 +19,7 @@
 namespace v8 {
 namespace internal {
 namespace compiler {
+using cheri::operator<<;
 
 // An operator represents description of the "computation" of a node in the
 // compiler IR. A computation takes values (i.e. data) as input and produces
diff --git src/compiler/raw-machine-assembler.cc src/compiler/raw-machine-assembler.cc
index 21f8c184f36..3bd73839e9b 100644
--- src/compiler/raw-machine-assembler.cc
+++ src/compiler/raw-machine-assembler.cc
@@ -73,7 +73,7 @@ Node* RawMachineAssembler::UndefinedConstant() {
 
 Node* RawMachineAssembler::RelocatableIntPtrConstant(intptr_t value,
                                                      RelocInfo::Mode rmode) {
-  return kSystemPointerSize == 8
+  return kSystemPointerAddrSize == 8
              ? RelocatableInt64Constant(value, rmode)
              : RelocatableInt32Constant(static_cast<int>(value), rmode);
 }
diff --git src/compiler/raw-machine-assembler.h src/compiler/raw-machine-assembler.h
index 787f7f92698..4a1338ce699 100644
--- src/compiler/raw-machine-assembler.h
+++ src/compiler/raw-machine-assembler.h
@@ -89,8 +89,8 @@ class V8_EXPORT_PRIVATE RawMachineAssembler {
   }
   Node* IntPtrConstant(intptr_t value) {
     // TODO(dcarney): mark generated code as unserializable if value != 0.
-    return kSystemPointerSize == 8 ? Int64Constant(value)
-                                   : Int32Constant(static_cast<int>(value));
+    return kSystemPointerAddrSize == 8 ? Int64Constant(value)
+                                       : Int32Constant(static_cast<int>(value));
   }
   Node* RelocatableIntPtrConstant(intptr_t value, RelocInfo::Mode rmode);
   Node* Int32Constant(int32_t value) {
@@ -586,8 +586,8 @@ class V8_EXPORT_PRIVATE RawMachineAssembler {
 
 #define INTPTR_BINOP(prefix, name)                           \
   Node* IntPtr##name(Node* a, Node* b) {                     \
-    return kSystemPointerSize == 8 ? prefix##64##name(a, b)  \
-                                   : prefix##32##name(a, b); \
+    return kSystemPointerAddrSize == 8 ? prefix##64##name(a, b)  \
+                                       : prefix##32##name(a, b); \
   }
 
   INTPTR_BINOP(Int, Add)
@@ -607,8 +607,8 @@ class V8_EXPORT_PRIVATE RawMachineAssembler {
 
 #define UINTPTR_BINOP(prefix, name)                          \
   Node* UintPtr##name(Node* a, Node* b) {                    \
-    return kSystemPointerSize == 8 ? prefix##64##name(a, b)  \
-                                   : prefix##32##name(a, b); \
+    return kSystemPointerAddrSize == 8 ? prefix##64##name(a, b)  \
+                                       : prefix##32##name(a, b); \
   }
 
   UINTPTR_BINOP(Uint, LessThan)
@@ -627,8 +627,8 @@ class V8_EXPORT_PRIVATE RawMachineAssembler {
   }
 
   Node* IntPtrAbsWithOverflow(Node* a) {
-    return kSystemPointerSize == 8 ? Int64AbsWithOverflow(a)
-                                   : Int32AbsWithOverflow(a);
+    return kSystemPointerAddrSize == 8 ? Int64AbsWithOverflow(a)
+                                       : Int32AbsWithOverflow(a);
   }
 
   Node* Float32Add(Node* a, Node* b) {
diff --git src/compiler/simplified-operator.cc src/compiler/simplified-operator.cc
index 6e47bca0d94..f5289687f01 100644
--- src/compiler/simplified-operator.cc
+++ src/compiler/simplified-operator.cc
@@ -6,6 +6,7 @@
 
 #include "include/v8-fast-api-calls.h"
 #include "src/base/lazy-instance.h"
+#include "src/common/cheri.h"
 #include "src/compiler/linkage.h"
 #include "src/compiler/opcodes.h"
 #include "src/compiler/operator.h"
@@ -23,6 +24,7 @@
 namespace v8 {
 namespace internal {
 namespace compiler {
+using cheri::operator<<;
 
 size_t hash_value(BaseTaggedness base_taggedness) {
   return static_cast<uint8_t>(base_taggedness);
diff --git src/compiler/types.h src/compiler/types.h
index dd7203046c4..fe4d694b4e2 100644
--- src/compiler/types.h
+++ src/compiler/types.h
@@ -572,7 +572,7 @@ class V8_EXPORT_PRIVATE Type {
 
   // If LSB is set, the payload is a bitset; if LSB is clear, the payload is
   // a pointer to a subtype of the TypeBase class.
-  uint64_t payload_;
+  uintptr_t payload_;
 };
 
 inline size_t hash_value(Type type) { return type.payload_; }
diff --git src/debug/debug.cc src/debug/debug.cc
index 1c2f738d23e..e45f5c1cbc3 100644
--- src/debug/debug.cc
+++ src/debug/debug.cc
@@ -388,7 +388,11 @@ void Debug::ThreadInit() {
   clear_restart_frame();
   clear_suspended_generator();
   base::Relaxed_Store(&thread_local_.current_debug_scope_,
+#if defined(__CHERI_PURE_CAPABILITY__)
+                      static_cast<base::AtomicIntPtr>(0));
+#else
                       static_cast<base::AtomicWord>(0));
+#endif
   thread_local_.break_on_next_function_call_ = false;
   UpdateHookOnFunctionCall();
   thread_local_.promise_stack_ = Smi::zero();
@@ -2591,7 +2595,11 @@ DebugScope::DebugScope(Debug* debug)
       no_interrupts_(debug_->isolate_) {
   // Link recursive debugger entry.
   base::Relaxed_Store(&debug_->thread_local_.current_debug_scope_,
+#if defined(__CHERI_PURE_CAPABILITY__)
+                      reinterpret_cast<base::AtomicIntPtr>(this));
+#else
                       reinterpret_cast<base::AtomicWord>(this));
+#endif
   // Store the previous frame id and return value.
   break_frame_id_ = debug_->break_frame_id();
 
@@ -2619,7 +2627,11 @@ DebugScope::~DebugScope() {
   }
   // Leaving this debugger entry.
   base::Relaxed_Store(&debug_->thread_local_.current_debug_scope_,
+#if defined(__CHERI_PURE_CAPABILITY__)
+                      reinterpret_cast<base::AtomicIntPtr>(prev_));
+#else
                       reinterpret_cast<base::AtomicWord>(prev_));
+#endif
 
   // Restore to the previous break state.
   debug_->thread_local_.break_frame_id_ = break_frame_id_;
diff --git src/diagnostics/objects-debug.cc src/diagnostics/objects-debug.cc
index 1512e8cee46..b06253407b4 100644
--- src/diagnostics/objects-debug.cc
+++ src/diagnostics/objects-debug.cc
@@ -1065,20 +1065,20 @@ void CodeDataContainer::CodeDataContainerVerify(Isolate* isolate) {
   VerifyObjectField(isolate, kNextCodeLinkOffset);
   CHECK(next_code_link().IsCodeT() || next_code_link().IsUndefined(isolate));
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-    if (raw_code() != Smi::zero()) {
+    if (raw_code(PtrComprCageBase(isolate->code_cage_base())) != Smi::zero()) {
 #ifdef V8_EXTERNAL_CODE_SPACE
       // kind and builtin_id() getters are not available on CodeDataContainer
       // when external code space is not enabled.
-      CHECK_EQ(code().kind(), kind());
-      CHECK_EQ(code().builtin_id(), builtin_id());
+      CHECK_EQ(code(PtrComprCageBase(isolate->code_cage_base())).kind(), kind());
+      CHECK_EQ(code(PtrComprCageBase(isolate->code_cage_base())).builtin_id(), builtin_id());
 #endif  // V8_EXTERNAL_CODE_SPACE
-      CHECK_EQ(code().code_data_container(kAcquireLoad), *this);
+      CHECK_EQ(code(PtrComprCageBase(isolate->code_cage_base())).code_data_container(kAcquireLoad), *this);
 
       // Ensure the cached code entry point corresponds to the Code object
       // associated with this CodeDataContainer.
 #ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
       if (V8_SHORT_BUILTIN_CALLS_BOOL) {
-        if (code().InstructionStart() == code_entry_point()) {
+        if (code(PtrComprCageBase(isolate->code_cage_base())).InstructionStart() == code_entry_point()) {
           // Most common case, all good.
         } else {
           // When shared pointer compression cage is enabled and it has the
@@ -1093,7 +1093,8 @@ void CodeDataContainer::CodeDataContainerVerify(Isolate* isolate) {
               isolate->heap()->GcSafeFindCodeForInnerPointer(
                   code_entry_point());
           CHECK(lookup_result.IsFound());
-          CHECK_EQ(lookup_result.ToCode(), code());
+          CHECK_EQ(lookup_result.ToCode(PtrComprCageBase(isolate->code_cage_base())),
+	           code(PtrComprCageBase(isolate->code_cage_base())));
         }
       } else {
         CHECK_EQ(code().InstructionStart(), code_entry_point());
@@ -1122,7 +1123,7 @@ void Code::CodeVerify(Isolate* isolate) {
   CHECK_IMPLIES(!ReadOnlyHeap::Contains(*this),
                 IsAligned(raw_instruction_start(), kCodeAlignment));
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-    CHECK_EQ(*this, code_data_container(kAcquireLoad).code());
+    CHECK_EQ(*this, code_data_container(kAcquireLoad).code(PtrComprCageBase(isolate->code_cage_base())));
   }
   // TODO(delphick): Refactor Factory::CodeBuilder::BuildInternal, so that the
   // following CHECK works builtin trampolines. It currently fails because
diff --git src/diagnostics/objects-printer.cc src/diagnostics/objects-printer.cc
index cf7202949e4..f9b55a7e396 100644
--- src/diagnostics/objects-printer.cc
+++ src/diagnostics/objects-printer.cc
@@ -5,6 +5,7 @@
 #include <iomanip>
 #include <memory>
 
+#include "src/common/cheri.h"
 #include "src/common/globals.h"
 #include "src/diagnostics/disasm.h"
 #include "src/diagnostics/disassembler.h"
@@ -29,6 +30,7 @@
 
 namespace v8 {
 namespace internal {
+using cheri::operator<<;
 
 #ifdef OBJECT_PRINT
 
@@ -1727,7 +1729,7 @@ void CodeDataContainer::CodeDataContainerPrint(std::ostream& os) {
   PrintHeader(os, "CodeDataContainer");
   os << "\n - kind_specific_flags: " << kind_specific_flags(kRelaxedLoad);
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-    os << "\n - code: " << Brief(code());
+    os << "\n - code: " << Brief(code(PtrComprCageBase(GetIsolate()->code_cage_base())));
     os << "\n - code_entry_point: "
        << reinterpret_cast<void*>(code_entry_point());
   }
@@ -1865,7 +1867,7 @@ void WasmStruct::WasmStructPrint(std::ostream& os) {
       case wasm::kRtt: {
         Tagged_t raw = base::ReadUnalignedValue<Tagged_t>(field_address);
 #if V8_COMPRESS_POINTERS
-        Address obj = DecompressTaggedPointer(address(), raw);
+        Address obj = V8HeapCompressionScheme::DecompressTagged(address(), raw);
 #else
         Address obj = raw;
 #endif
@@ -2858,8 +2860,8 @@ inline i::Object GetObjectFromRaw(void* object) {
   if (RoundDown<i::kPtrComprCageBaseAlignment>(object_ptr) == i::kNullAddress) {
     // Try to decompress pointer.
     i::Isolate* isolate = i::Isolate::Current();
-    object_ptr =
-        i::DecompressTaggedAny(isolate, static_cast<i::Tagged_t>(object_ptr));
+    object_ptr = i::V8HeapCompressionScheme::DecompressTaggedAny(
+        isolate, static_cast<i::Tagged_t>(object_ptr));
   }
 #endif
   return i::Object(object_ptr);
@@ -2926,7 +2928,8 @@ V8_EXPORT_PRIVATE extern void _v8_internal_Print_Code(void* object) {
     return;
   }
 
-  i::Code code = lookup_result.ToCode();
+  i::Code code = lookup_result.ToCode(
+    v8::internal::PtrComprCageBase(isolate->code_cage_base()));
 
 #ifdef ENABLE_DISASSEMBLER
   i::StdoutStream os;
diff --git src/execution/arm64/frame-constants-arm64.cc src/execution/arm64/frame-constants-arm64.cc
index 96f6f25e757..0fa8bc5b95a 100644
--- src/execution/arm64/frame-constants-arm64.cc
+++ src/execution/arm64/frame-constants-arm64.cc
@@ -20,11 +20,15 @@ Register JavaScriptFrame::context_register() { return cp; }
 Register JavaScriptFrame::constant_pool_pointer_register() { UNREACHABLE(); }
 
 int UnoptimizedFrameConstants::RegisterStackSlotCount(int register_count) {
+#ifdef __CHERI_PURE_CAPABILITY__
+  return register_count;
+#else
   static_assert(InterpreterFrameConstants::kFixedFrameSize % 16 == 8);
   // Interpreter frame header size is not 16-bytes aligned, so we'll need at
   // least one register slot to make the frame a multiple of 16 bytes. The code
   // below is equivalent to "RoundUp(register_count - 1, 2) + 1".
   return RoundDown(register_count, 2) + 1;
+#endif
 }
 
 int BuiltinContinuationFrameConstants::PaddingSlotCount(int register_count) {
diff --git src/execution/arm64/frame-constants-arm64.h src/execution/arm64/frame-constants-arm64.h
index 80bcda9de27..fd917c5b939 100644
--- src/execution/arm64/frame-constants-arm64.h
+++ src/execution/arm64/frame-constants-arm64.h
@@ -49,14 +49,23 @@ class EntryFrameConstants : public AllStatic {
  public:
   // This is the offset to where JSEntry pushes the current value of
   // Isolate::c_entry_fp onto the stack.
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static constexpr int kCallerFPOffset = -3 * kSystemPointerAddrSize;
+  static constexpr int kFixedFrameSize = 4 * kSystemPointerAddrSize;
+#else
   static constexpr int kCallerFPOffset = -3 * kSystemPointerSize;
   static constexpr int kFixedFrameSize = 4 * kSystemPointerSize;
+#endif
 
   // The following constants are defined so we can static-assert their values
   // near the relevant JSEntry assembly code, not because they're actually very
   // useful.
   static constexpr int kCalleeSavedRegisterBytesPushedBeforeFpLrPair =
+#if defined(__CHERI_PURE_CAPABILITY__)
+      18 * kSystemPointerAddrSize;
+#else
       18 * kSystemPointerSize;
+#endif
   static constexpr int kCalleeSavedRegisterBytesPushedAfterFpLrPair = 0;
   static constexpr int kOffsetToCalleeSavedRegisters = 0;
 
@@ -66,9 +75,17 @@ class EntryFrameConstants : public AllStatic {
       kCalleeSavedRegisterBytesPushedAfterFpLrPair +
       kOffsetToCalleeSavedRegisters;
   static constexpr int kDirectCallerPCOffset =
+#if defined(__CHERI_PURE_CAPABILITY__)
+      kDirectCallerFPOffset + 1 * kSystemPointerAddrSize;
+#else
       kDirectCallerFPOffset + 1 * kSystemPointerSize;
+#endif
   static constexpr int kDirectCallerSPOffset =
+#if defined(__CHERI_PURE_CAPABILITY__)
+      kDirectCallerPCOffset + 1 * kSystemPointerAddrSize +
+#else
       kDirectCallerPCOffset + 1 * kSystemPointerSize +
+#endif
       kCalleeSavedRegisterBytesPushedBeforeFpLrPair;
 };
 
@@ -86,7 +103,11 @@ class WasmCompileLazyFrameConstants : public TypedFrameConstants {
   static constexpr int kFixedFrameSizeFromFp =
       // Header is padded to 16 byte (see {MacroAssembler::EnterFrame}).
       RoundUp<16>(TypedFrameConstants::kFixedFrameSizeFromFp) +
+#if defined(__CHERI_PURE_CAPABILITY__)
+      kNumberOfSavedGpParamRegs * kSystemPointerAddrSize +
+#else
       kNumberOfSavedGpParamRegs * kSystemPointerSize +
+#endif
       kNumberOfSavedFpParamRegs * kSimd128Size;
 };
 
@@ -116,7 +137,11 @@ class WasmDebugBreakFrameConstants : public TypedFrameConstants {
   static constexpr int kLastPushedGpRegisterOffset =
       // Header is padded to 16 byte (see {MacroAssembler::EnterFrame}).
       -RoundUp<16>(TypedFrameConstants::kFixedFrameSizeFromFp) -
+#if defined(__CHERI_PURE_CAPABILITY__)
+      kSystemPointerAddrSize * kNumPushedGpRegisters;
+#else
       kSystemPointerSize * kNumPushedGpRegisters;
+#endif
   static constexpr int kLastPushedFpRegisterOffset =
       kLastPushedGpRegisterOffset - kSimd128Size * kNumPushedFpRegisters;
 
@@ -126,7 +151,11 @@ class WasmDebugBreakFrameConstants : public TypedFrameConstants {
     uint32_t lower_regs =
         kPushedGpRegs.bits() & ((uint32_t{1} << reg_code) - 1);
     return kLastPushedGpRegisterOffset +
+#if defined(__CHERI_PURE_CAPABILITY__)
+           base::bits::CountPopulation(lower_regs) * kSystemPointerAddrSize;
+#else
            base::bits::CountPopulation(lower_regs) * kSystemPointerSize;
+#endif
   }
 
   static int GetPushedFpRegisterOffset(int reg_code) {
diff --git src/execution/frame-constants.h src/execution/frame-constants.h
index c2fd4d57c2c..a1d4848671d 100644
--- src/execution/frame-constants.h
+++ src/execution/frame-constants.h
@@ -11,6 +11,12 @@
 namespace v8 {
 namespace internal {
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+constexpr int kPointerAddrSize = kSystemPointerAddrSize;
+#else
+constexpr int kPointerAddrSize = kSystemPointerSize;
+#endif
+
 // Every pointer in a frame has a slot id. On 32-bit platforms, doubles consume
 // two slots.
 //
@@ -50,7 +56,7 @@ namespace internal {
 //
 class CommonFrameConstants : public AllStatic {
  public:
-  static constexpr int kCallerFPOffset = 0 * kSystemPointerSize;
+  static constexpr int kCallerFPOffset = 0 * kPointerAddrSize;
   static constexpr int kCallerPCOffset = kCallerFPOffset + 1 * kFPOnStackSize;
   static constexpr int kCallerSPOffset = kCallerPCOffset + 1 * kPCOnStackSize;
 
@@ -60,13 +66,13 @@ class CommonFrameConstants : public AllStatic {
   // is the last object pointer.
   static constexpr int kFixedFrameSizeAboveFp = kPCOnStackSize + kFPOnStackSize;
   static constexpr int kFixedSlotCountAboveFp =
-      kFixedFrameSizeAboveFp / kSystemPointerSize;
+      kFixedFrameSizeAboveFp / kPointerAddrSize;
   static constexpr int kCPSlotSize =
-      FLAG_enable_embedded_constant_pool.value() ? kSystemPointerSize : 0;
-  static constexpr int kCPSlotCount = kCPSlotSize / kSystemPointerSize;
+      FLAG_enable_embedded_constant_pool.value() ? kPointerAddrSize : 0;
+  static constexpr int kCPSlotCount = kCPSlotSize / kPointerAddrSize;
   static constexpr int kConstantPoolOffset =
-      kCPSlotSize ? -1 * kSystemPointerSize : 0;
-  static constexpr int kContextOrFrameTypeSize = kSystemPointerSize;
+      kCPSlotSize ? -1 * kPointerAddrSize : 0;
+  static constexpr int kContextOrFrameTypeSize = kPointerAddrSize;
   static constexpr int kContextOrFrameTypeOffset =
       -(kCPSlotSize + kContextOrFrameTypeSize);
 };
@@ -109,17 +115,17 @@ class CommonFrameConstants : public AllStatic {
 class StandardFrameConstants : public CommonFrameConstants {
  public:
   static constexpr int kFixedFrameSizeFromFp =
-      3 * kSystemPointerSize + kCPSlotSize;
+      3 * kPointerAddrSize + kCPSlotSize;
   static constexpr int kFixedFrameSize =
       kFixedFrameSizeAboveFp + kFixedFrameSizeFromFp;
   static constexpr int kFixedSlotCountFromFp =
-      kFixedFrameSizeFromFp / kSystemPointerSize;
-  static constexpr int kFixedSlotCount = kFixedFrameSize / kSystemPointerSize;
+      kFixedFrameSizeFromFp / kPointerAddrSize;
+  static constexpr int kFixedSlotCount = kFixedFrameSize / kPointerAddrSize;
   static constexpr int kContextOffset = kContextOrFrameTypeOffset;
-  static constexpr int kFunctionOffset = -2 * kSystemPointerSize - kCPSlotSize;
-  static constexpr int kArgCOffset = -3 * kSystemPointerSize - kCPSlotSize;
+  static constexpr int kFunctionOffset = -2 * kPointerAddrSize - kCPSlotSize;
+  static constexpr int kArgCOffset = -3 * kPointerAddrSize - kCPSlotSize;
   static constexpr int kExpressionsOffset =
-      -4 * kSystemPointerSize - kCPSlotSize;
+      -4 * kPointerAddrSize - kCPSlotSize;
   static constexpr int kFirstPushedFrameValueOffset = kExpressionsOffset;
   static constexpr int kLastObjectOffset = kContextOffset;
 };
@@ -161,30 +167,30 @@ class TypedFrameConstants : public CommonFrameConstants {
   static constexpr int kFrameTypeOffset = kContextOrFrameTypeOffset;
   static constexpr int kFixedFrameSizeFromFp = kCPSlotSize + kFrameTypeSize;
   static constexpr int kFixedSlotCountFromFp =
-      kFixedFrameSizeFromFp / kSystemPointerSize;
+      kFixedFrameSizeFromFp / kPointerAddrSize;
   static constexpr int kFixedFrameSize =
       StandardFrameConstants::kFixedFrameSizeAboveFp + kFixedFrameSizeFromFp;
-  static constexpr int kFixedSlotCount = kFixedFrameSize / kSystemPointerSize;
+  static constexpr int kFixedSlotCount = kFixedFrameSize / kPointerAddrSize;
   static constexpr int kFirstPushedFrameValueOffset =
-      -kFixedFrameSizeFromFp - kSystemPointerSize;
+      -kFixedFrameSizeFromFp - kPointerAddrSize;
 };
 
 #define FRAME_PUSHED_VALUE_OFFSET(parent, x) \
-  (parent::kFirstPushedFrameValueOffset - (x)*kSystemPointerSize)
+  (parent::kFirstPushedFrameValueOffset - (x)*kPointerAddrSize)
 #define FRAME_SIZE(parent, count) \
-  (parent::kFixedFrameSize + (count)*kSystemPointerSize)
+  (parent::kFixedFrameSize + (count)*kPointerAddrSize)
 #define FRAME_SIZE_FROM_FP(parent, count) \
-  (parent::kFixedFrameSizeFromFp + (count)*kSystemPointerSize)
+  (parent::kFixedFrameSizeFromFp + (count)*kPointerAddrSize)
 #define DEFINE_FRAME_SIZES(parent, count)                                      \
   static constexpr int kFixedFrameSize = FRAME_SIZE(parent, count);            \
-  static constexpr int kFixedSlotCount = kFixedFrameSize / kSystemPointerSize; \
+  static constexpr int kFixedSlotCount = kFixedFrameSize / kPointerAddrSize; \
   static constexpr int kFixedFrameSizeFromFp =                                 \
       FRAME_SIZE_FROM_FP(parent, count);                                       \
   static constexpr int kFixedSlotCountFromFp =                                 \
-      kFixedFrameSizeFromFp / kSystemPointerSize;                              \
+      kFixedFrameSizeFromFp / kPointerAddrSize;                              \
   static constexpr int kExtraSlotCount =                                       \
-      kFixedFrameSize / kSystemPointerSize -                                   \
-      parent::kFixedFrameSize / kSystemPointerSize
+      kFixedFrameSize / kPointerAddrSize -                                   \
+      parent::kFixedFrameSize / kPointerAddrSize
 
 #define STANDARD_FRAME_EXTRA_PUSHED_VALUE_OFFSET(x) \
   FRAME_PUSHED_VALUE_OFFSET(StandardFrameConstants, x)
@@ -290,13 +296,13 @@ class ExitFrameConstants : public TypedFrameConstants {
 class BuiltinExitFrameConstants : public ExitFrameConstants {
  public:
   static constexpr int kNewTargetOffset =
-      kCallerPCOffset + 1 * kSystemPointerSize;
+      kCallerPCOffset + 1 * kPointerAddrSize;
   static constexpr int kTargetOffset =
-      kNewTargetOffset + 1 * kSystemPointerSize;
-  static constexpr int kArgcOffset = kTargetOffset + 1 * kSystemPointerSize;
-  static constexpr int kPaddingOffset = kArgcOffset + 1 * kSystemPointerSize;
+      kNewTargetOffset + 1 * kPointerAddrSize;
+  static constexpr int kArgcOffset = kTargetOffset + 1 * kPointerAddrSize;
+  static constexpr int kPaddingOffset = kArgcOffset + 1 * kPointerAddrSize;
   static constexpr int kFirstArgumentOffset =
-      kPaddingOffset + 1 * kSystemPointerSize;
+      kPaddingOffset + 1 * kPointerAddrSize;
   static constexpr int kNumExtraArgsWithoutReceiver = 4;
   static constexpr int kNumExtraArgsWithReceiver =
       kNumExtraArgsWithoutReceiver + 1;
@@ -354,7 +360,7 @@ class UnoptimizedFrameConstants : public StandardFrameConstants {
   static constexpr int kFirstParamFromFp =
       StandardFrameConstants::kCallerSPOffset;
   static constexpr int kRegisterFileFromFp =
-      -kFixedFrameSizeFromFp - kSystemPointerSize;
+      -kFixedFrameSizeFromFp - kPointerAddrSize;
   static constexpr int kExpressionsOffset = kRegisterFileFromFp;
 
   // Expression index for {JavaScriptFrame::GetExpressionAddress}.
@@ -394,12 +400,12 @@ class BaselineFrameConstants : public UnoptimizedFrameConstants {
 
 inline static int FPOffsetToFrameSlot(int frame_offset) {
   return StandardFrameConstants::kFixedSlotCountAboveFp - 1 -
-         frame_offset / kSystemPointerSize;
+         frame_offset / kPointerAddrSize;
 }
 
 inline static int FrameSlotToFPOffset(int slot) {
   return (StandardFrameConstants::kFixedSlotCountAboveFp - 1 - slot) *
-         kSystemPointerSize;
+         kPointerAddrSize;
 }
 
 }  // namespace internal
diff --git src/execution/frames.cc src/execution/frames.cc
index b0dde552408..2021682fb9e 100644
--- src/execution/frames.cc
+++ src/execution/frames.cc
@@ -567,7 +567,7 @@ CodeLookupResult GetContainingCode(Isolate* isolate, Address pc) {
 Code StackFrame::LookupCode() const {
   CodeLookupResult result = GetContainingCode(isolate(), pc());
   DCHECK(result.IsFound());
-  Code code = result.ToCode();
+  Code code = result.ToCode(PtrComprCageBase(isolate()->code_cage_base()));
   DCHECK_GE(pc(), code.InstructionStart(isolate(), pc()));
   DCHECK_LT(pc(), code.InstructionEnd(isolate(), pc()));
   return code;
@@ -595,7 +595,7 @@ CodeLookupResult StackFrame::LookupCodeT() const {
 void StackFrame::IteratePc(RootVisitor* v, Address* pc_address,
                            Address* constant_pool_address,
                            CodeLookupResult lookup_result) const {
-  Code holder = lookup_result.ToCode();
+  Code holder = lookup_result.ToCode(PtrComprCageBase(isolate()->code_cage_base()));
   Address old_pc = ReadPC(pc_address);
   DCHECK(ReadOnlyHeap::Contains(holder) ||
          holder.GetHeap()->GcSafeCodeContains(holder, old_pc));
@@ -680,7 +680,7 @@ StackFrame::Type StackFrame::ComputeType(const StackFrameIteratorBase* iterator,
         case CodeKind::BUILTIN: {
           if (StackFrame::IsTypeMarker(marker)) break;
           // TODO(v8:11880): avoid unnecessary conversion to Code or CodeT.
-          Code code_obj = lookup_result.ToCode();
+          Code code_obj = lookup_result.ToCode(PtrComprCageBase(iterator->isolate()->code_cage_base()));
           if (code_obj.is_interpreter_trampoline_builtin() ||
               // Frames for baseline entry trampolines on the stack are still
               // interpreted frames.
@@ -1090,16 +1090,16 @@ void CommonFrame::IterateCompiledFrame(RootVisitor* v) const {
         isolate()->inner_pointer_to_code_cache()->GetCacheEntry(inner_pointer);
     if (!entry->safepoint_entry.is_initialized()) {
       entry->safepoint_entry =
-          entry->code.ToCode().GetSafepointEntry(isolate(), inner_pointer);
+          entry->code.ToCode(PtrComprCageBase(isolate()->code_cage_base())).GetSafepointEntry(isolate(), inner_pointer);
       DCHECK(entry->safepoint_entry.is_initialized());
     } else {
-      DCHECK_EQ(entry->safepoint_entry, entry->code.ToCode().GetSafepointEntry(
+      DCHECK_EQ(entry->safepoint_entry, entry->code.ToCode(PtrComprCageBase(isolate()->code_cage_base())).GetSafepointEntry(
                                             isolate(), inner_pointer));
     }
 
     CHECK(entry->code.IsFound());
     code_lookup_result = entry->code;
-    Code code = entry->code.ToCode();
+    Code code = entry->code.ToCode(PtrComprCageBase(isolate()->code_cage_base()));
     safepoint_entry = entry->safepoint_entry;
     stack_slots = code.stack_slots();
 
@@ -1232,8 +1232,8 @@ void CommonFrame::IterateCompiledFrame(RootVisitor* v) const {
         Address value = *spill_slot.location();
         if (!HAS_SMI_TAG(value) && value <= 0xffffffff) {
           // We don't need to update smi values or full pointers.
-          *spill_slot.location() =
-              DecompressTaggedPointer(cage_base, static_cast<Tagged_t>(value));
+          *spill_slot.location() = V8HeapCompressionScheme::DecompressTaggedAny(
+	      cage_base, static_cast<Tagged_t>(value));
           if (DEBUG_BOOL) {
             // Ensure that the spill slot contains correct heap object.
             HeapObject raw = HeapObject::cast(Object(*spill_slot.location()));
@@ -1259,8 +1259,8 @@ void CommonFrame::IterateCompiledFrame(RootVisitor* v) const {
             static_cast<Tagged_t>(*spill_slot.location());
         if (!HAS_SMI_TAG(compressed_value)) {
           // We don't need to update smi values.
-          *spill_slot.location() =
-              DecompressTaggedPointer(cage_base, compressed_value);
+          *spill_slot.location() = V8HeapCompressionScheme::DecompressTaggedAny(
+	      cage_base, compressed_value);
         }
       }
 #endif
@@ -1900,7 +1900,7 @@ DeoptimizationData OptimizedFrame::GetDeoptimizationData(
     CodeLookupResult lookup_result =
         isolate()->heap()->GcSafeFindCodeForInnerPointer(pc());
     CHECK(lookup_result.IsFound());
-    code = lookup_result.ToCode();
+    code = lookup_result.ToCode(PtrComprCageBase(isolate()->code_cage_base()));
   }
   DCHECK(!code.is_null());
   DCHECK(CodeKindCanDeoptimize(code.kind()));
@@ -2459,7 +2459,7 @@ void InternalFrame::Iterate(RootVisitor* v) const {
   // This is used for the WasmCompileLazy builtin, where we actually pass
   // untagged arguments and also store untagged values on the stack.
   // TODO(v8:11880): avoid unnecessary conversion to Code or CodeT.
-  if (code.ToCode().has_tagged_outgoing_params()) IterateExpressions(v);
+  if (code.ToCode(PtrComprCageBase(isolate()->code_cage_base())).has_tagged_outgoing_params()) IterateExpressions(v);
 }
 
 // -------------------------------------------------------------------------
diff --git src/execution/frames.h src/execution/frames.h
index 44397765099..41411d604c3 100644
--- src/execution/frames.h
+++ src/execution/frames.h
@@ -199,7 +199,11 @@ class StackFrame {
   // and should be converted back to a stack frame type using MarkerToType.
   // Otherwise, the value is a tagged function pointer.
   static bool IsTypeMarker(intptr_t function_or_marker) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return (function_or_marker & (size_t) kSmiTagMask) == kSmiTag;
+#else
     return (function_or_marker & kSmiTagMask) == kSmiTag;
+#endif
   }
 
   // Copy constructor; it breaks the connection to host iterator
diff --git src/execution/isolate-data.h src/execution/isolate-data.h
index 35309558b9a..d87c2121391 100644
--- src/execution/isolate-data.h
+++ src/execution/isolate-data.h
@@ -25,7 +25,8 @@ class Isolate;
 #define ISOLATE_DATA_FIELDS(V)                                                \
   /* Misc. fields. */                                                         \
   V(kCageBaseOffset, kSystemPointerSize, cage_base)                           \
-  V(kStackGuardOffset, StackGuard::kSizeInBytes, stack_guard)                 \
+  V(kStackGuardOffset, RoundUp<kSystemPointerSize>(StackGuard::kSizeInBytes), \
+    stack_guard)                                                              \
   /* Tier 0 tables (small but fast access). */                                \
   V(kBuiltinTier0EntryTableOffset,                                            \
     Builtins::kBuiltinTier0Count* kSystemPointerSize,                         \
@@ -38,13 +39,16 @@ class Isolate;
   V(kFastCCallCallerFPOffset, kSystemPointerSize, fast_c_call_caller_fp)      \
   V(kFastCCallCallerPCOffset, kSystemPointerSize, fast_c_call_caller_pc)      \
   V(kFastApiCallTargetOffset, kSystemPointerSize, fast_api_call_target)       \
-  V(kLongTaskStatsCounterOffset, kSizetSize, long_task_stats_counter)         \
+  V(kLongTaskStatsCounterOffset, RoundUp<kSystemPointerSize>(kSizetSize),     \
+    long_task_stats_counter)         \
   /* Full tables (arbitrary size, potentially slower access). */              \
   V(kRootsTableOffset, RootsTable::kEntriesCount* kSystemPointerSize,         \
     roots_table)                                                              \
   V(kExternalReferenceTableOffset, ExternalReferenceTable::kSizeInBytes,      \
     external_reference_table)                                                 \
-  V(kThreadLocalTopOffset, ThreadLocalTop::kSizeInBytes, thread_local_top)    \
+  V(kThreadLocalTopOffset,                                                    \
+    RoundUp<kSystemPointerSize>(ThreadLocalTop::kSizeInBytes),                \
+    thread_local_top)                                                         \
   V(kBuiltinEntryTableOffset, Builtins::kBuiltinCount* kSystemPointerSize,    \
     builtin_entry_table)                                                      \
   V(kBuiltinTableOffset, Builtins::kBuiltinCount* kSystemPointerSize,         \
@@ -79,7 +83,11 @@ class IsolateData final {
 
   // The value of the kRootRegister.
   Address isolate_root() const {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return reinterpret_cast<Address>(this) + (size_t) kIsolateRootBias;
+#else
     return reinterpret_cast<Address>(this) + kIsolateRootBias;
+#endif
   }
 
   // Root-register-relative offsets.
@@ -150,6 +158,18 @@ class IsolateData final {
   // cheaper it is to access them. See also: https://crbug.com/993264.
   // The recommended guideline is to put frequently-accessed fields close to
   // the beginning of IsolateData.
+#if defined(__CHERI_PURE_CAPABILITY__)
+#define FIELDS(V)                                                      \
+  ISOLATE_DATA_FIELDS(V)                                               \
+  /* This padding aligns IsolateData size by 16 bytes. */               \
+  V(kPaddingOffset,                                                    \
+    kSystemPointerSize + RoundUp<kSystemPointerSize>(static_cast<int>(kPaddingOffset)) - kPaddingOffset) \
+  /* Total size. */                                                    \
+  V(kSize, 0)
+
+  DEFINE_FIELD_OFFSET_CONSTANTS(0, FIELDS)
+#undef FIELDS
+#else
 #define FIELDS(V)                                                      \
   ISOLATE_DATA_FIELDS(V)                                               \
   /* This padding aligns IsolateData size by 8 bytes. */               \
@@ -160,6 +180,8 @@ class IsolateData final {
 
   DEFINE_FIELD_OFFSET_CONSTANTS(0, FIELDS)
 #undef FIELDS
+#endif
+
 
   const Address cage_base_;
 
diff --git src/execution/isolate.cc src/execution/isolate.cc
index 8b4c989a259..198196abed3 100644
--- src/execution/isolate.cc
+++ src/execution/isolate.cc
@@ -34,6 +34,7 @@
 #include "src/codegen/compilation-cache.h"
 #include "src/codegen/flush-instruction-cache.h"
 #include "src/common/assert-scope.h"
+#include "src/common/cheri.h"
 #include "src/common/ptr-compr-inl.h"
 #include "src/compiler-dispatcher/lazy-compile-dispatcher.h"
 #include "src/compiler-dispatcher/optimizing-compile-dispatcher.h"
@@ -391,7 +392,11 @@ base::AddressRegion Isolate::GetShortBuiltinsCallRegion() {
 
   DCHECK_LT(CurrentEmbeddedBlobCodeSize(), radius);
   Address embedded_blob_code_start =
+#if defined(__CHERI_PURE_CAPABILITY__)
+      reinterpret_cast<ptraddr_t>(CurrentEmbeddedBlobCode());
+#else
       reinterpret_cast<Address>(CurrentEmbeddedBlobCode());
+#endif
   if (embedded_blob_code_start == kNullAddress) {
     // Return empty region if there's no embedded blob.
     return base::AddressRegion(kNullAddress, 0);
@@ -430,9 +435,11 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
     // they change when creating the off-heap trampolines. Other data fields
     // must remain the same.
 #ifdef V8_EXTERNAL_CODE_SPACE
+#if !defined(__CHERI_PURE_CAPABILITY__)
     static_assert(Code::kMainCageBaseUpper32BitsOffset == Code::kDataStart);
     static_assert(Code::kInstructionSizeOffset ==
                   Code::kMainCageBaseUpper32BitsOffsetEnd + 1);
+#endif
 #else
     static_assert(Code::kInstructionSizeOffset == Code::kDataStart);
 #endif  // V8_EXTERNAL_CODE_SPACE
@@ -3815,6 +3822,7 @@ bool Isolate::InitWithSnapshot(SnapshotData* startup_snapshot_data,
 }
 
 namespace {
+using cheri::operator<<;
 static std::string ToHexString(uintptr_t address) {
   std::stringstream stream_address;
   stream_address << "0x" << std::hex << address;
@@ -3899,6 +3907,11 @@ bool Isolate::Init(SnapshotData* startup_snapshot_data,
                    SnapshotData* read_only_snapshot_data,
                    SnapshotData* shared_heap_snapshot_data, bool can_rehash) {
   TRACE_ISOLATE(init);
+  
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+  CHECK_EQ(V8HeapCompressionScheme::base(), cage_base());
+#endif  // V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+
   const bool create_heap_objects = (read_only_snapshot_data == nullptr);
   // We either have all or none.
   DCHECK_EQ(create_heap_objects, startup_snapshot_data == nullptr);
@@ -4036,10 +4049,37 @@ bool Isolate::Init(SnapshotData* startup_snapshot_data,
     }
   }
 #ifdef V8_EXTERNAL_CODE_SPACE
-  if (heap_.code_range()) {
-    code_cage_base_ = GetPtrComprCageBaseAddress(heap_.code_range()->base());
-  } else {
-    code_cage_base_ = cage_base();
+  {
+    VirtualMemoryCage* code_cage;	
+    if (heap_.code_range()) {
+      code_cage = heap_.code_range();
+    } else {
+      CHECK(jitless_);
+      // In jitless mode the code space pages will be allocated in the main
+      // pointer compression cage.
+      code_cage = GetPtrComprCage();
+    }
+
+  code_cage_base_ = ExternalCodeCompressionScheme::PrepareCageBaseAddress(
+      code_cage->base());
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+    CHECK_EQ(ExternalCodeCompressionScheme::base(), code_cage_base_);
+#endif  // V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+
+    // Ensure that ExternalCodeCompressionScheme is applicable to all objects
+    // stored in the code cage.
+    using ComprScheme = ExternalCodeCompressionScheme;
+    Address base = code_cage->base();
+    Address last = base + code_cage->size() - 1;
+    Address upper_bound = base + kPtrComprCageReservationSize - 1;
+    PtrComprCageBase code_cage_base{code_cage_base_};
+    CHECK_EQ(base, ComprScheme::DecompressTaggedPointer(
+                       code_cage_base, ComprScheme::CompressTagged(base)));
+    CHECK_EQ(last, ComprScheme::DecompressTaggedPointer(
+                       code_cage_base, ComprScheme::CompressTagged(last)));
+    CHECK_EQ(upper_bound,
+             ComprScheme::DecompressTaggedPointer(
+                 code_cage_base, ComprScheme::CompressTagged(upper_bound)));
   }
 #endif  // V8_EXTERNAL_CODE_SPACE
 
diff --git src/execution/isolate.h src/execution/isolate.h
index 71ab099e616..57ec01e4cca 100644
--- src/execution/isolate.h
+++ src/execution/isolate.h
@@ -1205,7 +1205,11 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   // The kRootRegister is set to this value.
   Address isolate_root() const { return isolate_data()->isolate_root(); }
   static size_t isolate_root_bias() {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return OFFSET_OF(Isolate, isolate_data_) + (size_t) IsolateData::kIsolateRootBias;
+#else
     return OFFSET_OF(Isolate, isolate_data_) + IsolateData::kIsolateRootBias;
+#endif
   }
   static Isolate* FromRootAddress(Address isolate_root) {
     return reinterpret_cast<Isolate*>(isolate_root - isolate_root_bias());
diff --git src/execution/stack-guard.h src/execution/stack-guard.h
index 5076f47a001..9d5bf7a79db 100644
--- src/execution/stack-guard.h
+++ src/execution/stack-guard.h
@@ -159,8 +159,17 @@ class V8_EXPORT_PRIVATE V8_NODISCARD StackGuard final {
 
     // jslimit_ and climit_ can be read without any lock.
     // Writing requires the ExecutionAccess lock.
+#if defined(__CHERI_PURE_CAPABILITY__)
+    // Whilst it is not strictly neccessary for these values to be 
+    // AtomicIntPtr values, it preserves the invartaint:
+    // static_assert(StackGuard::kSizeInBytes == sizeof(StackGuard));
+    // without changing kSizeInBytes to account for the architecture.
+    base::AtomicIntPtr jslimit_ = kIllegalLimit;
+    base::AtomicIntPtr climit_ = kIllegalLimit;
+#else
     base::AtomicWord jslimit_ = kIllegalLimit;
     base::AtomicWord climit_ = kIllegalLimit;
+#endif
 
     uintptr_t jslimit() {
       return base::bit_cast<uintptr_t>(base::Relaxed_Load(&jslimit_));
diff --git src/heap/base/active-system-pages.h src/heap/base/active-system-pages.h
index 0c30cb928f8..03afef0377f 100644
--- src/heap/base/active-system-pages.h
+++ src/heap/base/active-system-pages.h
@@ -26,7 +26,7 @@ class ActiveSystemPages final {
 
   // Adds the pages for this memory range. Returns the number of freshly added
   // pages.
-  V8_EXPORT_PRIVATE size_t Add(size_t start, size_t end, size_t page_size_bits);
+  V8_EXPORT_PRIVATE size_t Add(uintptr_t start, uintptr_t end, size_t page_size_bits);
 
   // Replaces the current bitset with the given argument. The new bitset needs
   // to be a proper subset of the current pages, which means this operation
diff --git src/heap/base/asm/arm64/push_registers_asm.cc src/heap/base/asm/arm64/push_registers_asm.cc
index 1efcc3430b3..392d999604f 100644
--- src/heap/base/asm/arm64/push_registers_asm.cc
+++ src/heap/base/asm/arm64/push_registers_asm.cc
@@ -32,16 +32,28 @@ asm(
     "PushAllRegistersAndIterateStack:                   \n"
 #endif  // !defined(__APPLE__)
     // x19-x29 are callee-saved.
+#ifdef __CHERI_PURE_CAPABILITY__
+    "  stp c19, c20, [csp, #-32]!                       \n"
+    "  stp c21, c22, [csp, #-32]!                       \n"
+    "  stp c23, c24, [csp, #-32]!                       \n"
+    "  stp c25, c26, [csp, #-32]!                       \n"
+    "  stp c27, c28, [csp, #-32]!                       \n"
+#else
     "  stp x19, x20, [sp, #-16]!                        \n"
     "  stp x21, x22, [sp, #-16]!                        \n"
     "  stp x23, x24, [sp, #-16]!                        \n"
     "  stp x25, x26, [sp, #-16]!                        \n"
     "  stp x27, x28, [sp, #-16]!                        \n"
+#endif
 #ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY
     // Sign return address.
     "  paciasp                                          \n"
 #endif
+#ifdef __CHERI_PURE_CAPABILITY__
+    "  stp fp, lr,   [csp, #-32]!                       \n"
+#else
     "  stp fp, lr,   [sp, #-16]!                        \n"
+#endif
     // Maintain frame pointer.
     "  mov fp, sp                                       \n"
     // Pass 1st parameter (x0) unchanged (Stack*).
@@ -49,14 +61,26 @@ asm(
     // Save 3rd parameter (x2; IterateStackCallback)
     "  mov x7, x2                                       \n"
     // Pass 3rd parameter as sp (stack pointer).
+#ifdef __CHERI_PURE_CAPABILITY__
+    "  mov c2, csp                                      \n"
+#else
     "  mov x2, sp                                       \n"
+#endif
     "  blr x7                                           \n"
     // Load return address and frame pointer.
+#ifdef __CHERI_PURE_CAPABILITY__
+    "  ldp fp, lr, [csp], #32                           \n"
+#else
     "  ldp fp, lr, [sp], #16                            \n"
+#endif
 #ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY
     // Authenticate return address.
     "  autiasp                                          \n"
 #endif
     // Drop all callee-saved registers.
+#ifdef __CHERI_PURE_CAPABILITY__
+    "  add csp, csp, #160                               \n"
+#else
     "  add sp, sp, #80                                  \n"
+#endif
     "  ret                                              \n");
diff --git src/heap/basic-memory-chunk.cc src/heap/basic-memory-chunk.cc
index e076d6bc81a..9e59b03d52c 100644
--- src/heap/basic-memory-chunk.cc
+++ src/heap/basic-memory-chunk.cc
@@ -76,7 +76,11 @@ bool BasicMemoryChunk::InLargeObjectSpace() const {
 #ifdef THREAD_SANITIZER
 void BasicMemoryChunk::SynchronizedHeapLoad() const {
   CHECK(reinterpret_cast<Heap*>(
+#if defined(__CHERI_PURE_CAPABILITY__)
+            base::Acquire_Load(reinterpret_cast<base::AtomicIntPtr*>(
+#else
             base::Acquire_Load(reinterpret_cast<base::AtomicWord*>(
+#endif
                 &(const_cast<BasicMemoryChunk*>(this)->heap_)))) != nullptr ||
         InReadOnlySpaceRaw());
 }
diff --git src/heap/basic-memory-chunk.h src/heap/basic-memory-chunk.h
index abd51302773..8e5a462cb91 100644
--- src/heap/basic-memory-chunk.h
+++ src/heap/basic-memory-chunk.h
@@ -31,7 +31,11 @@ class BasicMemoryChunk {
     }
   };
 
+#ifdef __CHERI_PURE_CAPABILITY__
+  enum Flag : size_t {
+#else
   enum Flag : uintptr_t {
+#endif
     NO_FLAGS = 0u,
     IS_EXECUTABLE = 1u << 0,
     POINTERS_TO_HERE_ARE_INTERESTING = 1u << 1,
@@ -102,7 +106,11 @@ class BasicMemoryChunk {
     IN_SHARED_HEAP = 1u << 22,
   };
 
+#ifdef __CHERI_PURE_CAPABILITY__
+  using MainThreadFlags = base::Flags<Flag, size_t>;
+#else
   using MainThreadFlags = base::Flags<Flag, uintptr_t>;
+#endif
 
   static constexpr MainThreadFlags kAllFlagsMask = ~MainThreadFlags(NO_FLAGS);
 
@@ -127,7 +135,11 @@ class BasicMemoryChunk {
   static const intptr_t kAlignment =
       (static_cast<uintptr_t>(1) << kPageSizeBits);
 
+#ifdef __CHERI_PURE_CAPABILITY__
+  __attribute__((cheri_no_provenance)) static const intptr_t kAlignmentMask = kAlignment - 1;
+#else
   static const intptr_t kAlignmentMask = kAlignment - 1;
+#endif
 
   BasicMemoryChunk(Heap* heap, BaseSpace* space, size_t chunk_size,
                    Address area_start, Address area_end,
@@ -176,6 +188,7 @@ class BasicMemoryChunk {
 
   void SetFlag(Flag flag) { main_thread_flags_ |= flag; }
   bool IsFlagSet(Flag flag) const { return main_thread_flags_ & flag; }
+
   void ClearFlag(Flag flag) {
     main_thread_flags_ = main_thread_flags_.without(flag);
   }
@@ -267,8 +280,13 @@ class BasicMemoryChunk {
   static const intptr_t kHeapOffset = MemoryChunkLayout::kHeapOffset;
   static const intptr_t kAreaStartOffset = MemoryChunkLayout::kAreaStartOffset;
   static const intptr_t kAreaEndOffset = MemoryChunkLayout::kAreaEndOffset;
+#ifdef __CHERI_PURE_CAPABILITY__
+  __attribute__((cheri_no_provenance)) static const intptr_t kMarkingBitmapOffset =
+      MemoryChunkLayout::kMarkingBitmapOffset;
+#else
   static const intptr_t kMarkingBitmapOffset =
       MemoryChunkLayout::kMarkingBitmapOffset;
+#endif
   static const size_t kHeaderSize =
       MemoryChunkLayout::kBasicMemoryChunkHeaderSize;
 
@@ -290,7 +308,11 @@ class BasicMemoryChunk {
         Bitmap::FromAddress(address() + kMarkingBitmapOffset));
   }
 
+#ifdef __CHERI_PURE_CAPABILITY__
+  Address HighWaterMark() const { return address() + (size_t) high_water_mark_; }
+#else
   Address HighWaterMark() const { return address() + high_water_mark_; }
+#endif
 
   static inline void UpdateHighWaterMark(Address mark) {
     if (mark == kNullAddress) return;
diff --git src/heap/code-range.cc src/heap/code-range.cc
index badef8e17f5..73c80a00ec0 100644
--- src/heap/code-range.cc
+++ src/heap/code-range.cc
@@ -60,8 +60,13 @@ Address CodeRangeAddressHint::GetAddressHint(size_t code_range_size,
       // with a higher chances to point to the free address space range.
       return RoundUp(preferred_region.begin(), alignment);
     }
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return static_cast<ptraddr_t>(RoundUp(FUNCTION_ADDR(&FunctionInStaticBinaryForAddressHint),
+                                  alignment));
+#else
     return RoundUp(FUNCTION_ADDR(&FunctionInStaticBinaryForAddressHint),
                    alignment);
+#endif
   }
 
   // Try to reuse near code range first.
@@ -124,10 +129,7 @@ bool CodeRange::InitReservation(v8::PageAllocator* page_allocator,
   // is enabled so that InitReservation would not break the alignment in
   // GetAddressHint().
   const size_t allocate_page_size = page_allocator->AllocatePageSize();
-  params.base_alignment =
-      V8_EXTERNAL_CODE_SPACE_BOOL
-          ? base::bits::RoundUpToPowerOfTwo(requested)
-          : VirtualMemoryCage::ReservationParams::kAnyBaseAlignment;
+  params.base_alignment = VirtualMemoryCage::ReservationParams::kAnyBaseAlignment;
   params.base_bias_size = RoundUp(reserved_area, allocate_page_size);
   params.page_size = MemoryChunk::kPageSize;
   params.requested_start_hint =
@@ -137,16 +139,6 @@ bool CodeRange::InitReservation(v8::PageAllocator* page_allocator,
 
   if (!VirtualMemoryCage::InitReservation(params)) return false;
 
-  if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-    // Ensure that the code range does not cross the 4Gb boundary and thus
-    // default compression scheme of truncating the Code pointers to 32-bit
-    // still work.
-    Address base = page_allocator_->begin();
-    Address last = base + page_allocator_->size() - 1;
-    CHECK_EQ(GetPtrComprCageBaseAddress(base),
-             GetPtrComprCageBaseAddress(last));
-  }
-
   // On some platforms, specifically Win64, we need to reserve some pages at
   // the beginning of an executable space. See
   //   https://cs.chromium.org/chromium/src/components/crash/content/
@@ -310,6 +302,16 @@ std::shared_ptr<CodeRange> CodeRange::EnsureProcessWideCodeRange(
           nullptr, "Failed to reserve virtual memory for CodeRange");
     }
     *process_wide_code_range_.Pointer() = code_range;
+#ifdef V8_EXTERNAL_CODE_SPACE
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+    // This might be not the first time we create a code range because previous
+    // code range instance could have been deleted if it wasn't kept alive by an
+    // active Isolate. Let the base be initialized from scratch once again.
+    ExternalCodeCompressionScheme::InitBase(
+        ExternalCodeCompressionScheme::PrepareCageBaseAddress(
+            code_range->base()));
+#endif  // V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+#endif  // V8_EXTERNAL_CODE_SPACE
   }
   return code_range;
 }
diff --git src/heap/cppgc/globals.h src/heap/cppgc/globals.h
index 44bec42223f..5da1046af41 100644
--- src/heap/cppgc/globals.h
+++ src/heap/cppgc/globals.h
@@ -32,7 +32,11 @@ enum class AccessMode : uint8_t { kNonAtomic, kAtomic };
 // practice: long double) cannot be used unrestricted in garbage-collected
 // objects.
 #if defined(V8_TARGET_ARCH_64_BIT)
+#if defined(V8_TARGET_ARCH_CHERI_PURE_CAPABILITY)
+constexpr size_t kAllocationGranularity = 16;
+#else
 constexpr size_t kAllocationGranularity = 8;
+#endif
 #else   // !V8_TARGET_ARCH_64_BIT
 constexpr size_t kAllocationGranularity = 4;
 #endif  // !V8_TARGET_ARCH_64_BIT
diff --git src/heap/cppgc/heap-object-header.h src/heap/cppgc/heap-object-header.h
index a6efb8defd0..fdc134914f2 100644
--- src/heap/cppgc/heap-object-header.h
+++ src/heap/cppgc/heap-object-header.h
@@ -148,6 +148,15 @@ class HeapObjectHeader {
   inline void StoreEncoded(uint16_t bits, uint16_t mask);
 
 #if defined(V8_TARGET_ARCH_64_BIT)
+#if defined(V8_TARGET_ARCH_CHERI_PURE_CAPABILITY)
+  // If cage is enabled, to save on space required by sweeper metadata, we store
+  // the list of to-be-finalized objects inlined in HeapObjectHeader.
+#if defined(CPPGC_CAGED_HEAP)
+  uint64_t next_unfinalized_ = 0;
+#else   // !defined(CPPGC_CAGED_HEAP)
+  uint64_t padding_ = 0;
+#endif  // !defined(CPPGC_CAGED_HEAP)
+#else
   // If cage is enabled, to save on space required by sweeper metadata, we store
   // the list of to-be-finalized objects inlined in HeapObjectHeader.
 #if defined(CPPGC_CAGED_HEAP)
@@ -155,6 +164,7 @@ class HeapObjectHeader {
 #else   // !defined(CPPGC_CAGED_HEAP)
   uint32_t padding_ = 0;
 #endif  // !defined(CPPGC_CAGED_HEAP)
+#endif
 #endif  // defined(V8_TARGET_ARCH_64_BIT)
   uint16_t encoded_high_;
   uint16_t encoded_low_;
diff --git src/heap/factory-base.cc src/heap/factory-base.cc
index b1bda7cf331..9c431ea4c3e 100644
--- src/heap/factory-base.cc
+++ src/heap/factory-base.cc
@@ -84,8 +84,6 @@ Handle<CodeDataContainer> FactoryBase<Impl>::NewCodeDataContainer(
                                     SKIP_WRITE_BARRIER);
   data_container.set_kind_specific_flags(flags, kRelaxedStore);
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-    data_container.set_code_cage_base(impl()->isolate()->code_cage_base(),
-                                      kRelaxedStore);
     Isolate* isolate_for_sandbox = impl()->isolate_for_sandbox();
     data_container.AllocateExternalPointerEntries(isolate_for_sandbox);
     data_container.set_raw_code(Smi::zero(), SKIP_WRITE_BARRIER);
diff --git src/heap/heap-inl.h src/heap/heap-inl.h
index f99c6e3ad2b..e36b06a25b6 100644
--- src/heap/heap-inl.h
+++ src/heap/heap-inl.h
@@ -481,7 +481,8 @@ bool Heap::IsPendingAllocationInternal(HeapObject object) {
 bool Heap::IsPendingAllocation(HeapObject object) {
   bool result = IsPendingAllocationInternal(object);
   if (FLAG_trace_pending_allocations && result) {
-    StdoutStream{} << "Pending allocation: " << std::hex << "0x" << object.ptr()
+    // TODO: Temporary fix as ostream is missing an overloaded operator<< for uintptr_t.
+    StdoutStream{} << "Pending allocation: " << std::hex << "0x" << (size_t) object.ptr()
                    << "\n";
   }
   return result;
diff --git src/heap/heap-write-barrier-inl.h src/heap/heap-write-barrier-inl.h
index 1efbcde84da..852806b8025 100644
--- src/heap/heap-write-barrier-inl.h
+++ src/heap/heap-write-barrier-inl.h
@@ -44,6 +44,22 @@ V8_EXPORT_PRIVATE void Heap_GenerationalEphemeronKeyBarrierSlow(
 namespace heap_internals {
 
 struct MemoryChunk {
+#ifdef __CHERI_PURE_CAPABILITY__
+  // Note: That as clients of this interface should be exposed to the details of the heap internals.
+  // The kFlagsOffset and kHeapOffset must be calcualted based on the knowledge of the size of
+  // the fields and the padding introduced by the alignment requirements of max_align_t (which
+  // is a stronger alignment requirement than for 64bit and 32bit architectures).
+  // The size field is of type size_t, as the next field (flags_) is of type size_t, the
+  // heap_ field is properly aligned and no padding is inserted.
+  __attribute__((cheri_no_provenance)) static constexpr uintptr_t kFlagsOffset = kSizetSize;
+  __attribute__((cheri_no_provenance)) static constexpr uintptr_t kHeapOffset = kSizetSize + kSizetSize;
+  __attribute__((cheri_no_provenance)) static constexpr uintptr_t kIsExecutableBit = uintptr_t{1} << 0;
+  __attribute__((cheri_no_provenance)) static constexpr uintptr_t kMarkingBit = uintptr_t{1} << 17;
+  __attribute__((cheri_no_provenance)) static constexpr uintptr_t kFromPageBit = uintptr_t{1} << 3;
+  __attribute__((cheri_no_provenance)) static constexpr uintptr_t kToPageBit = uintptr_t{1} << 4;
+  __attribute__((cheri_no_provenance)) static constexpr uintptr_t kReadOnlySpaceBit = uintptr_t{1} << 20;
+  __attribute__((cheri_no_provenance)) static constexpr uintptr_t kInSharedHeapBit = uintptr_t{1} << 22;
+#else
   static constexpr uintptr_t kFlagsOffset = kSizetSize;
   static constexpr uintptr_t kHeapOffset = kSizetSize + kUIntptrSize;
   static constexpr uintptr_t kIsExecutableBit = uintptr_t{1} << 0;
@@ -52,6 +68,7 @@ struct MemoryChunk {
   static constexpr uintptr_t kToPageBit = uintptr_t{1} << 4;
   static constexpr uintptr_t kReadOnlySpaceBit = uintptr_t{1} << 20;
   static constexpr uintptr_t kInSharedHeapBit = uintptr_t{1} << 22;
+#endif
 
   V8_INLINE static heap_internals::MemoryChunk* FromHeapObject(
       HeapObject object) {
@@ -65,20 +82,33 @@ struct MemoryChunk {
 
   V8_INLINE bool InYoungGeneration() const {
     if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return false;
+#ifdef __CHERI_PURE_CAPABILITY__
+    __attribute__((cheri_no_provenance)) constexpr uintptr_t kYoungGenerationMask = kFromPageBit | kToPageBit;
+#else
     constexpr uintptr_t kYoungGenerationMask = kFromPageBit | kToPageBit;
+#endif
     return GetFlags() & kYoungGenerationMask;
   }
 
   // Checks whether chunk is either in young gen or shared heap.
   V8_INLINE bool IsYoungOrSharedChunk() const {
     if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return false;
+#ifdef __CHERI_PURE_CAPABILITY__
+    __attribute__((cheri_no_provenance)) constexpr uintptr_t kYoungOrSharedChunkMask =
+        kFromPageBit | kToPageBit | kInSharedHeapBit;
+#else
     constexpr uintptr_t kYoungOrSharedChunkMask =
         kFromPageBit | kToPageBit | kInSharedHeapBit;
+#endif
     return GetFlags() & kYoungOrSharedChunkMask;
   }
 
   V8_INLINE uintptr_t GetFlags() const {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return *reinterpret_cast<const size_t*>(reinterpret_cast<Address>(this) +
+#else
     return *reinterpret_cast<const uintptr_t*>(reinterpret_cast<Address>(this) +
+#endif
                                                kFlagsOffset);
   }
 
@@ -124,7 +154,8 @@ inline void CombinedWriteBarrierInternal(HeapObject host, HeapObjectSlot slot,
 
 }  // namespace heap_internals
 
-inline void WriteBarrierForCode(Code host, RelocInfo* rinfo, Object value) {
+inline void WriteBarrierForCode(Code host,
+		RelocInfo* rinfo, Object value) {
   DCHECK(!HasWeakHeapObjectTag(value));
   if (!value.IsHeapObject()) return;
   WriteBarrierForCode(host, rinfo, HeapObject::cast(value));
diff --git src/heap/large-spaces.h src/heap/large-spaces.h
index 81703288508..08ec475648a 100644
--- src/heap/large-spaces.h
+++ src/heap/large-spaces.h
@@ -56,7 +56,11 @@ class LargePage : public MemoryChunk {
   friend class MemoryAllocator;
 };
 
+#ifndef __CHERI_PURE_CAPABILITY__
+// The calculation of kHeaderSize is broken for CHERI, as it doesn't take into 
+// account the padding under stronger alignments. I'm doubtful its worth fixing.
 static_assert(sizeof(LargePage) <= MemoryChunk::kHeaderSize);
+#endif
 
 // -----------------------------------------------------------------------------
 // Large objects ( > kMaxRegularHeapObjectSize ) are allocated and managed by
diff --git src/heap/mark-compact.cc src/heap/mark-compact.cc
index b849a2c725a..a60e9a976cf 100644
--- src/heap/mark-compact.cc
+++ src/heap/mark-compact.cc
@@ -2898,7 +2898,7 @@ void MarkCompactCollector::ProcessOldCodeCandidates() {
       CodeT baseline_codet =
           CodeT::cast(flushing_candidate.function_data(kAcquireLoad));
       // Safe to do a relaxed load here since the CodeT was acquire-loaded.
-      Code baseline_code = FromCodeT(baseline_codet, kRelaxedLoad);
+      Code baseline_code = FromCodeT(baseline_codet, baseline_codet.GetIsolate(), kRelaxedLoad);
       if (non_atomic_marking_state()->IsBlackOrGrey(baseline_code)) {
         // Currently baseline code holds bytecode array strongly and it is
         // always ensured that bytecode is live if baseline code is live. Hence
@@ -3390,6 +3390,14 @@ MaybeObject MakeSlotValue<FullMaybeObjectSlot, HeapObjectReferenceType::STRONG>(
   return HeapObjectReference::Strong(heap_object);
 }
 
+#ifdef V8_EXTERNAL_CODE_SPACE
+template <>
+Object MakeSlotValue<CodeObjectSlot, HeapObjectReferenceType::STRONG>(
+    HeapObject heap_object) {
+  return heap_object;
+}
+#endif  // V8_EXTERNAL_CODE_SPACE
+
 // The following specialization
 //   MakeSlotValue<FullMaybeObjectSlot, HeapObjectReferenceType::WEAK>()
 // is not used.
@@ -3405,9 +3413,10 @@ static inline SlotCallbackResult UpdateSlot(PtrComprCageBase cage_base,
                     std::is_same<TSlot, ObjectSlot>::value ||
                     std::is_same<TSlot, FullMaybeObjectSlot>::value ||
                     std::is_same<TSlot, MaybeObjectSlot>::value ||
-                    std::is_same<TSlot, OffHeapObjectSlot>::value,
-                "Only [Full|OffHeap]ObjectSlot and [Full]MaybeObjectSlot are "
-                "expected here");
+                    std::is_same<TSlot, OffHeapObjectSlot>::value ||
+		    std::is_same<TSlot, CodeObjectSlot>::value,
+                "Only [Full|OffHeap]ObjectSlot and [Full]MaybeObjectSlot "
+                "or CodeObjectSlot are expected here");
   MapWord map_word = heap_obj.map_word(cage_base, kRelaxedLoad);
   if (map_word.IsForwardingAddress()) {
     DCHECK_IMPLIES(!Heap::InFromPage(heap_obj),
diff --git src/heap/marking-visitor-inl.h src/heap/marking-visitor-inl.h
index 2d6c17252b6..1662211f385 100644
--- src/heap/marking-visitor-inl.h
+++ src/heap/marking-visitor-inl.h
@@ -197,7 +197,7 @@ int MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitSharedFunctionInfo(
     DCHECK(IsBaselineCodeFlushingEnabled(code_flush_mode_));
     CodeT baseline_codet = CodeT::cast(shared_info.function_data(kAcquireLoad));
     // Safe to do a relaxed load here since the CodeT was acquire-loaded.
-    Code baseline_code = FromCodeT(baseline_codet, kRelaxedLoad);
+    Code baseline_code = FromCodeT(baseline_codet, baseline_codet.GetIsolate(), kRelaxedLoad);
     // Visit the bytecode hanging off baseline code.
     VisitPointer(baseline_code,
                  baseline_code.RawField(
diff --git src/heap/memory-allocator.h src/heap/memory-allocator.h
index 4e96ab452b5..be753ec3e02 100644
--- src/heap/memory-allocator.h
+++ src/heap/memory-allocator.h
@@ -269,8 +269,8 @@ class MemoryAllocator {
   struct MemoryChunkAllocationResult {
     void* start;
     size_t size;
-    size_t area_start;
-    size_t area_end;
+    uintptr_t area_start;
+    uintptr_t area_end;
     VirtualMemory reservation;
   };
 
diff --git src/heap/memory-chunk-layout.h src/heap/memory-chunk-layout.h
index 69c1151f2e9..51d5ea0c554 100644
--- src/heap/memory-chunk-layout.h
+++ src/heap/memory-chunk-layout.h
@@ -5,6 +5,8 @@
 #ifndef V8_HEAP_MEMORY_CHUNK_LAYOUT_H_
 #define V8_HEAP_MEMORY_CHUNK_LAYOUT_H_
 
+#include <cstddef>
+
 #include "src/heap/base/active-system-pages.h"
 #include "src/heap/heap.h"
 #include "src/heap/list.h"
@@ -37,14 +39,23 @@ using ActiveSystemPages = ::heap::base::ActiveSystemPages;
 
 class V8_EXPORT_PRIVATE MemoryChunkLayout {
  public:
-  static const int kNumSets = NUMBER_OF_REMEMBERED_SET_TYPES;
-  static const int kNumTypes = ExternalBackingStoreType::kNumTypes;
+  static constexpr int kNumSets = NUMBER_OF_REMEMBERED_SET_TYPES;
+  static constexpr int kNumTypes = ExternalBackingStoreType::kNumTypes;
+#if V8_CC_MSVC && V8_TARGET_ARCH_IA32
+  static constexpr int kMemoryChunkAlignment = 8;
+#else
+  static constexpr int kMemoryChunkAlignment = alignof(std::max_align_t);
+#endif  // V8_CC_MSVC && V8_TARGET_ARCH_IA32
 #define FIELD(Type, Name) \
   k##Name##Offset, k##Name##End = k##Name##Offset + sizeof(Type) - 1
   enum Header {
     // BasicMemoryChunk fields:
     FIELD(size_t, Size),
+#ifdef __CHERI_PURE_CAPABILITY__
+    FIELD(size_t, Flags),
+#else
     FIELD(uintptr_t, Flags),
+#endif
     FIELD(Heap*, Heap),
     FIELD(Address, AreaStart),
     FIELD(Address, AreaEnd),
@@ -56,6 +67,9 @@ class V8_EXPORT_PRIVATE MemoryChunkLayout {
     // MemoryChunk fields:
     FIELD(SlotSet* [kNumSets], SlotSet),
     FIELD(ProgressBar, ProgressBar),
+#if defined(__CHERI_PURE_CAPABILITY__)
+    FIELD(size_t, Padding0),
+#endif
     FIELD(std::atomic<intptr_t>, LiveByteCount),
     FIELD(TypedSlotsSet* [kNumSets], TypedSlotSet),
     FIELD(void* [kNumSets], InvalidatedSlots),
@@ -75,11 +89,17 @@ class V8_EXPORT_PRIVATE MemoryChunkLayout {
     FIELD(ObjectStartBitmap, ObjectStartBitmap),
 #endif
     kMarkingBitmapOffset,
-    kMemoryChunkHeaderSize = kMarkingBitmapOffset,
+    kMemoryChunkHeaderSize =
+        kMarkingBitmapOffset +
+        ((kMarkingBitmapOffset % kMemoryChunkAlignment) == 0
+             ? 0
+             : kMemoryChunkAlignment -
+                   (kMarkingBitmapOffset % kMemoryChunkAlignment)),
     kMemoryChunkHeaderStart = kSlotSetOffset,
     kBasicMemoryChunkHeaderSize = kMemoryChunkHeaderStart,
     kBasicMemoryChunkHeaderStart = 0,
   };
+#undef FIELD
   static size_t CodePageGuardStartOffset();
   static size_t CodePageGuardSize();
   static intptr_t ObjectStartOffsetInCodePage();
diff --git src/heap/memory-chunk.cc src/heap/memory-chunk.cc
index d9e1de92a7f..5003640843e 100644
--- src/heap/memory-chunk.cc
+++ src/heap/memory-chunk.cc
@@ -38,9 +38,14 @@ void MemoryChunk::InitializationMemoryFence() {
   // to tell TSAN that there is no data race when emitting a
   // InitializationMemoryFence. Note that the other thread still needs to
   // perform MemoryChunk::synchronized_heap().
+#if defined(__CHERI_PURE_CAPABILITY__)
+  base::Release_Store(reinterpret_cast<base::AtomicIntPtr*>(&heap_),
+                      reinterpret_cast<base::AtomicIntPtr>(heap_));
+#else
   base::Release_Store(reinterpret_cast<base::AtomicWord*>(&heap_),
                       reinterpret_cast<base::AtomicWord>(heap_));
 #endif
+#endif
 }
 
 void MemoryChunk::DecrementWriteUnprotectCounterAndMaybeSetPermissions(
diff --git src/heap/paged-spaces.h src/heap/paged-spaces.h
index 11481eb593d..697d83c05b3 100644
--- src/heap/paged-spaces.h
+++ src/heap/paged-spaces.h
@@ -505,7 +505,11 @@ class OldSpace final : public PagedSpace {
                    allocation_info) {}
 
   static bool IsAtPageStart(Address addr) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return static_cast<intptr_t>(addr & (size_t) kPageAlignmentMask) ==
+#else
     return static_cast<intptr_t>(addr & kPageAlignmentMask) ==
+#endif
            MemoryChunkLayout::ObjectStartOffsetInDataPage();
   }
 
diff --git src/heap/read-only-spaces.cc src/heap/read-only-spaces.cc
index 73046781b06..eb0e688bbdf 100644
--- src/heap/read-only-spaces.cc
+++ src/heap/read-only-spaces.cc
@@ -28,7 +28,8 @@ namespace v8 {
 namespace internal {
 
 void CopyAndRebaseRoots(Address* src, Address* dst, Address new_base) {
-  Address src_base = GetIsolateRootAddress(src[0]);
+  Address src_base =
+      V8HeapCompressionScheme::GetPtrComprCageBaseAddress(src[0]);
   for (size_t i = 0; i < ReadOnlyHeap::kEntriesCount; ++i) {
     dst[i] = src[i] - src_base + new_base;
   }
@@ -188,7 +189,9 @@ ReadOnlyHeap* PointerCompressedReadOnlyArtifacts::GetReadOnlyHeapForIsolate(
   Address isolate_root = isolate->isolate_root();
   for (Object original_object : original_cache) {
     Address original_address = original_object.ptr();
-    Address new_address = isolate_root + CompressTagged(original_address);
+    Address new_address =
+	isolate_root +
+	V8HeapCompressionScheme::CompressTagged(original_address);
     Object new_object = Object(new_address);
     cache.push_back(new_object);
   }
@@ -238,7 +241,8 @@ void PointerCompressedReadOnlyArtifacts::Initialize(
     pages_.push_back(new_page);
     shared_memory_.push_back(std::move(shared_memory));
     // This is just CompressTagged but inlined so it will always compile.
-    Tagged_t compressed_address = CompressTagged(page->address());
+    Tagged_t compressed_address =
+	V8HeapCompressionScheme::CompressTagged(page->address());
     page_offsets_.push_back(compressed_address);
 
     // 3. Update the accounting stats so the allocated bytes are for the new
diff --git src/heap/remembered-set-inl.h src/heap/remembered-set-inl.h
index b0908839ea4..a49e1a29ed2 100644
--- src/heap/remembered-set-inl.h
+++ src/heap/remembered-set-inl.h
@@ -37,13 +37,15 @@ SlotCallbackResult UpdateTypedSlotHelper::UpdateTypedSlot(Heap* heap,
       return UpdateEmbeddedPointer(heap, &rinfo, callback);
     }
     case SlotType::kConstPoolEmbeddedObjectCompressed: {
-      HeapObject old_target = HeapObject::cast(Object(
-          DecompressTaggedAny(heap->isolate(), base::Memory<Tagged_t>(addr))));
+      HeapObject old_target = 
+	  HeapObject::cast(Object(V8HeapCompressionScheme::DecompressTaggedAny(
+              heap->isolate(), base::Memory<Tagged_t>(addr))));
       HeapObject new_target = old_target;
       SlotCallbackResult result = callback(FullMaybeObjectSlot(&new_target));
       DCHECK(!HasWeakHeapObjectTag(new_target));
       if (new_target != old_target) {
-        base::Memory<Tagged_t>(addr) = CompressTagged(new_target.ptr());
+        base::Memory<Tagged_t>(addr) =
+            V8HeapCompressionScheme::CompressTagged(new_target.ptr());
       }
       return result;
     }
diff --git src/heap/slot-set.h src/heap/slot-set.h
index 7da8108f671..befa0293178 100644
--- src/heap/slot-set.h
+++ src/heap/slot-set.h
@@ -87,7 +87,11 @@ class PossiblyEmptyBuckets {
   static const int kWordSize = sizeof(uintptr_t);
   static const int kBitsPerWord = kWordSize * kBitsPerByte;
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+  bool IsAllocated() { return bitmap_ & (size_t) kPointerTag; }
+#else
   bool IsAllocated() { return bitmap_ & kPointerTag; }
+#endif
 
   void Allocate(size_t buckets) {
     DCHECK(!IsAllocated());
@@ -99,7 +103,11 @@ class PossiblyEmptyBuckets {
     for (size_t word_idx = 1; word_idx < words; word_idx++) {
       ptr[word_idx] = 0;
     }
+#if defined(__CHERI_PURE_CAPABILITY__)
+    bitmap_ = reinterpret_cast<Address>(ptr) + (size_t) kPointerTag;
+#else
     bitmap_ = reinterpret_cast<Address>(ptr) + kPointerTag;
+#endif
     DCHECK(IsAllocated());
   }
 
@@ -116,7 +124,11 @@ class PossiblyEmptyBuckets {
 
   uintptr_t* BitmapArray() {
     DCHECK(IsAllocated());
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return reinterpret_cast<uintptr_t*>(bitmap_ & ~(size_t) kPointerTag);
+#else
     return reinterpret_cast<uintptr_t*>(bitmap_ & ~kPointerTag);
+#endif
   }
 
   FRIEND_TEST(PossiblyEmptyBucketsTest, WordsForBuckets);
@@ -153,10 +165,21 @@ class SlotSet {
     // faster access in the write barrier. The number of buckets is needed for
     // calculating the size of this data structure.
     size_t buckets_size = buckets * sizeof(Bucket*);
+#if defined(__CHERI_PURE_CAPABILITY__)
+    size_t size = RoundUp(kInitialBucketsSize, alignof(max_align_t)) + buckets_size;
+#else
     size_t size = kInitialBucketsSize + buckets_size;
+#endif
     void* allocation = AlignedAlloc(size, kSystemPointerSize);
+#if defined(__CHERI_PURE_CAPABILITY__)
+    SlotSet* slot_set = reinterpret_cast<SlotSet*>(
+        reinterpret_cast<uint8_t*>(RoundUp(
+        reinterpret_cast<uintptr_t>(allocation) + kInitialBucketsSize,
+        alignof(max_align_t))));
+#else
     SlotSet* slot_set = reinterpret_cast<SlotSet*>(
         reinterpret_cast<uint8_t*>(allocation) + kInitialBucketsSize);
+#endif
     DCHECK(
         IsAligned(reinterpret_cast<uintptr_t>(slot_set), kSystemPointerSize));
 #ifdef DEBUG
@@ -486,7 +509,11 @@ class SlotSet {
               int bit_offset = base::bits::CountTrailingZeros(cell);
               uint32_t bit_mask = 1u << bit_offset;
               Address slot = (cell_offset + bit_offset) << kTaggedSizeLog2;
+#if defined(__CHERI_PURE_CAPABILITY__)
+              if (callback(MaybeObjectSlot(chunk_start + (size_t) slot)) == KEEP_SLOT) {
+#else
               if (callback(MaybeObjectSlot(chunk_start + slot)) == KEEP_SLOT) {
+#endif
                 ++in_bucket_count;
               } else {
                 mask |= bit_mask;
@@ -593,7 +620,7 @@ class SlotSet {
 
 #ifdef DEBUG
   size_t* initial_buckets() { return reinterpret_cast<size_t*>(this) - 1; }
-  static const int kInitialBucketsSize = sizeof(size_t);
+  static const int kInitialBucketsSize = sizeof(size_t *);
 #else
   static const int kInitialBucketsSize = 0;
 #endif
diff --git src/heap/spaces-inl.h src/heap/spaces-inl.h
index e4b224965cb..4e258328022 100644
--- src/heap/spaces-inl.h
+++ src/heap/spaces-inl.h
@@ -171,7 +171,11 @@ LocalAllocationBuffer LocalAllocationBuffer::FromResult(Heap* heap,
   USE(ok);
   DCHECK(ok);
   Address top = HeapObject::cast(obj).address();
+#if defined(__CHERI_PURE_CAPABILITY__)
+  return LocalAllocationBuffer(heap, LinearAllocationArea(top, top + (size_t) size));
+#else
   return LocalAllocationBuffer(heap, LinearAllocationArea(top, top + size));
+#endif
 }
 
 bool LocalAllocationBuffer::TryMerge(LocalAllocationBuffer* other) {
diff --git src/heap/spaces.h src/heap/spaces.h
index 08009d1d826..3864b183915 100644
--- src/heap/spaces.h
+++ src/heap/spaces.h
@@ -233,11 +233,19 @@ class Page : public MemoryChunk {
   // is in fact in a page.
   static Page* FromAddress(Address addr) {
     DCHECK(!V8_ENABLE_THIRD_PARTY_HEAP_BOOL);
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return reinterpret_cast<Page*>(addr & ~(size_t) kPageAlignmentMask);
+#else
     return reinterpret_cast<Page*>(addr & ~kPageAlignmentMask);
+#endif
   }
   static Page* FromHeapObject(HeapObject o) {
     DCHECK(!V8_ENABLE_THIRD_PARTY_HEAP_BOOL);
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return reinterpret_cast<Page*>(o.ptr() & ~(size_t) kAlignmentMask);
+#else
     return reinterpret_cast<Page*>(o.ptr() & ~kAlignmentMask);
+#endif
   }
 
   static Page* cast(MemoryChunk* chunk) {
@@ -261,7 +269,11 @@ class Page : public MemoryChunk {
 
   // Checks whether an address is page aligned.
   static bool IsAlignedToPageSize(Address addr) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return (addr & (size_t) kPageAlignmentMask) == 0;
+#else
     return (addr & kPageAlignmentMask) == 0;
+#endif
   }
 
   static Page* ConvertNewToOld(Page* old_page);
@@ -335,10 +347,15 @@ class Page : public MemoryChunk {
   friend class MemoryAllocator;
 };
 
+#ifndef __CHERI_PURE_CAPABILITY__
+// The calculation of kHeaderSize is broken for CHERI, as it doesn't take into 
+// account the padding under stronger alignments. I'm doubtful its worth fixing.
+
 // Validate our estimates on the header size.
 static_assert(sizeof(BasicMemoryChunk) <= BasicMemoryChunk::kHeaderSize);
 static_assert(sizeof(MemoryChunk) <= MemoryChunk::kHeaderSize);
 static_assert(sizeof(Page) <= MemoryChunk::kHeaderSize);
+#endif
 
 // -----------------------------------------------------------------------------
 // Interface for heap object iterator to be implemented by all object space
diff --git src/init/isolate-allocator.cc src/init/isolate-allocator.cc
index 61822b8e314..9da4af5a5af 100644
--- src/init/isolate-allocator.cc
+++ src/init/isolate-allocator.cc
@@ -5,6 +5,7 @@
 #include "src/init/isolate-allocator.h"
 
 #include "src/base/bounded-page-allocator.h"
+#include "src/common/ptr-compr-inl.h"
 #include "src/execution/isolate.h"
 #include "src/heap/code-range.h"
 #include "src/sandbox/sandbox.h"
@@ -50,7 +51,11 @@ struct PtrComprCageReservationParams
     page_size =
         RoundUp(size_t{1} << kPageSizeBits, page_allocator->AllocatePageSize());
     requested_start_hint =
+#if defined(__CHERI_PURE_CAPABILITY__)
+	reinterpret_cast<Address>(nullptr);
+#else
         reinterpret_cast<Address>(page_allocator->GetRandomMmapAddr());
+#endif
     jit = JitPermission::kNoJit;
   }
 };
@@ -108,7 +113,14 @@ void IsolateAllocator::InitializeOncePerProcess() {
         "Failed to reserve virtual memory for process-wide V8 "
         "pointer compression cage");
   }
-#endif
+  V8HeapCompressionScheme::InitBase(GetProcessWidePtrComprCage()->base());
+  #ifdef V8_EXTERNAL_CODE_SPACE
+  // Speculatively set the code cage base to the same value in case jitless
+  // mode will be used. Once the process-wide CodeRange instance is created
+  // the code cage base will be set accordingly.
+  ExternalCodeCompressionScheme::InitBase(GetProcessWidePtrComprCage()->base());
+#endif  // V8_EXTERNAL_CODE_SPACE
+#endif  // V8_COMPRESS_POINTERS_IN_SHARED_CAGE
 }
 
 IsolateAllocator::IsolateAllocator() {
diff --git src/init/v8.cc src/init/v8.cc
index 986dc39fac2..038f8bd426d 100644
--- src/init/v8.cc
+++ src/init/v8.cc
@@ -306,14 +306,23 @@ void V8::DisposePlatform() {
 
 v8::Platform* V8::GetCurrentPlatform() {
   v8::Platform* platform = reinterpret_cast<v8::Platform*>(
+#if defined(__CHERI_PURE_CAPABILITY__)
+      base::Relaxed_Load(reinterpret_cast<base::AtomicIntPtr*>(&platform_)));
+#else
       base::Relaxed_Load(reinterpret_cast<base::AtomicWord*>(&platform_)));
+#endif
   DCHECK(platform);
   return platform;
 }
 
 void V8::SetPlatformForTesting(v8::Platform* platform) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+  base::Relaxed_Store(reinterpret_cast<base::AtomicIntPtr*>(&platform_),
+                      reinterpret_cast<base::AtomicIntPtr>(platform));
+#else
   base::Relaxed_Store(reinterpret_cast<base::AtomicWord*>(&platform_),
                       reinterpret_cast<base::AtomicWord>(platform));
+#endif
 }
 
 void V8::SetSnapshotBlob(StartupData* snapshot_blob) {
diff --git src/logging/log-file.h src/logging/log-file.h
index de033268029..a905c4360f8 100644
--- src/logging/log-file.h
+++ src/logging/log-file.h
@@ -15,6 +15,7 @@
 #include "src/base/optional.h"
 #include "src/base/platform/mutex.h"
 #include "src/common/assert-scope.h"
+#include "src/common/cheri.h"
 #include "src/flags/flags.h"
 #include "src/utils/allocation.h"
 #include "src/utils/ostreams.h"
@@ -27,6 +28,7 @@ class Vector;
 }  // namespace base
 
 namespace internal {
+using cheri::operator<<;
 
 class V8FileLogger;
 
diff --git src/numbers/integer-literal.h src/numbers/integer-literal.h
index 5ac3ae76eee..38fe510d0ec 100644
--- src/numbers/integer-literal.h
+++ src/numbers/integer-literal.h
@@ -33,14 +33,19 @@ class IntegerLiteral {
            Compare(IntegerLiteral(std::numeric_limits<T>::max(), false)) <= 0;
   }
 
-  template <typename T>
-  T To() const {
-    static_assert(std::is_integral<T>::value, "Integral type required");
-    DCHECK(IsRepresentableAs<T>());
-    uint64_t v = absolute_value_;
-    if (negative_) v = ~v + 1;
-    return static_cast<T>(v);
+#if defined(__CHERI_PURE_CAPABILITY__)
+  template <>
+  bool IsRepresentableAs<intptr_t>() const {
+    return Compare(IntegerLiteral(std::numeric_limits<intmax_t>::min(), false)) >= 0 &&
+           Compare(IntegerLiteral(std::numeric_limits<intmax_t>::max(), false)) <= 0;
+  }
+
+  template <>
+  bool IsRepresentableAs<uintptr_t>() const {
+    return Compare(IntegerLiteral(std::numeric_limits<uintmax_t>::min(), false)) >= 0 &&
+           Compare(IntegerLiteral(std::numeric_limits<uintmax_t>::max(), false)) <= 0;
   }
+#endif
 
   template <typename T>
   base::Optional<T> TryTo() const {
@@ -48,6 +53,14 @@ class IntegerLiteral {
     if (!IsRepresentableAs<T>()) return base::nullopt;
     return To<T>();
   }
+  template <typename T>
+  T To() const {
+    static_assert(std::is_integral<T>::value, "Integral type required");
+    DCHECK(IsRepresentableAs<T>());
+    uint64_t v = absolute_value_;
+    if (negative_) v = ~v + 1;
+    return static_cast<T>(v);
+  }
 
   int Compare(const IntegerLiteral& other) const {
     if (absolute_value_ == other.absolute_value_) {
diff --git src/objects/bigint.h src/objects/bigint.h
index b58869bc463..13da6fc9920 100644
--- src/objects/bigint.h
+++ src/objects/bigint.h
@@ -54,7 +54,13 @@ class BigIntBase : public PrimitiveHeapObject {
   // somewhere below that maximum.
   static const int kMaxLengthBits = 1 << 30;  // ~1 billion.
   static const int kMaxLength =
-      kMaxLengthBits / (kSystemPointerSize * kBitsPerByte);
+#if UINTPTR_MAX == 0xFFFFFFFF
+      kMaxLengthBits / (kUInt32Size * kBitsPerByte);
+#elif UINTPTR_MAX == 0xFFFFFFFFFFFFFFFF
+      kMaxLengthBits / (kUInt64Size * kBitsPerByte);
+#else
+#error Unsupported platform.
+#endif
 
   // Sign and length are stored in the same bitfield.  Since the GC needs to be
   // able to read the length concurrently, the getters and setters are atomic.
@@ -87,10 +93,23 @@ class BigIntBase : public PrimitiveHeapObject {
   friend class ::v8::internal::BigInt;  // MSVC wants full namespace.
   friend class MutableBigInt;
 
-  using digit_t = uintptr_t;
+#if UINTPTR_MAX == 0xFFFFFFFF
+  using digit_t = uint32_t;
+#elif UINTPTR_MAX == 0xFFFFFFFFFFFFFFFF
+  using digit_t = uint64_t;
+#else
+#error Unsupported platform.
+#endif
+
   static const int kDigitSize = sizeof(digit_t);
   // kMaxLength definition assumes this:
-  static_assert(kDigitSize == kSystemPointerSize);
+#if UINTPTR_MAX == 0xFFFFFFFF
+  static_assert(kDigitSize == kUInt32Size);
+#elif UINTPTR_MAX == 0xFFFFFFFFFFFFFFFF
+  static_assert(kDigitSize == kUInt64Size);
+#else
+#error Unsupported platform.
+#endif
 
   static const int kDigitBits = kDigitSize * kBitsPerByte;
   static const int kHalfDigitBits = kDigitBits / 2;
diff --git src/objects/code-inl.h src/objects/code-inl.h
index 16e8d5be0c0..f1cbbce18bf 100644
--- src/objects/code-inl.h
+++ src/objects/code-inl.h
@@ -222,6 +222,7 @@ CODE_ACCESSORS_CHECKED2(bytecode_offset_table, ByteArray, kPositionTableOffset,
 // Concurrent marker needs to access kind specific flags in code data container.
 RELEASE_ACQUIRE_CODE_ACCESSORS(code_data_container, CodeDataContainer,
                                kCodeDataContainerOffset)
+
 #undef CODE_ACCESSORS
 #undef CODE_ACCESSORS_CHECKED2
 #undef RELEASE_ACQUIRE_CODE_ACCESSORS
@@ -229,8 +230,13 @@ RELEASE_ACQUIRE_CODE_ACCESSORS(code_data_container, CodeDataContainer,
 
 PtrComprCageBase Code::main_cage_base() const {
 #ifdef V8_EXTERNAL_CODE_SPACE
+#if defined(__CHERI_PURE_CAPABILITY__)
+  Address cage_base = ReadField<Address>(kMainCageBaseOffset);
+  return PtrComprCageBase(cage_base);
+#else
   Address cage_base_hi = ReadField<Tagged_t>(kMainCageBaseUpper32BitsOffset);
   return PtrComprCageBase(cage_base_hi << 32);
+#endif
 #else
   return GetPtrComprCageBase(*this);
 #endif
@@ -238,9 +244,14 @@ PtrComprCageBase Code::main_cage_base() const {
 
 PtrComprCageBase Code::main_cage_base(RelaxedLoadTag) const {
 #ifdef V8_EXTERNAL_CODE_SPACE
+#if defined(__CHERI_PURE_CAPABILITY__)
+  Address cage_base = ReadField<Address>(kMainCageBaseOffset);
+  return PtrComprCageBase(cage_base);
+#else
   Address cage_base_hi =
       Relaxed_ReadField<Tagged_t>(kMainCageBaseUpper32BitsOffset);
   return PtrComprCageBase(cage_base_hi << 32);
+#endif
 #else
   return GetPtrComprCageBase(*this);
 #endif
@@ -248,8 +259,12 @@ PtrComprCageBase Code::main_cage_base(RelaxedLoadTag) const {
 
 void Code::set_main_cage_base(Address cage_base, RelaxedStoreTag) {
 #ifdef V8_EXTERNAL_CODE_SPACE
+#if defined(__CHERI_PURE_CAPABILITY__)
+  Relaxed_WriteField<Address>(kMainCageBaseOffset, cage_base);
+#else
   Tagged_t cage_base_hi = static_cast<Tagged_t>(cage_base >> 32);
   Relaxed_WriteField<Tagged_t>(kMainCageBaseUpper32BitsOffset, cage_base_hi);
+#endif
 #else
   UNREACHABLE();
 #endif
@@ -303,9 +318,18 @@ inline Code FromCodeT(CodeT code) {
 #endif
 }
 
-inline Code FromCodeT(CodeT code, RelaxedLoadTag) {
+inline Code FromCodeT(CodeT code, PtrComprCageBase code_cage_base, RelaxedLoadTag tag) {
 #ifdef V8_EXTERNAL_CODE_SPACE
-  return code.code(kRelaxedLoad);
+  return code.code(code_cage_base, tag);
+#else
+  return code;
+#endif
+}
+
+inline Code FromCodeT(CodeT code, Isolate* isolate, RelaxedLoadTag tag) {
+#ifdef V8_EXTERNAL_CODE_SPACE
+  PtrComprCageBase code_cage_base(isolate->code_cage_base());
+  return FromCodeT(code, code_cage_base, tag);
 #else
   return code;
 #endif
@@ -350,7 +374,7 @@ Builtin CodeLookupResult::builtin_id() const {
 #endif
 }
 
-Code CodeLookupResult::ToCode() const {
+Code CodeLookupResult::ToCode(PtrComprCageBase cage_base) const {
 #ifdef V8_EXTERNAL_CODE_SPACE
   return IsCode() ? code() : FromCodeT(code_data_container());
 #else
@@ -473,7 +497,7 @@ Address CodeDataContainer::InstructionEnd(Isolate* isolate, Address pc) const {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   return V8_UNLIKELY(is_off_heap_trampoline())
              ? OffHeapInstructionEnd(isolate, pc)
-             : code().raw_instruction_end();
+             : code(PtrComprCageBase{isolate->code_cage_base()}).raw_instruction_end();
 }
 #endif
 
@@ -979,90 +1003,41 @@ static_assert(FIELD_SIZE(CodeDataContainer::kKindSpecificFlagsOffset) ==
 RELAXED_INT32_ACCESSORS(CodeDataContainer, kind_specific_flags,
                         kKindSpecificFlagsOffset)
 
-#if defined(V8_TARGET_LITTLE_ENDIAN)
-static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL ||
-                  (CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
-                   CodeDataContainer::kCodeOffset + kTaggedSize),
-              "CodeDataContainer::code field layout requires updating "
-              "for little endian architectures");
-#elif defined(V8_TARGET_BIG_ENDIAN)
-static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL,
-              "CodeDataContainer::code field layout requires updating "
-              "for big endian architectures");
-#endif
-
-Object CodeDataContainer::raw_code() const {
-  PtrComprCageBase cage_base = code_cage_base();
-  return CodeDataContainer::raw_code(cage_base);
-}
-
 Object CodeDataContainer::raw_code(PtrComprCageBase cage_base) const {
-  CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
-  Object value = TaggedField<Object, kCodeOffset>::load(cage_base, *this);
-  return value;
+#ifdef V8_EXTERNAL_CODE_SPACE
+  return ExternalCodeField<Object>::load(cage_base, *this);
+#else
+  UNREACHABLE();
+#endif // V8_EXTERNAL_CODE_SPACE
 }
 
 void CodeDataContainer::set_raw_code(Object value, WriteBarrierMode mode) {
-  CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
-  TaggedField<Object, kCodeOffset>::store(*this, value);
+#ifdef V8_EXTERNAL_CODE_SPACE
+  ExternalCodeField<Object>::Release_Store(*this, value);
   CONDITIONAL_WRITE_BARRIER(*this, kCodeOffset, value, mode);
-}
-
-Object CodeDataContainer::raw_code(RelaxedLoadTag tag) const {
-  PtrComprCageBase cage_base = code_cage_base(tag);
-  return CodeDataContainer::raw_code(cage_base, tag);
+#else
+  UNREACHABLE();
+#endif // V8_EXTERNAL_CODE_SPACE
 }
 
 Object CodeDataContainer::raw_code(PtrComprCageBase cage_base,
                                    RelaxedLoadTag) const {
-  Object value =
-      TaggedField<Object, kCodeOffset>::Relaxed_Load(cage_base, *this);
-  CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
-  return value;
-}
-
-ACCESSORS(CodeDataContainer, next_code_link, Object, kNextCodeLinkOffset)
-
-PtrComprCageBase CodeDataContainer::code_cage_base() const {
 #ifdef V8_EXTERNAL_CODE_SPACE
-  // TODO(v8:10391): consider protecting this value with the sandbox.
-  Address code_cage_base_hi =
-      ReadField<Tagged_t>(kCodeCageBaseUpper32BitsOffset);
-  return PtrComprCageBase(code_cage_base_hi << 32);
-#else
-  return GetPtrComprCageBase(*this);
-#endif
-}
-
-void CodeDataContainer::set_code_cage_base(Address code_cage_base) {
-#ifdef V8_EXTERNAL_CODE_SPACE
-  Tagged_t code_cage_base_hi = static_cast<Tagged_t>(code_cage_base >> 32);
-  WriteField<Tagged_t>(kCodeCageBaseUpper32BitsOffset, code_cage_base_hi);
+  return ExternalCodeField<Object>::Relaxed_Load(cage_base, *this);
 #else
   UNREACHABLE();
-#endif
+#endif // V8_EXTERNAL_CODE_SPACE
 }
 
-PtrComprCageBase CodeDataContainer::code_cage_base(RelaxedLoadTag) const {
-#ifdef V8_EXTERNAL_CODE_SPACE
-  // TODO(v8:10391): consider protecting this value with the sandbox.
-  Address code_cage_base_hi =
-      Relaxed_ReadField<Tagged_t>(kCodeCageBaseUpper32BitsOffset);
-  return PtrComprCageBase(code_cage_base_hi << 32);
-#else
-  return GetPtrComprCageBase(*this);
-#endif
-}
+ACCESSORS(CodeDataContainer, next_code_link, Object, kNextCodeLinkOffset)
 
-void CodeDataContainer::set_code_cage_base(Address code_cage_base,
-                                           RelaxedStoreTag) {
+PtrComprCageBase CodeDataContainer::code_cage_base() const {
 #ifdef V8_EXTERNAL_CODE_SPACE
-  Tagged_t code_cage_base_hi = static_cast<Tagged_t>(code_cage_base >> 32);
-  Relaxed_WriteField<Tagged_t>(kCodeCageBaseUpper32BitsOffset,
-                               code_cage_base_hi);
+  Isolate* isolate = GetIsolateFromWritableObject(*this);
+  return PtrComprCageBase(isolate->code_cage_base());
 #else
-  UNREACHABLE();
-#endif
+  return GetPtrComprCageBase(*this);
+#endif  // V8_EXTERNAL_CODE_SPACE
 }
 
 void CodeDataContainer::AllocateExternalPointerEntries(Isolate* isolate) {
@@ -1075,22 +1050,24 @@ Code CodeDataContainer::code() const {
   return CodeDataContainer::code(cage_base);
 }
 Code CodeDataContainer::code(PtrComprCageBase cage_base) const {
-  CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
-  return Code::cast(raw_code(cage_base));
+#ifdef V8_EXTERNAL_CODE_SPACE
+  return ExternalCodeField<Code>::load(cage_base, *this);
+#else
+  UNREACHABLE();
+#endif  // V8_EXTERNAL_CODE_SPACE
 }
 
 Code CodeDataContainer::code(RelaxedLoadTag tag) const {
-  PtrComprCageBase cage_base = code_cage_base(tag);
+  PtrComprCageBase cage_base = code_cage_base();
   return CodeDataContainer::code(cage_base, tag);
 }
-
 Code CodeDataContainer::code(PtrComprCageBase cage_base,
                              RelaxedLoadTag tag) const {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   return Code::cast(raw_code(cage_base, tag));
 }
 
-DEF_GETTER(CodeDataContainer, code_entry_point, Address) {
+Address CodeDataContainer::code_entry_point() const {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   Isolate* isolate = GetIsolateForSandbox(*this);
   return ReadExternalPointerField(kCodeEntryPointOffset, isolate,
@@ -1113,7 +1090,7 @@ void CodeDataContainer::SetCodeAndEntryPoint(Isolate* isolate_for_sandbox,
 void CodeDataContainer::UpdateCodeEntryPoint(Isolate* isolate_for_sandbox,
                                              Code code) {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
-  DCHECK_EQ(raw_code(), code);
+  DCHECK_EQ(raw_code(PtrComprCageBase(isolate_for_sandbox->code_cage_base())), code);
   set_code_entry_point(isolate_for_sandbox, code.InstructionStart());
 }
 
diff --git src/objects/code.h src/objects/code.h
index 89b244be3f1..a9ab4b00e46 100644
--- src/objects/code.h
+++ src/objects/code.h
@@ -73,19 +73,15 @@ class CodeDataContainer : public HeapObject {
   // When V8_EXTERNAL_CODE_SPACE is enabled, Code objects are allocated in
   // a separate pointer compression cage instead of the cage where all the
   // other objects are allocated.
-  // This field contains code cage base value which is used for decompressing
-  // the reference to respective Code. Basically, |code_cage_base| and |code|
-  // fields together form a full pointer. The reason why they are split is that
-  // the code field must also support atomic access and the word alignment of
-  // the full value is not guaranteed.
+  // This helper method returns code cage base value which is used for
+  // decompressing the reference to respective Code. It loads the Isolate from
+  // the page header (since the CodeDataContainer objects are always writable)
+  // and then the code cage base value from there.
   inline PtrComprCageBase code_cage_base() const;
-  inline void set_code_cage_base(Address code_cage_base);
-  inline PtrComprCageBase code_cage_base(RelaxedLoadTag) const;
-  inline void set_code_cage_base(Address code_cage_base, RelaxedStoreTag);
-
+  
   // Cached value of code().InstructionStart().
   // Available only when V8_EXTERNAL_CODE_SPACE is defined.
-  DECL_GETTER(code_entry_point, Address)
+  inline Address code_entry_point() const;
 
   inline void SetCodeAndEntryPoint(
       Isolate* isolate_for_sandbox, Code code,
@@ -192,8 +188,6 @@ class CodeDataContainer : public HeapObject {
   V(kCodeOffset, V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)     \
   V(kCodePointerFieldsStrongEndOffset, 0)                           \
   /* Raw data fields. */                                            \
-  V(kCodeCageBaseUpper32BitsOffset,                                 \
-    V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)                  \
   V(kCodeEntryPointOffset,                                          \
     V8_EXTERNAL_CODE_SPACE_BOOL ? kExternalPointerSize : 0)         \
   V(kFlagsOffset, V8_EXTERNAL_CODE_SPACE_BOOL ? kUInt16Size : 0)    \
@@ -206,6 +200,12 @@ class CodeDataContainer : public HeapObject {
   DEFINE_FIELD_OFFSET_CONSTANTS(HeapObject::kHeaderSize, CODE_DATA_FIELDS)
 #undef CODE_DATA_FIELDS
 
+#ifdef V8_EXTERNAL_CODE_SPACE
+  template <typename T>
+  using ExternalCodeField =
+      TaggedField<T, kCodeOffset, ExternalCodeCompressionScheme>;
+#endif  // V8_EXTERNAL_CODE_SPACE
+
   class BodyDescriptor;
 
   // Flags layout.
@@ -632,6 +632,44 @@ class Code : public HeapObject {
   class OptimizedCodeIterator;
 
   // Layout description.
+#if defined(__CHERI_PURE_CAPABILITY__)
+#define CODE_FIELDS(V)                                                        \
+  V(kRelocationInfoOffset, kTaggedSize)                                       \
+  V(kDeoptimizationDataOrInterpreterDataOffset, kTaggedSize)                  \
+  V(kPositionTableOffset, kTaggedSize)                                        \
+  V(kCodeDataContainerOffset, kTaggedSize)                                    \
+  /* Data or code not directly visited by GC directly starts here. */         \
+  /* The serializer needs to copy bytes starting from here verbatim. */       \
+  /* Objects embedded into code is visited via reloc info. */                 \
+  V(kDataStart, 0)                                                            \
+  /* 4 x kTaggedSize leaves the kMainCageBaseOffset field */                  \
+  /* kSystemPointerSize aligned (regardless of COMPRESS_POINTERS_BOOL). */    \
+  /* Assuming the code object is at least kSystemPointerSize aligned, */      \
+  /* this preserves the capability tag. */                                    \
+  /* Note that kCodeAlignment is 64bytes for ARM64. */                        \
+  V(kPadding0Offset, kTaggedSize)                                             \
+  V(kPadding1Offset, kTaggedSize)                                             \
+  V(kPadding2Offset, kTaggedSize)                                             \
+  V(kMainCageBaseOffset,                                                      \
+    V8_EXTERNAL_CODE_SPACE_BOOL ? kSystemPointerSize: 0)                      \
+  V(kInstructionSizeOffset, kIntSize)                                         \
+  V(kMetadataSizeOffset, kIntSize)                                            \
+  V(kFlagsOffset, kInt32Size)                                                 \
+  V(kBuiltinIndexOffset, kIntSize)                                            \
+  V(kInlinedBytecodeSizeOffset, kIntSize)                                     \
+  V(kOsrOffsetOffset, kInt32Size)                                             \
+  /* Offsets describing inline metadata tables, relative to MetadataStart. */ \
+  V(kHandlerTableOffsetOffset, kIntSize)                                      \
+  V(kConstantPoolOffsetOffset,                                                \
+    FLAG_enable_embedded_constant_pool.value() ? kIntSize : 0)                \
+  V(kCodeCommentsOffsetOffset, kIntSize)                                      \
+  V(kUnwindingInfoOffsetOffset, kInt32Size)                                   \
+  V(kUnalignedHeaderSize, 0)                                                  \
+  /* Add padding to align the instruction start following right after */      \
+  /* the Code object header. */                                               \
+  V(kOptionalPaddingOffset, CODE_POINTER_PADDING(kOptionalPaddingOffset))     \
+  V(kHeaderSize, 0)
+#else
 #define CODE_FIELDS(V)                                                        \
   V(kRelocationInfoOffset, kTaggedSize)                                       \
   V(kDeoptimizationDataOrInterpreterDataOffset, kTaggedSize)                  \
@@ -660,6 +698,7 @@ class Code : public HeapObject {
   /* the Code object header. */                                               \
   V(kOptionalPaddingOffset, CODE_POINTER_PADDING(kOptionalPaddingOffset))     \
   V(kHeaderSize, 0)
+#endif
 
   DEFINE_FIELD_OFFSET_CONSTANTS(HeapObject::kHeaderSize, CODE_FIELDS)
 #undef CODE_FIELDS
@@ -667,8 +706,14 @@ class Code : public HeapObject {
   // This documents the amount of free space we have in each Code object header
   // due to padding for code alignment.
 #if V8_TARGET_ARCH_ARM64
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static_assert(V8_EXTERNAL_CODE_SPACE_BOOL);
+  static_assert(COMPRESS_POINTERS_BOOL);
+  static constexpr int kHeaderPaddingSize = 12;
+#else
   static constexpr int kHeaderPaddingSize =
       V8_EXTERNAL_CODE_SPACE_BOOL ? 4 : (COMPRESS_POINTERS_BOOL ? 8 : 20);
+#endif
 #elif V8_TARGET_ARCH_MIPS64
   static constexpr int kHeaderPaddingSize = 20;
 #elif V8_TARGET_ARCH_LOONG64
@@ -862,7 +907,7 @@ class CodeLookupResult {
   // Helper method, coverts the successful lookup result to Code object.
   // It's not safe to be used from GC because conversion to Code might perform
   // a map check.
-  inline Code ToCode() const;
+  inline Code ToCode(PtrComprCageBase cage_base) const;
 
   // Helper method, coverts the successful lookup result to CodeT object.
   // It's not safe to be used from GC because conversion to CodeT might perform
@@ -907,12 +952,11 @@ class Code::OptimizedCodeIterator {
 inline CodeT ToCodeT(Code code);
 inline Handle<CodeT> ToCodeT(Handle<Code> code, Isolate* isolate);
 inline Code FromCodeT(CodeT code);
-inline Code FromCodeT(CodeT code, RelaxedLoadTag);
-inline Code FromCodeT(CodeT code, AcquireLoadTag);
-inline Code FromCodeT(CodeT code, PtrComprCageBase);
+inline Code FromCodeT(CodeT code, Isolate*, RelaxedLoadTag);
 inline Code FromCodeT(CodeT code, PtrComprCageBase, RelaxedLoadTag);
-inline Code FromCodeT(CodeT code, PtrComprCageBase, AcquireLoadTag);
 inline Handle<CodeT> FromCodeT(Handle<Code> code, Isolate* isolate);
+inline Handle<AbstractCode> ToAbstractCode(Handle<CodeT> code,
+		                           Isolate* isolate);
 inline CodeDataContainer CodeDataContainerFromCodeT(CodeT code);
 
 class AbstractCode : public HeapObject {
diff --git src/objects/compressed-slots-inl.h src/objects/compressed-slots-inl.h
index 0f99810219e..1f1f0530bee 100644
--- src/objects/compressed-slots-inl.h
+++ src/objects/compressed-slots-inl.h
@@ -11,8 +11,7 @@
 #include "src/objects/compressed-slots.h"
 #include "src/objects/maybe-object-inl.h"
 
-namespace v8 {
-namespace internal {
+namespace v8::internal {
 
 //
 // CompressedObjectSlot implementation.
@@ -36,16 +35,16 @@ bool CompressedObjectSlot::contains_map_value(Address raw_value) const {
 
 Object CompressedObjectSlot::operator*() const {
   Tagged_t value = *location();
-  return Object(DecompressTaggedAny(address(), value));
+  return Object(TCompressionScheme::DecompressTaggedAny(address(), value));
 }
 
 Object CompressedObjectSlot::load(PtrComprCageBase cage_base) const {
   Tagged_t value = *location();
-  return Object(DecompressTaggedAny(cage_base, value));
+  return Object(TCompressionScheme::DecompressTaggedAny(cage_base, value));
 }
 
 void CompressedObjectSlot::store(Object value) const {
-  *location() = CompressTagged(value.ptr());
+  *location() = TCompressionScheme::CompressTagged(value.ptr());
 }
 
 void CompressedObjectSlot::store_map(Map map) const {
@@ -64,36 +63,36 @@ Map CompressedObjectSlot::load_map() const {
 
 Object CompressedObjectSlot::Acquire_Load() const {
   AtomicTagged_t value = AsAtomicTagged::Acquire_Load(location());
-  return Object(DecompressTaggedAny(address(), value));
+  return Object(TCompressionScheme::DecompressTaggedAny(address(), value));
 }
 
 Object CompressedObjectSlot::Relaxed_Load() const {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location());
-  return Object(DecompressTaggedAny(address(), value));
+  return Object(TCompressionScheme::DecompressTaggedAny(address(), value));
 }
 
 Object CompressedObjectSlot::Relaxed_Load(PtrComprCageBase cage_base) const {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location());
-  return Object(DecompressTaggedAny(cage_base, value));
+  return Object(TCompressionScheme::DecompressTaggedAny(cage_base, value));
 }
 
 void CompressedObjectSlot::Relaxed_Store(Object value) const {
-  Tagged_t ptr = CompressTagged(value.ptr());
+  Tagged_t ptr = TCompressionScheme::CompressTagged(value.ptr());
   AsAtomicTagged::Relaxed_Store(location(), ptr);
 }
 
 void CompressedObjectSlot::Release_Store(Object value) const {
-  Tagged_t ptr = CompressTagged(value.ptr());
+  Tagged_t ptr = TCompressionScheme::CompressTagged(value.ptr());
   AsAtomicTagged::Release_Store(location(), ptr);
 }
 
 Object CompressedObjectSlot::Release_CompareAndSwap(Object old,
                                                     Object target) const {
-  Tagged_t old_ptr = CompressTagged(old.ptr());
-  Tagged_t target_ptr = CompressTagged(target.ptr());
+  Tagged_t old_ptr = TCompressionScheme::CompressTagged(old.ptr());
+  Tagged_t target_ptr = TCompressionScheme::CompressTagged(target.ptr());
   Tagged_t result =
       AsAtomicTagged::Release_CompareAndSwap(location(), old_ptr, target_ptr);
-  return Object(DecompressTaggedAny(address(), result));
+  return Object(TCompressionScheme::DecompressTaggedAny(address(), result));
 }
 
 //
@@ -102,38 +101,38 @@ Object CompressedObjectSlot::Release_CompareAndSwap(Object old,
 
 MaybeObject CompressedMaybeObjectSlot::operator*() const {
   Tagged_t value = *location();
-  return MaybeObject(DecompressTaggedAny(address(), value));
+  return MaybeObject(TCompressionScheme::DecompressTaggedAny(address(), value));
 }
 
 MaybeObject CompressedMaybeObjectSlot::load(PtrComprCageBase cage_base) const {
   Tagged_t value = *location();
-  return MaybeObject(DecompressTaggedAny(cage_base, value));
+  return MaybeObject(TCompressionScheme::DecompressTaggedAny(cage_base, value));
 }
 
 void CompressedMaybeObjectSlot::store(MaybeObject value) const {
-  *location() = CompressTagged(value.ptr());
+  *location() = TCompressionScheme::CompressTagged(value.ptr());
 }
 
 MaybeObject CompressedMaybeObjectSlot::Relaxed_Load() const {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location());
-  return MaybeObject(DecompressTaggedAny(address(), value));
+  return MaybeObject(TCompressionScheme::DecompressTaggedAny(address(), value));
 }
 
 MaybeObject CompressedMaybeObjectSlot::Relaxed_Load(
     PtrComprCageBase cage_base) const {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location());
-  return MaybeObject(DecompressTaggedAny(cage_base, value));
+  return MaybeObject(TCompressionScheme::DecompressTaggedAny(cage_base, value));
 }
 
 void CompressedMaybeObjectSlot::Relaxed_Store(MaybeObject value) const {
-  Tagged_t ptr = CompressTagged(value.ptr());
+  Tagged_t ptr = TCompressionScheme::CompressTagged(value.ptr());
   AsAtomicTagged::Relaxed_Store(location(), ptr);
 }
 
 void CompressedMaybeObjectSlot::Release_CompareAndSwap(
     MaybeObject old, MaybeObject target) const {
-  Tagged_t old_ptr = CompressTagged(old.ptr());
-  Tagged_t target_ptr = CompressTagged(target.ptr());
+  Tagged_t old_ptr = TCompressionScheme::CompressTagged(old.ptr());
+  Tagged_t target_ptr = TCompressionScheme::CompressTagged(target.ptr());
   AsAtomicTagged::Release_CompareAndSwap(location(), old_ptr, target_ptr);
 }
 
@@ -143,73 +142,86 @@ void CompressedMaybeObjectSlot::Release_CompareAndSwap(
 
 HeapObjectReference CompressedHeapObjectSlot::operator*() const {
   Tagged_t value = *location();
-  return HeapObjectReference(DecompressTaggedPointer(address(), value));
+  return HeapObjectReference(
+      TCompressionScheme::DecompressTaggedPointer(address(), value));
 }
 
 HeapObjectReference CompressedHeapObjectSlot::load(
     PtrComprCageBase cage_base) const {
   Tagged_t value = *location();
-  return HeapObjectReference(DecompressTaggedPointer(cage_base, value));
+  return HeapObjectReference(
+      TCompressionScheme::DecompressTaggedPointer(cage_base, value));
 }
 
 void CompressedHeapObjectSlot::store(HeapObjectReference value) const {
-  *location() = CompressTagged(value.ptr());
+  *location() = TCompressionScheme::CompressTagged(value.ptr());
 }
 
 HeapObject CompressedHeapObjectSlot::ToHeapObject() const {
   Tagged_t value = *location();
   DCHECK(HAS_STRONG_HEAP_OBJECT_TAG(value));
-  return HeapObject::cast(Object(DecompressTaggedPointer(address(), value)));
+  return HeapObject::cast(
+      Object(TCompressionScheme::DecompressTaggedPointer(address(), value)));
 }
 
 void CompressedHeapObjectSlot::StoreHeapObject(HeapObject value) const {
-  *location() = CompressTagged(value.ptr());
+  *location() = TCompressionScheme::CompressTagged(value.ptr());
 }
 
 //
 // OffHeapCompressedObjectSlot implementation.
 //
 
-Object OffHeapCompressedObjectSlot::load(PtrComprCageBase cage_base) const {
-  Tagged_t value = *location();
-  return Object(DecompressTaggedAny(cage_base, value));
+template <typename CompressionScheme>
+Object OffHeapCompressedObjectSlot<CompressionScheme>::load(
+    PtrComprCageBase cage_base) const {
+  Tagged_t value = *TSlotBase::location();
+  return Object(CompressionScheme::DecompressTaggedAny(cage_base, value));
 }
 
-void OffHeapCompressedObjectSlot::store(Object value) const {
-  *location() = CompressTagged(value.ptr());
+template <typename CompressionScheme>
+void OffHeapCompressedObjectSlot<CompressionScheme>::store(Object value) const {
+  *TSlotBase::location() = CompressionScheme::CompressTagged(value.ptr());
 }
 
-Object OffHeapCompressedObjectSlot::Relaxed_Load(
+template <typename CompressionScheme>
+Object OffHeapCompressedObjectSlot<CompressionScheme>::Relaxed_Load(
     PtrComprCageBase cage_base) const {
-  AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location());
-  return Object(DecompressTaggedAny(cage_base, value));
+  AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(TSlotBase::location());
+  return Object(CompressionScheme::DecompressTaggedAny(cage_base, value));
 }
 
-Object OffHeapCompressedObjectSlot::Acquire_Load(
+template <typename CompressionScheme>
+Object OffHeapCompressedObjectSlot<CompressionScheme>::Acquire_Load(
     PtrComprCageBase cage_base) const {
-  AtomicTagged_t value = AsAtomicTagged::Acquire_Load(location());
-  return Object(DecompressTaggedAny(cage_base, value));
+  AtomicTagged_t value = AsAtomicTagged::Acquire_Load(TSlotBase::location());
+  return Object(CompressionScheme::DecompressTaggedAny(cage_base, value));
 }
 
-void OffHeapCompressedObjectSlot::Relaxed_Store(Object value) const {
-  Tagged_t ptr = CompressTagged(value.ptr());
-  AsAtomicTagged::Relaxed_Store(location(), ptr);
+template <typename CompressionScheme>
+void OffHeapCompressedObjectSlot<CompressionScheme>::Relaxed_Store(
+    Object value) const {
+  Tagged_t ptr = CompressionScheme::CompressTagged(value.ptr());
+  AsAtomicTagged::Relaxed_Store(TSlotBase::location(), ptr);
 }
 
-void OffHeapCompressedObjectSlot::Release_Store(Object value) const {
-  Tagged_t ptr = CompressTagged(value.ptr());
-  AsAtomicTagged::Release_Store(location(), ptr);
+template <typename CompressionScheme>
+void OffHeapCompressedObjectSlot<CompressionScheme>::Release_Store(
+    Object value) const {
+  Tagged_t ptr = CompressionScheme::CompressTagged(value.ptr());
+  AsAtomicTagged::Release_Store(TSlotBase::location(), ptr);
 }
 
-void OffHeapCompressedObjectSlot::Release_CompareAndSwap(Object old,
-                                                         Object target) const {
-  Tagged_t old_ptr = CompressTagged(old.ptr());
-  Tagged_t target_ptr = CompressTagged(target.ptr());
-  AsAtomicTagged::Release_CompareAndSwap(location(), old_ptr, target_ptr);
+template <typename CompressionScheme>
+void OffHeapCompressedObjectSlot<CompressionScheme>::Release_CompareAndSwap(
+    Object old, Object target) const {
+  Tagged_t old_ptr = CompressionScheme::CompressTagged(old.ptr());
+  Tagged_t target_ptr = CompressionScheme::CompressTagged(target.ptr());
+  AsAtomicTagged::Release_CompareAndSwap(TSlotBase::location(), old_ptr,
+                                         target_ptr);
 }
 
-}  // namespace internal
-}  // namespace v8
+}  // namespace v8::internal
 
 #endif  // V8_COMPRESS_POINTERS
 
diff --git src/objects/compressed-slots.h src/objects/compressed-slots.h
index c31856d0a57..fd6d9acf484 100644
--- src/objects/compressed-slots.h
+++ src/objects/compressed-slots.h
@@ -8,16 +8,19 @@
 #include "include/v8config.h"
 #include "src/objects/slots.h"
 
-namespace v8 {
-namespace internal {
+namespace v8::internal {
 
 #ifdef V8_COMPRESS_POINTERS
+
+class V8HeapCompressionScheme;
+
 // A CompressedObjectSlot instance describes a kTaggedSize-sized field ("slot")
 // holding a compressed tagged pointer (smi or heap object).
 // Its address() is the address of the slot.
 // The slot's contents can be read and written using operator* and store().
 class CompressedObjectSlot : public SlotBase<CompressedObjectSlot, Tagged_t> {
  public:
+  using TCompressionScheme = V8HeapCompressionScheme;
   using TObject = Object;
   using THeapObjectSlot = CompressedHeapObjectSlot;
 
@@ -64,6 +67,7 @@ class CompressedObjectSlot : public SlotBase<CompressedObjectSlot, Tagged_t> {
 class CompressedMaybeObjectSlot
     : public SlotBase<CompressedMaybeObjectSlot, Tagged_t> {
  public:
+  using TCompressionScheme = V8HeapCompressionScheme;
   using TObject = MaybeObject;
   using THeapObjectSlot = CompressedHeapObjectSlot;
 
@@ -100,6 +104,8 @@ class CompressedMaybeObjectSlot
 class CompressedHeapObjectSlot
     : public SlotBase<CompressedHeapObjectSlot, Tagged_t> {
  public:
+  using TCompressionScheme = V8HeapCompressionScheme;
+
   CompressedHeapObjectSlot() : SlotBase(kNullAddress) {}
   explicit CompressedHeapObjectSlot(Address ptr) : SlotBase(ptr) {}
   explicit CompressedHeapObjectSlot(Object* ptr)
@@ -123,18 +129,23 @@ class CompressedHeapObjectSlot
 // and so does not provide an operator* with implicit Isolate* calculation.
 // Its address() is the address of the slot.
 // The slot's contents can be read and written using load() and store().
+template <typename CompressionScheme>
 class OffHeapCompressedObjectSlot
-    : public SlotBase<OffHeapCompressedObjectSlot, Tagged_t> {
+    : public SlotBase<OffHeapCompressedObjectSlot<CompressionScheme>,
+                      Tagged_t> {
  public:
+  using TSlotBase =
+      SlotBase<OffHeapCompressedObjectSlot<CompressionScheme>, Tagged_t>;
+  using TCompressionScheme = CompressionScheme;
   using TObject = Object;
-  using THeapObjectSlot = OffHeapCompressedObjectSlot;
+  using THeapObjectSlot = OffHeapCompressedObjectSlot<CompressionScheme>;
 
   static constexpr bool kCanBeWeak = false;
 
-  OffHeapCompressedObjectSlot() : SlotBase(kNullAddress) {}
-  explicit OffHeapCompressedObjectSlot(Address ptr) : SlotBase(ptr) {}
+  OffHeapCompressedObjectSlot() : TSlotBase(kNullAddress) {}
+  explicit OffHeapCompressedObjectSlot(Address ptr) : TSlotBase(ptr) {}
   explicit OffHeapCompressedObjectSlot(const uint32_t* ptr)
-      : SlotBase(reinterpret_cast<Address>(ptr)) {}
+      : TSlotBase(reinterpret_cast<Address>(ptr)) {}
 
   inline Object load(PtrComprCageBase cage_base) const;
   inline void store(Object value) const;
@@ -148,7 +159,6 @@ class OffHeapCompressedObjectSlot
 
 #endif  // V8_COMPRESS_POINTERS
 
-}  // namespace internal
-}  // namespace v8
+}  // namespace v8::internal
 
 #endif  // V8_OBJECTS_COMPRESSED_SLOTS_H_
diff --git src/objects/elements.cc src/objects/elements.cc
index c61e328a714..3f320c3f010 100644
--- src/objects/elements.cc
+++ src/objects/elements.cc
@@ -477,8 +477,8 @@ void SortIndices(Isolate* isolate, Handle<FixedArray> indices,
   AtomicSlot end(start + sort_size);
   std::sort(start, end, [isolate](Tagged_t elementA, Tagged_t elementB) {
 #ifdef V8_COMPRESS_POINTERS
-    Object a(DecompressTaggedAny(isolate, elementA));
-    Object b(DecompressTaggedAny(isolate, elementB));
+    Object a(V8HeapCompressionScheme::DecompressTaggedAny(isolate, elementA));
+    Object b(V8HeapCompressionScheme::DecompressTaggedAny(isolate, elementB));
 #else
     Object a(elementA);
     Object b(elementB);
diff --git src/objects/embedder-data-slot-inl.h src/objects/embedder-data-slot-inl.h
index 2a2ff513837..a069bc5fe60 100644
--- src/objects/embedder-data-slot-inl.h
+++ src/objects/embedder-data-slot-inl.h
@@ -177,7 +177,11 @@ void EmbedderDataSlot::PopulateEmbedderDataSnapshot(
     Map map, JSObject js_object, int entry_index,
     EmbedderDataSlotSnapshot& snapshot) {
 #ifdef V8_COMPRESS_POINTERS
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static_assert(sizeof(uintmax_t) == sizeof(AtomicTagged_t) * 2);
+#else
   static_assert(sizeof(EmbedderDataSlotSnapshot) == sizeof(AtomicTagged_t) * 2);
+#endif
 #else   // !V8_COMPRESS_POINTERS
   static_assert(sizeof(EmbedderDataSlotSnapshot) == sizeof(AtomicTagged_t));
 #endif  // !V8_COMPRESS_POINTERS
diff --git src/objects/fixed-array.h src/objects/fixed-array.h
index 3bc9525e176..9e454f6bfb5 100644
--- src/objects/fixed-array.h
+++ src/objects/fixed-array.h
@@ -88,7 +88,11 @@ class FixedArrayBase
   // -kTaggedSize is here to ensure that this max size always fits into Smi
   // which is necessary for being able to create a free space filler for the
   // whole array of kMaxSize.
+#ifndef __CHERI_PURE_CAPABILITY__
   static const int kMaxSize = 128 * kTaggedSize * MB - kTaggedSize;
+#else
+  static const int kMaxSize = 64 * kTaggedSize * MB - kTaggedSize;
+#endif
   static_assert(Smi::IsValid(kMaxSize));
 
  protected:
diff --git src/objects/free-space-inl.h src/objects/free-space-inl.h
index 9a2a00f729e..5490cbd72a7 100644
--- src/objects/free-space-inl.h
+++ src/objects/free-space-inl.h
@@ -27,13 +27,34 @@ int FreeSpace::Size() { return size(kRelaxedLoad); }
 
 FreeSpace FreeSpace::next() {
   DCHECK(IsValid());
+#ifdef V8_EXTERNAL_CODE_SPACE
+  intptr_t diff_to_next =
+      static_cast<intptr_t>(TaggedField<Smi, kNextOffset>::load(*this).value());
+  if (diff_to_next == 0) {
+    return FreeSpace();
+  }
+  Address next_ptr = ptr() + diff_to_next * kObjectAlignment;
+  return FreeSpace::unchecked_cast(Object(next_ptr));
+#else
   return FreeSpace::unchecked_cast(
       TaggedField<Object, kNextOffset>::load(*this));
+#endif
 }
 
 void FreeSpace::set_next(FreeSpace next) {
   DCHECK(IsValid());
-  RELAXED_WRITE_FIELD(*this, kNextOffset, next);
+#ifdef V8_EXTERNAL_CODE_SPACE
+   if (next.is_null()) {
+    TaggedField<Smi, kNextOffset>::Relaxed_Store(*this, Smi::zero());
+    return;
+  }
+  intptr_t diff_to_next = next.ptr() - ptr();
+  DCHECK(IsAligned(diff_to_next, kObjectAlignment));
+  TaggedField<Smi, kNextOffset>::Relaxed_Store(
+      *this, Smi::FromIntptr(diff_to_next / kObjectAlignment));
+#else
+ RELAXED_WRITE_FIELD(*this, kNextOffset, next);
+#endif
 }
 
 FreeSpace FreeSpace::cast(HeapObject o) {
diff --git src/objects/js-array-buffer-inl.h src/objects/js-array-buffer-inl.h
index b690c2aff03..8efb426bdd3 100644
--- src/objects/js-array-buffer-inl.h
+++ src/objects/js-array-buffer-inl.h
@@ -282,8 +282,13 @@ void JSTypedArray::RemoveExternalPointerCompensationForSerialization(
 void JSTypedArray::AddExternalPointerCompensationForDeserialization(
     Isolate* isolate) {
   DCHECK(is_on_heap());
+#if defined(__CHERI_PURE_CAPABILITY__)
+  Address pointer = ExternalPointerCompensationForOnHeapArray(isolate) +
+                    (size_t) ReadField<Address>(kExternalPointerOffset);
+#else
   Address pointer = ReadField<Address>(kExternalPointerOffset) +
                     ExternalPointerCompensationForOnHeapArray(isolate);
+#endif
   set_external_pointer(isolate, pointer);
 }
 
@@ -299,7 +304,7 @@ void* JSTypedArray::DataPtr() {
 
 void JSTypedArray::SetOffHeapDataPtr(Isolate* isolate, void* base,
                                      Address offset) {
-  Address address = reinterpret_cast<Address>(base) + offset;
+  Address address = reinterpret_cast<Address>(base) + (size_t) offset;
   set_external_pointer(isolate, address);
   // This is the only spot in which the `base_pointer` field can be mutated
   // after object initialization. Note this can happen at most once, when
diff --git src/objects/maybe-object-inl.h src/objects/maybe-object-inl.h
index 4b06fec5cb7..4459a8fe911 100644
--- src/objects/maybe-object-inl.h
+++ src/objects/maybe-object-inl.h
@@ -31,7 +31,11 @@ MaybeObject MaybeObject::FromObject(Object object) {
 
 MaybeObject MaybeObject::MakeWeak(MaybeObject object) {
   DCHECK(object.IsStrongOrWeak());
+#if defined(__CHERI_PURE_CAPABILITY__)
+  return MaybeObject(object.ptr() | (size_t) kWeakHeapObjectMask);
+#else
   return MaybeObject(object.ptr() | kWeakHeapObjectMask);
+#endif
 }
 
 // static
@@ -61,7 +65,11 @@ HeapObjectReference HeapObjectReference::Strong(Object object) {
 HeapObjectReference HeapObjectReference::Weak(Object object) {
   DCHECK(!object.IsSmi());
   DCHECK(!HasWeakHeapObjectTag(object));
+#if defined(__CHERI_PURE_CAPABILITY__)
+  return HeapObjectReference(object.ptr() | (size_t) kWeakHeapObjectMask);
+#else
   return HeapObjectReference(object.ptr() | kWeakHeapObjectMask);
+#endif
 }
 
 // static
@@ -84,8 +92,8 @@ HeapObjectReference HeapObjectReference::ClearedValue(
 #ifdef V8_COMPRESS_POINTERS
   // This is necessary to make pointer decompression computation also
   // suitable for cleared weak references.
-  Address raw_value =
-      DecompressTaggedPointer(cage_base, kClearedWeakHeapObjectLower32);
+  Address raw_value = V8HeapCompressionScheme::DecompressTaggedAny(
+      cage_base, kClearedWeakHeapObjectLower32);
 #else
   Address raw_value = kClearedWeakHeapObjectLower32;
 #endif
@@ -109,7 +117,11 @@ void HeapObjectReference::Update(THeapObjectSlot slot, HeapObject value) {
 #endif
 
   slot.store(
+#if defined(__CHERI_PURE_CAPABILITY__)
+      HeapObjectReference(new_value | (size_t) (old_value & (size_t) kWeakHeapObjectMask)));
+#else
       HeapObjectReference(new_value | (old_value & kWeakHeapObjectMask)));
+#endif
 
 #ifdef DEBUG
   bool weak_after = HAS_WEAK_HEAP_OBJECT_TAG((*slot).ptr());
diff --git src/objects/maybe-object.h src/objects/maybe-object.h
index 0393ef6497b..1005693ae1d 100644
--- src/objects/maybe-object.h
+++ src/objects/maybe-object.h
@@ -36,7 +36,7 @@ class MaybeObject : public TaggedImpl<HeapObjectReferenceType::WEAK, Address> {
 #endif
 
  private:
-  template <typename TFieldType, int kFieldOffset>
+  template <typename TFieldType, int kFieldOffset, typename CompressionScheme>
   friend class TaggedField;
 };
 
diff --git src/objects/object-macros.h src/objects/object-macros.h
index 0f2baa17b28..f24884f8307 100644
--- src/objects/object-macros.h
+++ src/objects/object-macros.h
@@ -18,14 +18,14 @@
 
 // Since this changes visibility, it should always be last in a class
 // definition.
-#define OBJECT_CONSTRUCTORS(Type, ...)             \
- public:                                           \
-  constexpr Type() : __VA_ARGS__() {}              \
-                                                   \
- protected:                                        \
-  template <typename TFieldType, int kFieldOffset> \
-  friend class TaggedField;                        \
-                                                   \
+#define OBJECT_CONSTRUCTORS(Type, ...)                                         \
+ public:                                                                       \
+  constexpr Type() : __VA_ARGS__() {}                                          \
+                                                                               \
+ protected:                                                                    \
+  template <typename TFieldType, int kFieldOffset, typename CompressionScheme> \
+  friend class TaggedField;                                                    \
+                                                                               \
   explicit inline Type(Address ptr)
 
 #define OBJECT_CONSTRUCTORS_IMPL(Type, Super) \
@@ -651,15 +651,15 @@ static_assert(sizeof(unsigned) == sizeof(uint32_t),
     set(IndexForEntry(i) + k##name##Offset, value);             \
   }
 
-#define TQ_OBJECT_CONSTRUCTORS(Type)               \
- public:                                           \
-  constexpr Type() = default;                      \
-                                                   \
- protected:                                        \
-  template <typename TFieldType, int kFieldOffset> \
-  friend class TaggedField;                        \
-                                                   \
-  inline explicit Type(Address ptr);               \
+#define TQ_OBJECT_CONSTRUCTORS(Type)                                           \
+ public:                                                                       \
+  constexpr Type() = default;                                                  \
+                                                                               \
+ protected:                                                                    \
+  template <typename TFieldType, int kFieldOffset, typename CompressionScheme> \
+  friend class TaggedField;                                                    \
+                                                                               \
+  inline explicit Type(Address ptr);                                           \
   friend class TorqueGenerated##Type<Type, Super>;
 
 #define TQ_OBJECT_CONSTRUCTORS_IMPL(Type) \
diff --git src/objects/objects-body-descriptors-inl.h src/objects/objects-body-descriptors-inl.h
index d236b71a547..c2b82b427e1 100644
--- src/objects/objects-body-descriptors-inl.h
+++ src/objects/objects-body-descriptors-inl.h
@@ -57,7 +57,7 @@ int FlexibleWeakBodyDescriptor<start_offset>::SizeOf(Map map,
 bool BodyDescriptorBase::IsValidJSObjectSlotImpl(Map map, HeapObject obj,
                                                  int offset) {
 #ifdef V8_COMPRESS_POINTERS
-  static_assert(kEmbedderDataSlotSize == 2 * kTaggedSize);
+  static_assert(kEmbedderDataSlotObservableSize == 2 * kTaggedSize);
   int embedder_fields_offset = JSObject::GetEmbedderFieldsStartOffset(map);
   int inobject_fields_offset = map.GetInObjectPropertyOffset(0);
   // |embedder_fields_offset| may be greater than |inobject_fields_offset| if
@@ -66,14 +66,14 @@ bool BodyDescriptorBase::IsValidJSObjectSlotImpl(Map map, HeapObject obj,
   if (embedder_fields_offset <= offset && offset < inobject_fields_offset) {
     // offset points to embedder fields area:
     // [embedder_fields_offset, inobject_fields_offset).
-    static_assert(base::bits::IsPowerOfTwo(kEmbedderDataSlotSize));
-    return ((offset - embedder_fields_offset) & (kEmbedderDataSlotSize - 1)) ==
+    static_assert(base::bits::IsPowerOfTwo(kEmbedderDataSlotObservableSize));
+    return ((offset - embedder_fields_offset) & (kEmbedderDataSlotObservableSize - 1)) ==
            EmbedderDataSlot::kTaggedPayloadOffset;
   }
 #else
   // We store raw aligned pointers as Smis, so it's safe to treat the whole
   // embedder field area as tagged slots.
-  static_assert(kEmbedderDataSlotSize == kTaggedSize);
+  static_assert(kEmbedderDataSlotObservableSize == kTaggedSize);
 #endif
   return true;
 }
@@ -84,7 +84,7 @@ void BodyDescriptorBase::IterateJSObjectBodyImpl(Map map, HeapObject obj,
                                                  int end_offset,
                                                  ObjectVisitor* v) {
 #ifdef V8_COMPRESS_POINTERS
-  static_assert(kEmbedderDataSlotSize == 2 * kTaggedSize);
+  static_assert(kEmbedderDataSlotObservableSize == 2 * kTaggedSize);
   int header_end_offset = JSObject::GetHeaderSize(map);
   int inobject_fields_start_offset = map.GetInObjectPropertyOffset(0);
   // We are always requested to process header and embedder fields.
@@ -107,7 +107,7 @@ void BodyDescriptorBase::IterateJSObjectBodyImpl(Map map, HeapObject obj,
 #else
   // We store raw aligned pointers as Smis, so it's safe to iterate the whole
   // embedder field area as tagged slots.
-  static_assert(kEmbedderDataSlotSize == kTaggedSize);
+  static_assert(kEmbedderDataSlotObservableSize == kTaggedSize);
 #endif
   IteratePointers(obj, start_offset, end_offset, v);
 }
@@ -1032,14 +1032,14 @@ class EmbedderDataArray::BodyDescriptor final : public BodyDescriptorBase {
  public:
   static bool IsValidSlot(Map map, HeapObject obj, int offset) {
 #ifdef V8_COMPRESS_POINTERS
-    static_assert(kEmbedderDataSlotSize == 2 * kTaggedSize);
-    static_assert(base::bits::IsPowerOfTwo(kEmbedderDataSlotSize));
+    static_assert(kEmbedderDataSlotObservableSize == 2 * kTaggedSize);
+    static_assert(base::bits::IsPowerOfTwo(kEmbedderDataSlotObservableSize));
     return (offset < EmbedderDataArray::kHeaderSize) ||
            (((offset - EmbedderDataArray::kHeaderSize) &
-             (kEmbedderDataSlotSize - 1)) ==
+             (kEmbedderDataSlotObservableSize - 1)) ==
             EmbedderDataSlot::kTaggedPayloadOffset);
 #else
-    static_assert(kEmbedderDataSlotSize == kTaggedSize);
+    static_assert(kEmbedderDataSlotObservableSize == kTaggedSize);
     // We store raw aligned pointers as Smis, so it's safe to iterate the whole
     // array.
     return true;
@@ -1050,7 +1050,7 @@ class EmbedderDataArray::BodyDescriptor final : public BodyDescriptorBase {
   static inline void IterateBody(Map map, HeapObject obj, int object_size,
                                  ObjectVisitor* v) {
 #ifdef V8_COMPRESS_POINTERS
-    static_assert(kEmbedderDataSlotSize == 2 * kTaggedSize);
+    static_assert(kEmbedderDataSlotObservableSize == 2 * kTaggedSize);
     for (int offset = EmbedderDataArray::OffsetOfElementAt(0);
          offset < object_size; offset += kEmbedderDataSlotSize) {
       IteratePointer(obj, offset + EmbedderDataSlot::kTaggedPayloadOffset, v);
@@ -1062,7 +1062,7 @@ class EmbedderDataArray::BodyDescriptor final : public BodyDescriptorBase {
 #else
     // We store raw aligned pointers as Smis, so it's safe to iterate the whole
     // array.
-    static_assert(kEmbedderDataSlotSize == kTaggedSize);
+    static_assert(kEmbedderDataSlotObservableSize == kTaggedSize);
     IteratePointers(obj, EmbedderDataArray::kHeaderSize, object_size, v);
 #endif
   }
diff --git src/objects/objects-inl.h src/objects/objects-inl.h
index 4dfde12920e..ac2f4af578f 100644
--- src/objects/objects-inl.h
+++ src/objects/objects-inl.h
@@ -151,9 +151,14 @@ template <class T,
                                       !std::is_floating_point<T>::value,
                                   int>::type>
 T Object::Relaxed_ReadField(size_t offset) const {
+// CHERI requires that both the code_cage_base and main_cage_base addresses
+// are stored by the Object, care has been taken to ensure that these
+// values are correctly aligned.
+#if !defined(__CHERI_PURE_CAPABILITY__)
   // Pointer compression causes types larger than kTaggedSize to be
   // unaligned. Atomic loads must be aligned.
   DCHECK_IMPLIES(COMPRESS_POINTERS_BOOL, sizeof(T) <= kTaggedSize);
+#endif
   using AtomicT = typename base::AtomicTypeFromByteWidth<sizeof(T)>::type;
   return static_cast<T>(base::AsAtomicImpl<AtomicT>::Relaxed_Load(
       reinterpret_cast<AtomicT*>(field_address(offset))));
@@ -165,9 +170,14 @@ template <class T,
                                       !std::is_floating_point<T>::value,
                                   int>::type>
 void Object::Relaxed_WriteField(size_t offset, T value) {
+// CHERI requires that both the code_cage_base and main_cage_base addresses
+// are stored by the Object, care has been taken to ensure that these
+// values are correctly aligned.
+#if !defined(__CHERI_PURE_CAPABILITY__)
   // Pointer compression causes types larger than kTaggedSize to be
   // unaligned. Atomic stores must be aligned.
   DCHECK_IMPLIES(COMPRESS_POINTERS_BOOL, sizeof(T) <= kTaggedSize);
+#endif
   using AtomicT = typename base::AtomicTypeFromByteWidth<sizeof(T)>::type;
   base::AsAtomicImpl<AtomicT>::Relaxed_Store(
       reinterpret_cast<AtomicT*>(field_address(offset)),
@@ -730,7 +740,11 @@ Map MapWord::ToMap() const {
 }
 
 bool MapWord::IsForwardingAddress() const {
+#if defined(__CHERI_PURE_CAPABILITY__)
+  return (value_ & (size_t) kForwardingTagMask) == kForwardingTag;
+#else
   return (value_ & kForwardingTagMask) == kForwardingTag;
+#endif
 }
 
 MapWord MapWord::FromForwardingAddress(HeapObject object) {
diff --git src/objects/objects.h src/objects/objects.h
index 1e4353f2a8f..6063adb46cb 100644
--- src/objects/objects.h
+++ src/objects/objects.h
@@ -892,7 +892,7 @@ class MapWord {
  private:
   // HeapObject calls the private constructor and directly reads the value.
   friend class HeapObject;
-  template <typename TFieldType, int kFieldOffset>
+  template <typename TFieldType, int kFieldOffset, typename CompressionScheme>
   friend class TaggedField;
 
   explicit MapWord(Address value) : value_(value) {}
diff --git src/objects/slots-inl.h src/objects/slots-inl.h
index 5a70d8fa018..3a0e2238438 100644
--- src/objects/slots-inl.h
+++ src/objects/slots-inl.h
@@ -168,7 +168,7 @@ inline void CopyTagged(Address dst, const Address src, size_t num_tagged) {
 // Sets |counter| number of kTaggedSize-sized values starting at |start| slot.
 inline void MemsetTagged(Tagged_t* start, Object value, size_t counter) {
 #ifdef V8_COMPRESS_POINTERS
-  Tagged_t raw_value = CompressTagged(value.ptr());
+  Tagged_t raw_value = V8HeapCompressionScheme::CompressTagged(value.ptr());
   MemsetUint32(start, raw_value, counter);
 #else
   Address raw_value = value.ptr();
diff --git src/objects/string.h src/objects/string.h
index 33f5c8a2f4a..4c7316d5a0c 100644
--- src/objects/string.h
+++ src/objects/string.h
@@ -548,7 +548,11 @@ class String : public TorqueGeneratedString<String, Name> {
       const uintptr_t non_one_byte_mask = kUintptrAllBitsSet / 0xFFFF * 0x00FF;
 #endif
       while (chars + sizeof(uintptr_t) <= limit) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+        if (*reinterpret_cast<const uintptr_t*>(chars) & (size_t) non_one_byte_mask) {
+#else
         if (*reinterpret_cast<const uintptr_t*>(chars) & non_one_byte_mask) {
+#endif
           break;
         }
         chars += (sizeof(uintptr_t) / sizeof(base::uc16));
diff --git src/objects/tagged-field-inl.h src/objects/tagged-field-inl.h
index 3ed08a95c95..21e9c8aa811 100644
--- src/objects/tagged-field-inl.h
+++ src/objects/tagged-field-inl.h
@@ -13,29 +13,31 @@ namespace v8 {
 namespace internal {
 
 // static
-template <typename T, int kFieldOffset>
-Address TaggedField<T, kFieldOffset>::address(HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+Address TaggedField<T, kFieldOffset, CompressionScheme>::address(
+    HeapObject host, int offset) {
   return host.address() + kFieldOffset + offset;
 }
 
 // static
-template <typename T, int kFieldOffset>
-Tagged_t* TaggedField<T, kFieldOffset>::location(HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+Tagged_t* TaggedField<T, kFieldOffset, CompressionScheme>::location(
+    HeapObject host, int offset) {
   return reinterpret_cast<Tagged_t*>(address(host, offset));
 }
 
 // static
-template <typename T, int kFieldOffset>
+template <typename T, int kFieldOffset, typename CompressionScheme>
 template <typename TOnHeapAddress>
-Address TaggedField<T, kFieldOffset>::tagged_to_full(
+Address TaggedField<T, kFieldOffset, CompressionScheme>::tagged_to_full(
     TOnHeapAddress on_heap_addr, Tagged_t tagged_value) {
 #ifdef V8_COMPRESS_POINTERS
   if (kIsSmi) {
-    return DecompressTaggedSigned(tagged_value);
+    return CompressionScheme::DecompressTaggedSigned(tagged_value);
   } else if (kIsHeapObject) {
-    return DecompressTaggedPointer(on_heap_addr, tagged_value);
+    return CompressionScheme::DecompressTaggedPointer(on_heap_addr, tagged_value);
   } else {
-    return DecompressTaggedAny(on_heap_addr, tagged_value);
+    return CompressionScheme::DecompressTaggedAny(on_heap_addr, tagged_value);
   }
 #else
   return tagged_value;
@@ -43,35 +45,37 @@ Address TaggedField<T, kFieldOffset>::tagged_to_full(
 }
 
 // static
-template <typename T, int kFieldOffset>
-Tagged_t TaggedField<T, kFieldOffset>::full_to_tagged(Address value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+Tagged_t TaggedField<T, kFieldOffset, CompressionScheme>::full_to_tagged(Address value) {
 #ifdef V8_COMPRESS_POINTERS
-  return CompressTagged(value);
+  return CompressionScheme::CompressTagged(value);
 #else
   return value;
 #endif
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::load(HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::load(HeapObject host,
+		                                        int offset) {
   Tagged_t value = *location(host, offset);
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   return T(tagged_to_full(host.ptr(), value));
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::load(PtrComprCageBase cage_base,
-                                     HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::load(
+     PtrComprCageBase cage_base, HeapObject host, int offset) {
   Tagged_t value = *location(host, offset);
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   return T(tagged_to_full(cage_base, value));
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::store(HeapObject host, T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::store(HeapObject host,
+		                                            T value) {
 #ifdef V8_ATOMIC_OBJECT_FIELD_WRITES
   Relaxed_Store(host, value);
 #else
@@ -82,8 +86,10 @@ void TaggedField<T, kFieldOffset>::store(HeapObject host, T value) {
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::store(HeapObject host, int offset, T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::store(HeapObject host,
+		                                            int offset,
+							    T value) {
 #ifdef V8_ATOMIC_OBJECT_FIELD_WRITES
   Relaxed_Store(host, offset, value);
 #else
@@ -94,107 +100,112 @@ void TaggedField<T, kFieldOffset>::store(HeapObject host, int offset, T value) {
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::Relaxed_Load(HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::Relaxed_Load(HeapObject host,
+		                                                int offset) {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location(host, offset));
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   return T(tagged_to_full(host.ptr(), value));
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::Relaxed_Load(PtrComprCageBase cage_base,
-                                             HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::Relaxed_Load(
+     PtrComprCageBase cage_base, HeapObject host, int offset) {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location(host, offset));
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   return T(tagged_to_full(cage_base, value));
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::Relaxed_Load_Map_Word(
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::Relaxed_Load_Map_Word(
     PtrComprCageBase cage_base, HeapObject host) {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location(host, 0));
   return T(tagged_to_full(cage_base, value));
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::Relaxed_Store_Map_Word(HeapObject host,
-                                                          T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::Relaxed_Store_Map_Word(
+    HeapObject host, T value) {
   AsAtomicTagged::Relaxed_Store(location(host), full_to_tagged(value.ptr()));
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::Relaxed_Store(HeapObject host, T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::Relaxed_Store(
+    HeapObject host, T value) {
   Address ptr = value.ptr();
   DCHECK_NE(kFieldOffset, HeapObject::kMapOffset);
   AsAtomicTagged::Relaxed_Store(location(host), full_to_tagged(ptr));
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::Relaxed_Store(HeapObject host, int offset,
-                                                 T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::Relaxed_Store(
+    HeapObject host, int offset, T value) {
   Address ptr = value.ptr();
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   AsAtomicTagged::Relaxed_Store(location(host, offset), full_to_tagged(ptr));
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::Acquire_Load(HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::Acquire_Load(HeapObject host,
+		                                                int offset) {
   AtomicTagged_t value = AsAtomicTagged::Acquire_Load(location(host, offset));
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   return T(tagged_to_full(host.ptr(), value));
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::Acquire_Load_No_Unpack(
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::Acquire_Load_No_Unpack(
     PtrComprCageBase cage_base, HeapObject host, int offset) {
   AtomicTagged_t value = AsAtomicTagged::Acquire_Load(location(host, offset));
   return T(tagged_to_full(cage_base, value));
 }
 
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::Acquire_Load(PtrComprCageBase cage_base,
-                                             HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::Acquire_Load(
+     PtrComprCageBase cage_base, HeapObject host, int offset) {
   AtomicTagged_t value = AsAtomicTagged::Acquire_Load(location(host, offset));
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   return T(tagged_to_full(cage_base, value));
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::Release_Store(HeapObject host, T value) {
-  Address ptr = value.ptr();
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::Release_Store(
+    HeapObject host, T value) { Address ptr = value.ptr();
   DCHECK_NE(kFieldOffset, HeapObject::kMapOffset);
   AsAtomicTagged::Release_Store(location(host), full_to_tagged(ptr));
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::Release_Store_Map_Word(HeapObject host,
-                                                          T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::Release_Store_Map_Word(
+    HeapObject host, T value) {
   Address ptr = value.ptr();
   AsAtomicTagged::Release_Store(location(host), full_to_tagged(ptr));
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::Release_Store(HeapObject host, int offset,
-                                                 T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::Release_Store(
+    HeapObject host, int offset, T value) {
   Address ptr = value.ptr();
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   AsAtomicTagged::Release_Store(location(host, offset), full_to_tagged(ptr));
 }
 
 // static
-template <typename T, int kFieldOffset>
-Tagged_t TaggedField<T, kFieldOffset>::Release_CompareAndSwap(HeapObject host,
-                                                              T old, T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+Tagged_t TaggedField<T, kFieldOffset,
+	             CompressionScheme>::Release_CompareAndSwap(HeapObject host,
+                                                                T old,
+								T value) {
   Tagged_t old_value = full_to_tagged(old.ptr());
   Tagged_t new_value = full_to_tagged(value.ptr());
   Tagged_t result = AsAtomicTagged::Release_CompareAndSwap(
@@ -203,43 +214,45 @@ Tagged_t TaggedField<T, kFieldOffset>::Release_CompareAndSwap(HeapObject host,
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::SeqCst_Load(HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::SeqCst_Load(HeapObject host,
+		                                               int offset) {
   AtomicTagged_t value = AsAtomicTagged::SeqCst_Load(location(host, offset));
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   return T(tagged_to_full(host.ptr(), value));
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::SeqCst_Load(PtrComprCageBase cage_base,
-                                            HeapObject host, int offset) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::SeqCst_Load(
+     PtrComprCageBase cage_base, HeapObject host, int offset) {
   AtomicTagged_t value = AsAtomicTagged::SeqCst_Load(location(host, offset));
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   return T(tagged_to_full(cage_base, value));
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::SeqCst_Store(HeapObject host, T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::SeqCst_Store(
+    HeapObject host, T value) {
   Address ptr = value.ptr();
   DCHECK_NE(kFieldOffset, HeapObject::kMapOffset);
   AsAtomicTagged::SeqCst_Store(location(host), full_to_tagged(ptr));
 }
 
 // static
-template <typename T, int kFieldOffset>
-void TaggedField<T, kFieldOffset>::SeqCst_Store(HeapObject host, int offset,
-                                                T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+void TaggedField<T, kFieldOffset, CompressionScheme>::SeqCst_Store(
+    HeapObject host, int offset, T value) {
   Address ptr = value.ptr();
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   AsAtomicTagged::SeqCst_Store(location(host, offset), full_to_tagged(ptr));
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::SeqCst_Swap(HeapObject host, int offset,
-                                            T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::SeqCst_Swap(
+     HeapObject host, int offset, T value) {
   Address ptr = value.ptr();
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   AtomicTagged_t old_value =
@@ -248,10 +261,9 @@ T TaggedField<T, kFieldOffset>::SeqCst_Swap(HeapObject host, int offset,
 }
 
 // static
-template <typename T, int kFieldOffset>
-T TaggedField<T, kFieldOffset>::SeqCst_Swap(PtrComprCageBase cage_base,
-                                            HeapObject host, int offset,
-                                            T value) {
+template <typename T, int kFieldOffset, typename CompressionScheme>
+T TaggedField<T, kFieldOffset, CompressionScheme>::SeqCst_Swap(
+    PtrComprCageBase cage_base, HeapObject host, int offset, T value) {
   Address ptr = value.ptr();
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   AtomicTagged_t old_value =
diff --git src/objects/tagged-field.h src/objects/tagged-field.h
index 7eb6a5a9cf5..aaf4b8e60b2 100644
--- src/objects/tagged-field.h
+++ src/objects/tagged-field.h
@@ -18,76 +18,60 @@ namespace internal {
 // For full-pointer mode this type adds no overhead but when pointer
 // compression is enabled such class allows us to use proper decompression
 // function depending on the field type.
-template <typename T, int kFieldOffset = 0>
+template <typename T, int kFieldOffset = 0,
+          typename CompressionScheme = V8HeapCompressionScheme>
 class TaggedField : public AllStatic {
  public:
   static_assert(std::is_base_of<Object, T>::value ||
                     std::is_same<MapWord, T>::value ||
                     std::is_same<MaybeObject, T>::value,
                 "T must be strong or weak tagged type or MapWord");
-
   // True for Smi fields.
   static constexpr bool kIsSmi = std::is_base_of<Smi, T>::value;
-
   // True for HeapObject and MapWord fields. The latter may look like a Smi
   // if it contains forwarding pointer but still requires tagged pointer
   // decompression.
   static constexpr bool kIsHeapObject =
       std::is_base_of<HeapObject, T>::value || std::is_same<MapWord, T>::value;
-
   static inline Address address(HeapObject host, int offset = 0);
-
   static inline T load(HeapObject host, int offset = 0);
   static inline T load(PtrComprCageBase cage_base, HeapObject host,
                        int offset = 0);
-
   static inline void store(HeapObject host, T value);
   static inline void store(HeapObject host, int offset, T value);
-
   static inline T Relaxed_Load(HeapObject host, int offset = 0);
   static inline T Relaxed_Load(PtrComprCageBase cage_base, HeapObject host,
                                int offset = 0);
-
   static inline void Relaxed_Store(HeapObject host, T value);
   static inline void Relaxed_Store(HeapObject host, int offset, T value);
-
   static inline T Acquire_Load(HeapObject host, int offset = 0);
   static inline T Acquire_Load_No_Unpack(PtrComprCageBase cage_base,
                                          HeapObject host, int offset = 0);
   static inline T Acquire_Load(PtrComprCageBase cage_base, HeapObject host,
                                int offset = 0);
-
   static inline T SeqCst_Load(HeapObject host, int offset = 0);
   static inline T SeqCst_Load(PtrComprCageBase cage_base, HeapObject host,
                               int offset = 0);
-
   static inline void Release_Store(HeapObject host, T value);
   static inline void Release_Store(HeapObject host, int offset, T value);
-
   static inline void SeqCst_Store(HeapObject host, T value);
   static inline void SeqCst_Store(HeapObject host, int offset, T value);
-
   static inline T SeqCst_Swap(HeapObject host, int offset, T value);
   static inline T SeqCst_Swap(PtrComprCageBase cage_base, HeapObject host,
                               int offset, T value);
-
   static inline Tagged_t Release_CompareAndSwap(HeapObject host, T old,
                                                 T value);
-
   // Note: Use these *_Map_Word methods only when loading a MapWord from a
   // MapField.
   static inline T Relaxed_Load_Map_Word(PtrComprCageBase cage_base,
                                         HeapObject host);
   static inline void Relaxed_Store_Map_Word(HeapObject host, T value);
   static inline void Release_Store_Map_Word(HeapObject host, T value);
-
  private:
   static inline Tagged_t* location(HeapObject host, int offset = 0);
-
   template <typename TOnHeapAddress>
   static inline Address tagged_to_full(TOnHeapAddress on_heap_addr,
                                        Tagged_t tagged_value);
-
   static inline Tagged_t full_to_tagged(Address value);
 };
 
diff --git src/objects/tagged-impl-inl.h src/objects/tagged-impl-inl.h
index 909f65a959e..a7fd044284d 100644
--- src/objects/tagged-impl-inl.h
+++ src/objects/tagged-impl-inl.h
@@ -34,7 +34,7 @@ Smi TaggedImpl<kRefType, StorageType>::ToSmi() const {
     return Smi(ptr_);
   }
   // Implementation for compressed pointers.
-  return Smi(DecompressTaggedSigned(static_cast<Tagged_t>(ptr_)));
+  return Smi(CompressionScheme::DecompressTaggedSigned(static_cast<Tagged_t>(ptr_)));
 }
 
 //
@@ -112,7 +112,7 @@ bool TaggedImpl<kRefType, StorageType>::GetHeapObjectIfStrong(
   // Implementation for compressed pointers.
   if (IsStrong()) {
     *result = HeapObject::cast(
-        Object(DecompressTaggedPointer(isolate, static_cast<Tagged_t>(ptr_))));
+        Object(CompressionScheme::DecompressTaggedPointer(isolate, static_cast<Tagged_t>(ptr_))));
     return true;
   }
   return false;
@@ -137,7 +137,7 @@ HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeStrong(
   // Implementation for compressed pointers.
   DCHECK(IsStrong());
   return HeapObject::cast(
-      Object(DecompressTaggedPointer(isolate, static_cast<Tagged_t>(ptr_))));
+      Object(CompressionScheme::DecompressTaggedPointer(isolate, static_cast<Tagged_t>(ptr_))));
 }
 
 //
@@ -207,7 +207,11 @@ HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObject() const {
   DCHECK(!IsSmi());
   if (kCanBeWeak) {
     DCHECK(!IsCleared());
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return HeapObject::cast(Object(ptr_ & (size_t) ~kWeakHeapObjectMask));
+#else
     return HeapObject::cast(Object(ptr_ & ~kWeakHeapObjectMask));
+#endif    
   } else {
     DCHECK(!HAS_WEAK_HEAP_OBJECT_TAG(ptr_));
     return HeapObject::cast(Object(ptr_));
@@ -222,12 +226,16 @@ HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObject(
   DCHECK(!IsSmi());
   if (kCanBeWeak) {
     DCHECK(!IsCleared());
-    return HeapObject::cast(Object(DecompressTaggedPointer(
+    return HeapObject::cast(Object(CompressionScheme::DecompressTaggedPointer(
+#if defined(__CHERI_PURE_CAPABILITY__)
         isolate, static_cast<Tagged_t>(ptr_) & ~kWeakHeapObjectMask)));
+#else
+        isolate, static_cast<Tagged_t>(ptr_) & ~kWeakHeapObjectMask)));
+#endif    
   } else {
     DCHECK(!HAS_WEAK_HEAP_OBJECT_TAG(ptr_));
     return HeapObject::cast(
-        Object(DecompressTaggedPointer(isolate, static_cast<Tagged_t>(ptr_))));
+        Object(CompressionScheme::DecompressTaggedPointer(isolate, static_cast<Tagged_t>(ptr_))));
   }
 }
 
@@ -250,7 +258,7 @@ Object TaggedImpl<kRefType, StorageType>::GetHeapObjectOrSmi(
   if (kIsFull) return GetHeapObjectOrSmi();
   // Implementation for compressed pointers.
   if (IsSmi()) {
-    return Object(DecompressTaggedSigned(static_cast<Tagged_t>(ptr_)));
+    return Object(CompressionScheme::DecompressTaggedSigned(static_cast<Tagged_t>(ptr_)));
   }
   return GetHeapObject(isolate);
 }
diff --git src/objects/tagged-impl.h src/objects/tagged-impl.h
index 6b01c6fe628..a78c2f6cf56 100644
--- src/objects/tagged-impl.h
+++ src/objects/tagged-impl.h
@@ -22,6 +22,11 @@ namespace internal {
 template <HeapObjectReferenceType kRefType, typename StorageType>
 class TaggedImpl {
  public:
+  // Compressed TaggedImpl are never used for external InstructionStream
+  // pointers, so we can use this shorter alias for calling decompression
+  // functions.
+  using CompressionScheme = V8HeapCompressionScheme;	
+
   static_assert(std::is_same<StorageType, Address>::value ||
                     std::is_same<StorageType, Tagged_t>::value,
                 "StorageType must be either Address or Tagged_t");
diff --git src/sandbox/external-pointer-table-inl.h src/sandbox/external-pointer-table-inl.h
index 6ff1e2eed1a..f914dd39b5e 100644
--- src/sandbox/external-pointer-table-inl.h
+++ src/sandbox/external-pointer-table-inl.h
@@ -23,7 +23,12 @@ void ExternalPointerTable::Init(Isolate* isolate) {
                    root_space->allocation_granularity()));
   buffer_ = root_space->AllocatePages(
       VirtualAddressSpace::kNoHint, kExternalPointerTableReservationSize,
+#if defined(__CHERI_PURE_CAPABILITY__)
+      // Need 
+      root_space->allocation_granularity(), PagePermissions::kReadWrite);
+#else
       root_space->allocation_granularity(), PagePermissions::kNoAccess);
+#endif
   if (!buffer_) {
     V8::FatalProcessOutOfMemory(
         isolate,
@@ -44,6 +49,15 @@ void ExternalPointerTable::Init(Isolate* isolate) {
   // empty EmbedderDataSlots represent nullptr.
   static_assert(kNullExternalPointer == 0);
   store(kNullExternalPointer, kNullAddress);
+#if defined(__CHERI_PURE_CAPABILITY__)
+  // ORing the type information with the pointer invalidates the capability.
+  // Therefore, for CHERI the external pointer table stores the tag information
+  // seperately in an entry after the pointer. Entries are store in the
+  // external pointer table as follows:
+  // index = pointer
+  // index + 1 = pointer tag
+  store(kNullExternalPointerTag, kNullAddress);
+#endif
 }
 
 void ExternalPointerTable::TearDown() {
@@ -61,22 +75,42 @@ void ExternalPointerTable::TearDown() {
 
 Address ExternalPointerTable::Get(uint32_t index,
                                   ExternalPointerTag tag) const {
+#if defined(__CHERI_PURE_CAPABILITY__)
+  DCHECK_LT(index, capacity_);
+  DCHECK_LT(index + 1, capacity_);
+  
+  Address entry = load_atomic(index);
+
+  Address entry_tag = load_atomic(index + 1);
+  DCHECK(!is_free(entry_tag));
+
+  return (ptraddr_t) (entry_tag & ~tag) == 0 ? entry : (Address) nullptr; 
+#else
   DCHECK_LT(index, capacity_);
 
   Address entry = load_atomic(index);
   DCHECK(!is_free(entry));
 
-  return entry & ~tag;
+ return entry & ~tag;
+#endif
 }
 
 void ExternalPointerTable::Set(uint32_t index, Address value,
                                ExternalPointerTag tag) {
-  DCHECK_LT(index, capacity_);
   DCHECK_NE(kNullExternalPointer, index);
-  DCHECK_EQ(0, value & kExternalPointerTagMask);
   DCHECK(is_marked(tag));
+  DCHECK_EQ(0, value & kExternalPointerTagMask);
+#if defined(__CHERI_PURE_CAPABILITY__)
+  DCHECK_LT(index, capacity_);
+  DCHECK_LT(index + 1, capacity_);
+
+  store_atomic(index, value);
+  store_atomic(index + 1, tag);
+#else
+  DCHECK_LT(index, capacity_);
 
   store_atomic(index, value | tag);
+#endif
 }
 
 uint32_t ExternalPointerTable::Allocate() {
@@ -124,10 +158,20 @@ uint32_t ExternalPointerTable::Allocate() {
 }
 
 void ExternalPointerTable::Mark(uint32_t index) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+  DCHECK_LT(index, capacity_);
+  DCHECK_LT(index + 1, capacity_);
+  static_assert(sizeof(base::AtomicIntPtr) == sizeof(Address));
+#else
   DCHECK_LT(index, capacity_);
   static_assert(sizeof(base::Atomic64) == sizeof(Address));
+#endif
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+  base::AtomicIntPtr old_val = load_atomic(index + 1);
+#else
   base::Atomic64 old_val = load_atomic(index);
+#endif
   DCHECK(!is_free(old_val));
   base::Atomic64 new_val = set_mark_bit(old_val);
 
@@ -135,7 +179,11 @@ void ExternalPointerTable::Mark(uint32_t index) {
   // to the old value, then the mutator must've just written a new value into
   // the entry. This in turn must've set the marking bit already (see
   // ExternalPointerTable::Set), so we don't need to do it again.
+#if defined(__CHERI_PURE_CAPABILITY__)
+  base::AtomicIntPtr* ptr = reinterpret_cast<base::AtomicIntPtr*>(entry_address(index + 1));
+#else
   base::Atomic64* ptr = reinterpret_cast<base::Atomic64*>(entry_address(index));
+#endif
   base::Atomic64 val = base::Relaxed_CompareAndSwap(ptr, old_val, new_val);
   DCHECK((val == old_val) || is_marked(val));
   USE(val);
diff --git src/sandbox/external-pointer-table.cc src/sandbox/external-pointer-table.cc
index 1ecfdd673fe..c947d698513 100644
--- src/sandbox/external-pointer-table.cc
+++ src/sandbox/external-pointer-table.cc
@@ -35,22 +35,44 @@ uint32_t ExternalPointerTable::Sweep(Isolate* isolate) {
 
   // Skip the special null entry.
   DCHECK_GE(capacity_, 1);
+#if defined(__CHERI_PURE_CAPABILITY__)
+  for (uint32_t i = capacity_ - 2; i > 0; i=i-2) {
+#else
   for (uint32_t i = capacity_ - 1; i > 0; i--) {
+#endif
     // No other threads are active during sweep, so there is no need to use
     // atomic operations here.
+#if defined(__CHERI_PURE_CAPABILITY__)
+    // Load the tag stored along with the external pointer
+    Address entry = load(i + 1);
+#else
     Address entry = load(i);
+#endif
     if (!is_marked(entry)) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+      store(i, current_freelist_head);
+      store(i + 1, kExternalPointerFreeEntryTag);
+#else
       store(i, make_freelist_entry(current_freelist_head));
+#endif
       current_freelist_head = i;
       freelist_size++;
     } else {
+#if defined(__CHERI_PURE_CAPABILITY__)
+      store(i + 1, clear_mark_bit(entry));
+#else
       store(i, clear_mark_bit(entry));
+#endif
     }
   }
 
   freelist_head_ = current_freelist_head;
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+  uint32_t num_active_entries = capacity_ / 2 - freelist_size;
+#else
   uint32_t num_active_entries = capacity_ - freelist_size;
+#endif
   isolate->counters()->sandboxed_external_pointers_count()->AddSample(
       num_active_entries);
   return num_active_entries;
@@ -76,12 +98,25 @@ uint32_t ExternalPointerTable::Grow() {
   capacity_ = new_capacity;
 
   // Build freelist bottom to top, which might be more cache friendly.
+#if defined(__CHERI_PURE_CAPABILITY__)
+  uint32_t start = std::max<uint32_t>(old_capacity, 2);  // Skip entry zero
+  uint32_t last = new_capacity - 2;
+  for (uint32_t i = start; i < last; i+=2) {
+    store(i, i + 2);
+    store(i + 1, kExternalPointerFreeEntryTag);
+#else
   uint32_t start = std::max<uint32_t>(old_capacity, 1);  // Skip entry zero
   uint32_t last = new_capacity - 1;
   for (uint32_t i = start; i < last; i++) {
     store(i, make_freelist_entry(i + 1));
+#endif
   }
+#if defined(__CHERI_PURE_CAPABILITY__)
+  store(last, 0);
+  store(last + 1, kExternalPointerFreeEntryTag);
+#else
   store(last, make_freelist_entry(0));
+#endif
 
   // This must be a release store to prevent reordering of the preceeding
   // stores to the freelist from being reordered past this store. See
diff --git src/sandbox/external-pointer-table.h src/sandbox/external-pointer-table.h
index d2cbc5d5ba7..a7a2e6cae18 100644
--- src/sandbox/external-pointer-table.h
+++ src/sandbox/external-pointer-table.h
@@ -114,7 +114,11 @@ class V8_EXPORT_PRIVATE ExternalPointerTable {
   // An external pointer table grows in blocks of this size. This is also the
   // initial size of the table.
   static const size_t kBlockSize = 64 * KB;
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static const size_t kEntriesPerBlock = kBlockSize / (2 * kSystemPointerSize);
+#else
   static const size_t kEntriesPerBlock = kBlockSize / kSystemPointerSize;
+#endif
 
   static const Address kExternalPointerMarkBit = 1ULL << 63;
 
@@ -146,13 +150,13 @@ class V8_EXPORT_PRIVATE ExternalPointerTable {
 
   // Atomically loads the value at the given index.
   inline Address load_atomic(uint32_t index) const {
-    auto addr = reinterpret_cast<base::Atomic64*>(entry_address(index));
+    auto addr = reinterpret_cast<base::AtomicIntPtr*>(entry_address(index));
     return base::Relaxed_Load(addr);
   }
 
   // Atomically stores the provided value at the given index.
   inline void store_atomic(uint32_t index, Address value) {
-    auto addr = reinterpret_cast<base::Atomic64*>(entry_address(index));
+    auto addr = reinterpret_cast<base::AtomicIntPtr*>(entry_address(index));
     base::Relaxed_Store(addr, value);
   }
 
diff --git src/sandbox/external-pointer.h src/sandbox/external-pointer.h
index cc81df39953..47c00b7a3bf 100644
--- src/sandbox/external-pointer.h
+++ src/sandbox/external-pointer.h
@@ -17,6 +17,9 @@ V8_INLINE Address DecodeExternalPointer(const Isolate* isolate,
                                         ExternalPointerTag tag);
 
 constexpr ExternalPointer_t kNullExternalPointer = 0;
+#if defined(__CHERI_PURE_CAPABILITY__)
+constexpr ExternalPointer_t kNullExternalPointerTag = 1;
+#endif
 
 // Creates zero-initialized entry in external pointer table and writes the entry
 // id to the field. When sandbox is not enabled, it's a no-op.
diff --git src/snapshot/deserializer.cc src/snapshot/deserializer.cc
index 1ece6b6584f..b2bac82f762 100644
--- src/snapshot/deserializer.cc
+++ src/snapshot/deserializer.cc
@@ -490,7 +490,6 @@ void Deserializer<IsolateT>::PostProcessNewObject(Handle<Map> map,
   } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     auto code_data_container = CodeDataContainer::cast(raw_obj);
-    code_data_container.set_code_cage_base(isolate()->code_cage_base());
     code_data_container.AllocateExternalPointerEntries(main_thread_isolate());
     code_data_container.UpdateCodeEntryPoint(main_thread_isolate(),
                                              code_data_container.code());
@@ -1195,8 +1194,13 @@ int Deserializer<IsolateT>::ReadSingleBytecodeData(byte data,
     case CASE_RANGE(kFixedRawData, 32): {
       // Deserialize raw data of fixed length from 1 to 32 times kTaggedSize.
       int size_in_tagged = FixedRawDataWithSize::Decode(data);
+#if defined(__CHERI_PURE_CAPABILITY__)
+      static_assert(kSystemPointerAddrSize == kTaggedSize ||
+                    kSystemPointerAddrSize == 2 * kTaggedSize);
+#else
       static_assert(TSlot::kSlotDataSize == kTaggedSize ||
                     TSlot::kSlotDataSize == 2 * kTaggedSize);
+#endif
       int size_in_slots = size_in_tagged / (TSlot::kSlotDataSize / kTaggedSize);
       // kFixedRawData can have kTaggedSize != TSlot::kSlotDataSize when
       // serializing Smi roots in pointer-compressed builds. In this case, the
diff --git src/snapshot/embedded/platform-embedded-file-writer-generic.cc src/snapshot/embedded/platform-embedded-file-writer-generic.cc
index d740ed9edf2..dc4a38f0f34 100644
--- src/snapshot/embedded/platform-embedded-file-writer-generic.cc
+++ src/snapshot/embedded/platform-embedded-file-writer-generic.cc
@@ -125,11 +125,11 @@ void PlatformEmbeddedFileWriterGeneric::DeclareFunctionBegin(const char* name,
       target_arch_ == EmbeddedTargetArch::kArm64) {
     // ELF format binaries on ARM use ".type <function name>, %function"
     // to create a DWARF subprogram entry.
-    fprintf(fp_, ".type %s, %%function\n", name);
+    fprintf(fp_, ".type %s, %%object\n", name);
   } else {
     // Other ELF Format binaries use ".type <function name>, @function"
     // to create a DWARF subprogram entry.
-    fprintf(fp_, ".type %s, @function\n", name);
+    fprintf(fp_, ".type %s, @object\n", name);
   }
   fprintf(fp_, ".size %s, %u\n", name, size);
 }
diff --git src/snapshot/serializer.cc src/snapshot/serializer.cc
index 43c2e859e21..228dded3ebf 100644
--- src/snapshot/serializer.cc
+++ src/snapshot/serializer.cc
@@ -270,7 +270,6 @@ void Serializer::PutSmiRoot(FullObjectSlot slot) {
   // Serializing a smi root in compressed pointer builds will serialize the
   // full object slot (of kSystemPointerSize) to avoid complications during
   // deserialization (endianness or smi sequences).
-  static_assert(decltype(slot)::kSlotDataSize == sizeof(Address));
   static_assert(decltype(slot)::kSlotDataSize == kSystemPointerSize);
   static constexpr int bytes_to_output = decltype(slot)::kSlotDataSize;
   static constexpr int size_in_tagged = bytes_to_output >> kTaggedSizeLog2;
@@ -942,11 +941,7 @@ void Serializer::ObjectSerializer::VisitCodePointer(HeapObject host,
   HandleScope scope(isolate());
   DisallowGarbageCollection no_gc;
 
-#ifdef V8_EXTERNAL_CODE_SPACE
   PtrComprCageBase code_cage_base(isolate()->code_cage_base());
-#else
-  PtrComprCageBase code_cage_base(isolate());
-#endif
   Object contents = slot.load(code_cage_base);
   DCHECK(HAS_STRONG_HEAP_OBJECT_TAG(contents.ptr()));
   DCHECK(contents.IsCode());
@@ -1205,15 +1200,13 @@ void Serializer::ObjectSerializer::OutputRawData(Address up_to) {
           sizeof(field_value), field_value);
     } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
                object_->IsCodeDataContainer(cage_base)) {
-      // code_cage_base and code_entry_point fields contain raw values that
+      // The code_entry_point field contains a raw value that
       // will be recomputed after deserialization, so write zeros to keep the
       // snapshot deterministic.
-      CHECK_EQ(CodeDataContainer::kCodeCageBaseUpper32BitsOffset + kTaggedSize,
-               CodeDataContainer::kCodeEntryPointOffset);
-      static byte field_value[kTaggedSize + kExternalPointerSize] = {0};
+      static byte field_value[kExternalPointerSize] = {0};
       OutputRawWithCustomField(
           sink_, object_start, base, bytes_to_output,
-          CodeDataContainer::kCodeCageBaseUpper32BitsOffset,
+          CodeDataContainer::kCodeEntryPointOffset,
           sizeof(field_value), field_value);
     } else {
       sink_->PutRaw(reinterpret_cast<byte*>(object_start + base),
diff --git src/snapshot/snapshot-source-sink.h src/snapshot/snapshot-source-sink.h
index 5a88fb7eb22..7de830b5160 100644
--- src/snapshot/snapshot-source-sink.h
+++ src/snapshot/snapshot-source-sink.h
@@ -57,6 +57,16 @@ class SnapshotByteSource final {
   }
 
   void CopySlots(Address* dest, int number_of_slots) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+    base::AtomicIntPtr* start = reinterpret_cast<base::AtomicIntPtr*>(dest);
+    base::AtomicIntPtr* end = start + number_of_slots;
+    for (base::AtomicIntPtr* p = start; p < end;
+      ++p, position_ += sizeof(base::AtomicIntPtr)) {
+      base::AtomicIntPtr val;
+      memcpy(&val, data_ + position_, sizeof(base::AtomicIntPtr));
+      base::Relaxed_Store(p, val);
+    }
+#else
     base::AtomicWord* start = reinterpret_cast<base::AtomicWord*>(dest);
     base::AtomicWord* end = start + number_of_slots;
     for (base::AtomicWord* p = start; p < end;
@@ -65,6 +75,7 @@ class SnapshotByteSource final {
       memcpy(&val, data_ + position_, sizeof(base::AtomicWord));
       base::Relaxed_Store(p, val);
     }
+#endif
   }
 
 #ifdef V8_COMPRESS_POINTERS
diff --git src/strings/string-stream.cc src/strings/string-stream.cc
index 9e079dff67d..2b5c2c7ef5d 100644
--- src/strings/string-stream.cc
+++ src/strings/string-stream.cc
@@ -408,6 +408,7 @@ void StringStream::PrintSecurityTokenIfChanged(JSFunction fun) {
 
 void StringStream::PrintFunction(JSFunction fun, Object receiver, Code* code) {
   PrintPrototype(fun, receiver);
+  Isolate* isolate = fun.GetIsolate();
   *code = FromCodeT(fun.code());
 }
 
diff --git src/strings/unicode-decoder.h src/strings/unicode-decoder.h
index 464869351bc..8466800bed5 100644
--- src/strings/unicode-decoder.h
+++ src/strings/unicode-decoder.h
@@ -31,7 +31,11 @@ inline int NonAsciiStart(const uint8_t* chars, int length) {
     DCHECK_EQ(unibrow::Utf8::kMaxOneByteChar, 0x7F);
     const uintptr_t non_one_byte_mask = kUintptrAllBitsSet / 0xFF * 0x80;
     while (chars + sizeof(uintptr_t) <= limit) {
+#if defined(__CHERI_PURE_CAPABILITY__)
+      if (*reinterpret_cast<const uintptr_t*>(chars) & (size_t) non_one_byte_mask) {
+#else
       if (*reinterpret_cast<const uintptr_t*>(chars) & non_one_byte_mask) {
+#endif
         return static_cast<int>(chars - start);
       }
       chars += sizeof(uintptr_t);
diff --git src/torque/implementation-visitor.cc src/torque/implementation-visitor.cc
index 1f233e12614..8b4c8729523 100644
--- src/torque/implementation-visitor.cc
+++ src/torque/implementation-visitor.cc
@@ -4936,10 +4936,16 @@ void ImplementationVisitor::GeneratePrintDefinitions(
     IfDefScope object_print(impl, "OBJECT_PRINT");
 
     impl << "#include <iosfwd>\n\n";
+    impl << "#include \"src/common/cheri.h\"\n\n";
     impl << "#include \"src/objects/all-objects-inl.h\"\n\n";
 
     NamespaceScope impl_namespaces(impl, {"v8", "internal"});
 
+#ifdef __CHERI_PURE_CAPABILITY__
+    // Workaround for missing ostream insertion operator, operator<<(u/intptr_t)
+    impl << "using cheri::operator<<;\n\n";
+#endif
+
     for (const ClassType* type : TypeOracle::GetClasses()) {
       if (!type->ShouldGeneratePrint()) continue;
       DCHECK(type->ShouldGenerateCppClassDefinitions());
diff --git src/torque/instructions.cc src/torque/instructions.cc
index 52f0f819765..24cd33bd0ca 100644
--- src/torque/instructions.cc
+++ src/torque/instructions.cc
@@ -153,12 +153,12 @@ void CallIntrinsicInstruction::TypeInstruction(Stack<const Type*>* stack,
                                                ControlFlowGraph* cfg) const {
   std::vector<const Type*> parameter_types =
       LowerParameterTypes(intrinsic->signature().parameter_types);
-  for (intptr_t i = parameter_types.size() - 1; i >= 0; --i) {
+  for (size_t i = parameter_types.size(); i > 0; --i) {
     const Type* arg_type = stack->Pop();
     const Type* parameter_type = parameter_types.back();
     parameter_types.pop_back();
     if (arg_type != parameter_type) {
-      ReportError("parameter ", i, ": expected type ", *parameter_type,
+      ReportError("parameter ", i - 1, ": expected type ", *parameter_type,
                   " but found type ", *arg_type);
     }
   }
@@ -208,12 +208,12 @@ void CallCsaMacroInstruction::TypeInstruction(Stack<const Type*>* stack,
                                               ControlFlowGraph* cfg) const {
   std::vector<const Type*> parameter_types =
       LowerParameterTypes(macro->signature().parameter_types);
-  for (intptr_t i = parameter_types.size() - 1; i >= 0; --i) {
+  for (size_t i = parameter_types.size(); i > 0; --i) {
     const Type* arg_type = stack->Pop();
     const Type* parameter_type = parameter_types.back();
     parameter_types.pop_back();
     if (arg_type != parameter_type) {
-      ReportError("parameter ", i, ": expected type ", *parameter_type,
+      ReportError("parameter ", i - 1, ": expected type ", *parameter_type,
                   " but found type ", *arg_type);
     }
   }
@@ -280,12 +280,12 @@ void CallCsaMacroAndBranchInstruction::TypeInstruction(
     Stack<const Type*>* stack, ControlFlowGraph* cfg) const {
   std::vector<const Type*> parameter_types =
       LowerParameterTypes(macro->signature().parameter_types);
-  for (intptr_t i = parameter_types.size() - 1; i >= 0; --i) {
+  for (size_t i = parameter_types.size(); i > 0; --i) {
     const Type* arg_type = stack->Pop();
     const Type* parameter_type = parameter_types.back();
     parameter_types.pop_back();
     if (arg_type != parameter_type) {
-      ReportError("parameter ", i, ": expected type ", *parameter_type,
+      ReportError("parameter ", i - 1, ": expected type ", *parameter_type,
                   " but found type ", *arg_type);
     }
   }
@@ -763,12 +763,12 @@ void MakeLazyNodeInstruction::TypeInstruction(Stack<const Type*>* stack,
                                               ControlFlowGraph* cfg) const {
   std::vector<const Type*> parameter_types =
       LowerParameterTypes(macro->signature().parameter_types);
-  for (intptr_t i = parameter_types.size() - 1; i >= 0; --i) {
+  for (size_t i = parameter_types.size(); i > 0; --i) {
     const Type* arg_type = stack->Pop();
     const Type* parameter_type = parameter_types.back();
     parameter_types.pop_back();
     if (arg_type != parameter_type) {
-      ReportError("parameter ", i, ": expected type ", *parameter_type,
+      ReportError("parameter ", i - 1, ": expected type ", *parameter_type,
                   " but found type ", *arg_type);
     }
   }
diff --git src/torque/runtime-macro-shims.h src/torque/runtime-macro-shims.h
index fe20a4052d7..e02965d9e91 100644
--- src/torque/runtime-macro-shims.h
+++ src/torque/runtime-macro-shims.h
@@ -21,8 +21,13 @@ namespace CodeStubAssembler {
 inline bool BoolConstant(bool b) { return b; }
 inline intptr_t ChangeInt32ToIntPtr(int32_t i) { return i; }
 inline uintptr_t ChangeUint32ToWord(uint32_t u) { return u; }
+#if defined(__CHERI_PURE_CAPABILITY__)
+inline intptr_t IntPtrAdd(intptr_t a, intptr_t b) { return (size_t) a + (size_t) b; }
+inline intptr_t IntPtrMul(intptr_t a, intptr_t b) { return (size_t) a * (size_t) b; }
+#else
 inline intptr_t IntPtrAdd(intptr_t a, intptr_t b) { return a + b; }
 inline intptr_t IntPtrMul(intptr_t a, intptr_t b) { return a * b; }
+#endif
 inline bool IntPtrLessThan(intptr_t a, intptr_t b) { return a < b; }
 inline bool IntPtrLessThanOrEqual(intptr_t a, intptr_t b) { return a <= b; }
 inline intptr_t Signed(uintptr_t u) { return static_cast<intptr_t>(u); }
diff --git src/torque/types.cc src/torque/types.cc
index 9fa92d87233..839ae94e3c5 100644
--- src/torque/types.cc
+++ src/torque/types.cc
@@ -1103,7 +1103,7 @@ TypeVector LowerType(const Type* type) {
 size_t LoweredSlotCount(const Type* type) { return LowerType(type).size(); }
 
 TypeVector LowerParameterTypes(const TypeVector& parameters) {
-  std::vector<const Type*> result;
+  std::vector<const Type*> result{};
   for (const Type* t : parameters) {
     AppendLoweredTypes(t, &result);
   }
diff --git src/tracing/trace-event.h src/tracing/trace-event.h
index d89d5492771..7169036d018 100644
--- src/tracing/trace-event.h
+++ src/tracing/trace-event.h
@@ -130,7 +130,7 @@ enum CategoryGroupEnabledFlags {
 
 // Defines atomic operations used internally by the tracing system.
 // Acquire/release barriers are important here: crbug.com/1330114#c8.
-#define TRACE_EVENT_API_ATOMIC_WORD v8::base::AtomicWord
+#define TRACE_EVENT_API_ATOMIC_WORD v8::base::AtomicIntPtr
 #define TRACE_EVENT_API_ATOMIC_LOAD(var) v8::base::Acquire_Load(&(var))
 #define TRACE_EVENT_API_ATOMIC_STORE(var, value) \
   v8::base::Release_Store(&(var), (value))
@@ -440,12 +440,12 @@ SetTraceValue(T arg, unsigned char* type, uint64_t* value) {
   *value = static_cast<uint64_t>(arg);
 }
 
+// TODO: how to fiux the static assert?
 #define INTERNAL_DECLARE_SET_TRACE_VALUE(actual_type, value_type_id)        \
   static V8_INLINE void SetTraceValue(actual_type arg, unsigned char* type, \
                                       uint64_t* value) {                    \
     *type = value_type_id;                                                  \
     *value = 0;                                                             \
-    static_assert(sizeof(arg) <= sizeof(*value));                           \
     memcpy(value, &arg, sizeof(arg));                                       \
   }
 INTERNAL_DECLARE_SET_TRACE_VALUE(double, TRACE_VALUE_TYPE_DOUBLE)
diff --git src/utils/allocation.cc src/utils/allocation.cc
index ac187407ce7..71bce6c713c 100644
--- src/utils/allocation.cc
+++ src/utils/allocation.cc
@@ -255,7 +255,13 @@ VirtualMemory::VirtualMemory(v8::PageAllocator* page_allocator, size_t size,
   PageAllocator::Permission permissions =
       jit == JitPermission::kMapAsJittable
           ? PageAllocator::kNoAccessWillJitLater
+#if defined(__CHERI_PURE_CAPABILITY__)
+	  // To ensure the ability to write a capability to the mapping
+	  // it must be created with read/write permissions.
+          : PageAllocator::kReadWrite;
+#else
           : PageAllocator::kNoAccess;
+#endif
   Address address = reinterpret_cast<Address>(AllocatePages(
       page_allocator_, hint, RoundUp(size, page_size), alignment, permissions));
   if (address != kNullAddress) {
diff --git src/utils/bit-vector.h src/utils/bit-vector.h
index fa06f9d080b..fbb74e99686 100644
--- src/utils/bit-vector.h
+++ src/utils/bit-vector.h
@@ -91,8 +91,13 @@ class V8_EXPORT_PRIVATE BitVector : public ZoneObject {
   };
 
   static const int kDataLengthForInline = 1;
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static const int kDataBits = kBitsPerPtrAddr;
+  static const int kDataBitShift = kBitsPerPtrAddrLog2;
+#else
   static const int kDataBits = kBitsPerSystemPointer;
   static const int kDataBitShift = kBitsPerSystemPointerLog2;
+#endif
   static const uintptr_t kOne = 1;  // This saves some static_casts.
 
   BitVector() : length_(0), data_length_(kDataLengthForInline), data_(0) {}
@@ -172,7 +177,11 @@ class V8_EXPORT_PRIVATE BitVector : public ZoneObject {
   bool Contains(int i) const {
     DCHECK(i >= 0 && i < length());
     uintptr_t block = is_inline() ? data_.inline_ : data_.ptr_[i / kDataBits];
+#if defined(__CHERI_PURE_CAPABILITY__)
+    return (block & (size_t) (kOne << (i % kDataBits))) != 0;
+#else
     return (block & (kOne << (i % kDataBits))) != 0;
+#endif
   }
 
   void Add(int i) {
diff --git src/utils/sparse-bit-vector.h src/utils/sparse-bit-vector.h
index 8c3bd8dde30..b2075c0a31b 100644
--- src/utils/sparse-bit-vector.h
+++ src/utils/sparse-bit-vector.h
@@ -21,7 +21,11 @@ class SparseBitVector : public ZoneObject {
   // 6 words for the bits plus {offset} plus {next} will be 8 machine words per
   // {Segment}. Most bit vectors are expected to fit in that single {Segment}.
   static constexpr int kNumWordsPerSegment = 6;
+#if defined(__CHERI_PURE_CAPABILITY__)
+  static constexpr int kBitsPerWord = kBitsPerByte * kPtrAddrSize;
+#else
   static constexpr int kBitsPerWord = kBitsPerByte * kSystemPointerSize;
+#endif
   static constexpr int kNumBitsPerSegment = kBitsPerWord * kNumWordsPerSegment;
 
   struct Segment {
diff --git src/zone/zone.h src/zone/zone.h
index caf17860728..d2c8d2234df 100644
--- src/zone/zone.h
+++ src/zone/zone.h
@@ -205,8 +205,13 @@ class V8_EXPORT_PRIVATE Zone final {
   // (e.g. tracking allocated bytes, maintaining linked lists, etc).
   void ReleaseSegment(Segment* segment);
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+  // All pointers returned from New() are alignof(max_align_t) aligned.
+  static const size_t kAlignmentInBytes = alignof(max_align_t);
+#else
   // All pointers returned from New() are 8-byte aligned.
   static const size_t kAlignmentInBytes = 8;
+#endif
 
   // Never allocate segments smaller than this size in bytes.
   static const size_t kMinimumSegmentSize = 8 * KB;
diff --git test/cctest/test-allocation.cc test/cctest/test-allocation.cc
index 032b1160c67..cb6ff0c9f85 100644
--- test/cctest/test-allocation.cc
+++ test/cctest/test-allocation.cc
@@ -61,7 +61,7 @@ size_t GetHugeMemoryAmount() {
   if (!huge_memory) {
     for (int i = 0; i < 100; i++) {
       huge_memory |=
-          v8::base::bit_cast<size_t>(v8::internal::GetRandomMmapAddr());
+          v8::base::bit_cast<uintptr_t>(v8::internal::GetRandomMmapAddr());
     }
     // Make it larger than the available address space.
     huge_memory *= 2;
diff --git test/unittests/base/atomic-utils-unittest.cc test/unittests/base/atomic-utils-unittest.cc
index 7ef0e948d7a..029b4b82d6d 100644
--- test/unittests/base/atomic-utils-unittest.cc
+++ test/unittests/base/atomic-utils-unittest.cc
@@ -118,7 +118,11 @@ TEST(AsAtomic8, CompareAndSwap_Concurrent) {
   }
 }
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+TEST(AsAtomicIntPtr, SetBits_Sequential) {
+#else
 TEST(AsAtomicWord, SetBits_Sequential) {
+#endif
   uintptr_t word = 0;
   // Fill the word with a repeated 0xF0 pattern.
   for (unsigned i = 0; i < sizeof(word); i++) {
@@ -132,7 +136,11 @@ TEST(AsAtomicWord, SetBits_Sequential) {
   uintptr_t mask = 0xFF;
   for (unsigned i = 0; i < sizeof(word); i++) {
     uintptr_t byte = static_cast<uintptr_t>(i) << (i * 8);
+#if defined(__CHERI_PURE_CAPABILITY__)
+    AsAtomicIntPtr::SetBits(&word, byte, mask);
+#else
     AsAtomicWord::SetBits(&word, byte, mask);
+#endif
     mask <<= 8;
   }
   for (unsigned i = 0; i < sizeof(word); i++) {
@@ -157,7 +165,11 @@ class BitSettingThread final : public Thread {
   void Run() override {
     uintptr_t bit = 1;
     bit = bit << bit_index_;
+#if defined(__CHERI_PURE_CAPABILITY__)
+    AsAtomicIntPtr::SetBits(word_addr_, bit, bit);
+#else
     AsAtomicWord::SetBits(word_addr_, bit, bit);
+#endif
   }
 
  private:
@@ -167,13 +179,21 @@ class BitSettingThread final : public Thread {
 
 }  // namespace.
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+TEST(AsAtomicIntPtr, SetBits_Concurrent) {
+#else
 TEST(AsAtomicWord, SetBits_Concurrent) {
+#endif
   const int kBitCount = sizeof(uintptr_t) * 8;
   const int kThreadCount = kBitCount / 2;
   BitSettingThread threads[kThreadCount];
 
   uintptr_t word;
+#if defined(__CHERI_PURE_CAPABILITY__)
+  AsAtomicIntPtr::Relaxed_Store(&word, 0);
+#else
   AsAtomicWord::Relaxed_Store(&word, 0);
+#endif
   for (int i = 0; i < kThreadCount; i++) {
     // Thread i sets bit number i * 2.
     threads[i].Initialize(&word, i * 2);
@@ -184,7 +204,11 @@ TEST(AsAtomicWord, SetBits_Concurrent) {
   for (int i = 0; i < kThreadCount; i++) {
     threads[i].Join();
   }
+#if defined(__CHERI_PURE_CAPABILITY__)
+  uintptr_t actual_word = AsAtomicIntPtr::Relaxed_Load(&word);
+#else
   uintptr_t actual_word = AsAtomicWord::Relaxed_Load(&word);
+#endif
   for (int i = 0; i < kBitCount; i++) {
     // Every second bit must be set.
     uintptr_t expected = (i % 2 == 0);
diff --git test/unittests/compiler/node-test-utils.cc test/unittests/compiler/node-test-utils.cc
index b449faee8db..b86fda47573 100644
--- test/unittests/compiler/node-test-utils.cc
+++ test/unittests/compiler/node-test-utils.cc
@@ -6,6 +6,7 @@
 
 #include <vector>
 
+#include "src/common/cheri.h"
 #include "src/compiler/common-operator.h"
 #include "src/compiler/js-operator.h"
 #include "src/compiler/node-properties.h"
@@ -28,6 +29,7 @@ bool operator==(Handle<HeapObject> const& lhs, Handle<HeapObject> const& rhs) {
 }
 
 namespace compiler {
+using cheri::operator<<;
 
 namespace {
 
diff --git test/unittests/heap/unmapper-unittest.cc test/unittests/heap/unmapper-unittest.cc
index 0db6a51ebb2..5ab6969cfda 100644
--- test/unittests/heap/unmapper-unittest.cc
+++ test/unittests/heap/unmapper-unittest.cc
@@ -6,6 +6,7 @@
 #include <optional>
 
 #include "src/base/region-allocator.h"
+#include "src/common/cheri.h"
 #include "src/execution/isolate.h"
 #include "src/heap/heap-inl.h"
 #include "src/heap/memory-allocator.h"
@@ -16,6 +17,7 @@
 
 namespace v8 {
 namespace internal {
+using cheri::operator<<;
 
 // This is a v8::PageAllocator implementation that decorates provided page
 // allocator object with page tracking functionality.
